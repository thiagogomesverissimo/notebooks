@article{ref0_550b11,
    author = "Verbert, Katrien and Duval, Erik and Klerkx, Joris and Govaerts, Sten and Santos, José",
    title = "Learning Analytics Dashboard Applications",
    year = "2013",
    issn = "0002-7642",
    doi = "10.1177/0002764213479363",
    abstract = "This article introduces learning analytics dashboards that visualize learning traces for learners and teachers. We present a conceptual framework that helps to analyze learning analytics applications for these kinds of users. We then present our own work in this area and compare with 15 related dashboard applications for learning. Most evaluations evaluate only part of our conceptual framework and do not assess whether dashboards contribute to behavior change or new understanding, probably also because such assessment requires longitudinal studies.",
    url = "https://doi.org/10.1177/0002764213479363"
}

@article{ref1_683190,
    author = "Matcha, Wannisa and Uzir, Nora’ayu and Gašević, Dragan and Pardo, Abelardo",
    title = "A Systematic Review of Empirical Studies on Learning Analytics Dashboards: A Self-Regulated Learning Perspective",
    year = "2019",
    issn = "1939-1382",
    doi = "10.1109/tlt.2019.2916802",
    url = "https://doi.org/10.1109/tlt.2019.2916802"
}

@article{ref2_68d9a2,
    author = "Park, Yeonjeong and Jo, Il",
    title = "Development of the Learning Analytics Dashboard to Support Students' Learning Performance",
    year = "2015",
    doi = "10.3217/jucs-021-01-0110",
    url = "https://doi.org/10.3217/jucs-021-01-0110"
}

@article{ref3_6eeab8,
    author = "Bodily, Robert and Verbert, Katrien",
    title = "Review of Research on Student-Facing Learning Analytics Dashboards and Educational Recommender Systems",
    year = "2017",
    issn = "1939-1382",
    doi = "10.1109/tlt.2017.2740172",
    abstract = "This article is a comprehensive literature review of student-facing learning analytics reporting systems that track learning analytics data and report it directly to students. This literature review builds on four previously conducted literature reviews in similar domains. Out of the 945 articles retrieved from databases and journals, 93 articles were included in the analysis. Articles were coded based on the following five categories: functionality, data sources, design analysis, student perceptions, and measured effects. Based on this review, we need research on learning analytics reporting systems that targets the design and development process of reporting systems, not only the final products. This design and development process includes needs analyses, visual design analyses, information selection justifications, and student perception surveys. In addition, experiments to determine the effect of these systems on student behavior, achievement, and skills are needed to add to the small existing body of evidence. Furthermore, experimental studies should include usability tests and methodologies to examine student use of these systems, as these factors may affect experimental findings. Finally, observational study methods, such as propensity score matching, should be used to increase student access to these systems but still rigorously measure experimental effects.",
    url = "https://doi.org/10.1109/tlt.2017.2740172"
}

@article{ref4_5265b9,
    author = "Bodily, Robert and Kay, Judy and Aleven, Vincent and Jivet, Ioana and Davis, Dan and Xhakaj, Françeska and Verbert, Katrien",
    title = "Open learner models and learning analytics dashboards",
    year = "2018",
    doi = "10.1145/3170358.3170409",
    abstract = "This paper aims to link student facing Learning Analytics Dashboards (LADs) to the corpus of research on Open Learner Models (OLMs), as both have similar goals. We conducted a systematic review of literature on OLMs and compared the results with a previously conducted review of LADs for learners in terms of (i) data use and modelling, (ii) key publication venues, (iii) authors and articles, (iv) key themes, and (v) system evaluation. We highlight the similarities and differences between the research on LADs and OLMs. Our key contribution is a bridge between these two areas as a foundation for building upon the strengths of each. We report the following key results from the review: in reports of new OLMs, almost 60\% are based on a single type of data; 33\% use behavioral metrics; 39\% support input from the user; 37\% have complex models; and just 6\% involve multiple applications. Key associated themes include intelligent tutoring systems, learning analytics, and self-regulated learning. Notably, compared with LADs, OLM research is more likely to be interactive (81\% of papers compared with 31\% for LADs), report evaluations (76\% versus 59\%), use assessment data (100\% versus 37\%), provide a comparison standard for students (52\% versus 38\%), but less likely to use behavioral metrics, or resource use data (33\% against 75\% for LADs). In OLM work, there was a heightened focus on learner control and access to their own data.",
    url = "https://doi.org/10.1145/3170358.3170409"
}

@article{ref5_c9f8cd,
    author = "Jivet, Ioana and Scheffel, Maren and Drachsler, Hendrik and Specht, Marcus",
    title = "Awareness Is Not Enough: Pitfalls of Learning Analytics Dashboards in the Educational Practice",
    year = "2017",
    issn = "0302-9743",
    doi = "10.1007/978-3-319-66610-5\_7",
    url = "https://doi.org/10.1007/978-3-319-66610-5\_7"
}

@article{ref6_591f6d,
    author = "Verbert, Katrien and Ochôa, Xavier and De Croon, Robin and Dourado, Raphael and De Laet, Tinne",
    title = "Learning analytics dashboards",
    year = "2020",
    doi = "10.1145/3375462.3375504",
    abstract = {Learning analytics dashboards are at the core of the LAK vision to involve the human into the decision-making process. The key focus of these dashboards is to support better human sense-making and decision-making by visualising data about learners to a variety of stakeholders. Early research on learning analytics dashboards focused on the use of visualisation and prediction techniques and demonstrates the rich potential of dashboards in a variety of learning settings. Present research increasingly uses participatory design methods to tailor dashboards to the needs of stakeholders, employs multimodal data acquisition techniques, and starts to research theoretical underpinnings of dashboards. In this paper, we present these past and present research efforts as well as the results of the VISLA19 workshop on "Visual approaches to Learning Analytics" that was held at LAK19 with experts in the domain to identify and articulate common practices and challenges for the domain. Based on an analysis of the results, we present a research agenda to help shape the future of learning analytics dashboards.},
    url = "https://doi.org/10.1145/3375462.3375504"
}

@article{ref7_1a9ad9,
    author = "Roberts, Lynne and Howell, Joel and Seaman, Kristen",
    title = "Give Me a Customizable Dashboard: Personalized Learning Analytics Dashboards in Higher Education",
    year = "2017",
    issn = "2211-1662",
    doi = "10.1007/s10758-017-9316-1",
    url = "https://doi.org/10.1007/s10758-017-9316-1"
}

@article{ref8_d505cb,
    author = "Vela, Francisco and Seipp, Karsten and Ochôa, Xavier and Chiluiza, Katherine and De Laet, Tinne and Verbert, Katrien",
    title = "LADA: A learning analytics dashboard for academic advising",
    year = "2018",
    issn = "0747-5632",
    doi = "10.1016/j.chb.2018.12.004",
    url = "https://doi.org/10.1016/j.chb.2018.12.004"
}

@article{ref9_1c3ff3,
    author = "Kim, Jeonghyun and Jo, Il-Hyun and Park, Yeonjeong",
    title = "Effects of learning analytics dashboard: analyzing the relations among dashboard utilization, satisfaction, and learning achievement",
    year = "2015",
    issn = "1598-1037",
    doi = "10.1007/s12564-015-9403-8",
    url = "https://doi.org/10.1007/s12564-015-9403-8"
}

@article{ref10_bebacc,
    author = "Charleer, Sven and Moere, Andrew and Klerkx, Joris and Verbert, Katrien and De Laet, Tinne",
    title = "Learning Analytics Dashboards to Support Adviser-Student Dialogue",
    year = "2017",
    issn = "1939-1382",
    doi = "10.1109/tlt.2017.2720670",
    url = "https://doi.org/10.1109/tlt.2017.2720670"
}

@article{ref11_b4037c,
    author = "Sedrakyan, Gayane and Malmberg, Jonna and Verbert, Katrien and Järvelä, Sanna and Kirschner, Paul",
    title = "Linking learning behavior analytics and learning science concepts: Designing a learning analytics dashboard for feedback to support learning regulation",
    year = "2018",
    issn = "0747-5632",
    doi = "10.1016/j.chb.2018.05.004",
    url = "https://doi.org/10.1016/j.chb.2018.05.004"
}

@article{ref12_124a75,
    author = "Klerkx, Joris and Verbert, Katrien and Duval, Erik",
    title = "Learning Analytics Dashboards",
    year = "2017",
    doi = "10.18608/hla17.012",
    abstract = "BACKGROUNDChapter 12: Learning Analytics Dashboards guidelines on how to get started with the development of learning analytics dashboards are presented for practitioners and researchers.",
    url = "https://doi.org/10.18608/hla17.012"
}

@article{ref13_cc942e,
    author = "Sušnjak, Teo and Ramaswami, Gomathy and Mathrani, Anuradha",
    title = "Learning analytics dashboard: a tool for providing actionable insights to learners",
    year = "2022",
    issn = "2365-9440",
    doi = "10.1186/s41239-021-00313-7",
    abstract = "Abstract This study investigates current approaches to learning analytics (LA) dashboarding while highlighting challenges faced by education providers in their operationalization. We analyze recent dashboards for their ability to provide actionable insights which promote informed responses by learners in making adjustments to their learning habits. Our study finds that most LA dashboards merely employ surface-level descriptive analytics, while only few go beyond and use predictive analytics. In response to the identified gaps in recently published dashboards, we propose a state-of-the-art dashboard that not only leverages descriptive analytics components, but also integrates machine learning in a way that enables both predictive and prescriptive analytics. We demonstrate how emerging analytics tools can be used in order to enable learners to adequately interpret the predictive model behavior, and more specifically to understand how a predictive model arrives at a given prediction. We highlight how these capabilities build trust and satisfy emerging regulatory requirements surrounding predictive analytics. Additionally, we show how data-driven prescriptive analytics can be deployed within dashboards in order to provide concrete advice to the learners, and thereby increase the likelihood of triggering behavioral changes. Our proposed dashboard is the first of its kind in terms of breadth of analytics that it integrates, and is currently deployed for trials at a higher education institution.",
    url = "https://doi.org/10.1186/s41239-021-00313-7"
}

@article{ref14_bf06d8,
    author = "Han, Jeongyun and Kim, Kwan and Rhee, Wonjong and Cho, Young",
    title = "Learning analytics dashboards for adaptive support in face-to-face collaborative argumentation",
    year = "2020",
    issn = "0360-1315",
    doi = "10.1016/j.compedu.2020.104041",
    abstract = "Despite the potential of learning analytics for personalized learning, it is seldom used to support collaborative learning particularly in face-to-face (F2F) learning contexts. This study uses learning analytics to develop a dashboard system that provides adaptive support for F2F collaborative argumentation (FCA). This study developed two dashboards for students and instructors, which enabled students to monitor their FCA process through adaptive feedback and helped the instructor provide adaptive support at the right time. The effectiveness of the dashboards was examined in a university class with 88 students (56 females, 32 males) for 4 weeks. The dashboards significantly improved the FCA process and outcomes, encouraging students to actively participate in FCA and create high-quality arguments. Students had a positive attitude toward the dashboard and perceived it as useful and easy to use. These findings indicate the usefulness of learning analytics dashboards in improving collaborative learning through adaptive feedback and support. Suggestions are provided on how to design dashboards for adaptive support in F2F learning contexts using learning analytics.",
    url = "https://doi.org/10.1016/j.compedu.2020.104041"
}

@article{ref15_335c3a,
    author = "Rienties, Bart and Herodotou, Christothea and Olney, Tom and Schencks, Mat and Boroowa, Avi",
    title = "Making Sense of Learning Analytics Dashboards: A Technology Acceptance Perspective of 95 Teachers",
    year = "2018",
    issn = "1492-3831",
    doi = "10.19173/irrodl.v19i5.3493",
    abstract = "The importance of teachers in online learning is widely acknowledged to effectively support and stimulate learners. With the increasing availability of learning analytics data, online teachers might be able to use learning analytics dashboards to facilitate learners with different learning needs. However, deployment of learning analytics visualisations by teachers also requires buy-in from teachers. Using the principles of technology acceptance model, in this embedded case-study, we explored teachers’ readiness for learning analytics visualisations amongst 95 experienced teaching staff at one of the largest distance learning universities by using an innovative training method called Analytics4Action Workshop. The findings indicated that participants appreciated the interactive and hands-on approach, but at the same time were skeptical about the perceived ease of use of learning analytics tools they were offered. Most teachers indicated a need for additional training and follow-up support for working with learning analytics tools. Our results highlight a need for institutions to provide effective professional development opportunities for learning analytics.",
    url = "https://doi.org/10.19173/irrodl.v19i5.3493"
}

@article{ref16_cffd1c,
    author = "Ahn, June and Campos, Fabio and Hays, Maria and DiGiacomo, Daniela",
    title = "Designing in Context: Reaching Beyond Usability in Learning Analytics Dashboard Design",
    year = "2019",
    issn = "1929-7750",
    doi = "10.18608/jla.2019.62.5",
    abstract = "Researchers and developers of learning analytics (LA) systems are increasingly adopting human-centred design (HCD) approaches, with growing need to understand how to apply design practice in different educational settings. In this paper, we present a design narrative of our experience developing dashboards to support middle school mathematics teachers’ pedagogical practices, in a multi-university, multi-school district, improvement science initiative in the United States. Through documentation of our design experience, we offer ways to adapt common HCD methods — contextual design and design tensions — when developing visual analytics systems for educators. We also illuminate how adopting these design methods within the context of improvement science and research–practice partnerships fundamentally influences the design choices we make and the focal questions we undertake. The results of this design process flow naturally from the appropriation and repurposing of tools by district partners and directly inform improvement goals.",
    url = "https://doi.org/10.18608/jla.2019.62.5"
}

@article{ref17_9d1439,
    author = "Vozniuk, Andrii and Govaerts, Sten and Gillet, Denis",
    title = "Towards Portable Learning Analytics Dashboards",
    year = "2013",
    doi = "10.1109/icalt.2013.126",
    abstract = "This paper proposes a novel approach to build and deploy learning analytics dashboards in multiple learning environments. Existing learning dashboards are barely portable: once deployed on a learning platform, it requires considerable effort to deploy the dashboard elsewhere. We suggest constructing dashboards from lightweight web applications, namely widgets. Our approach allows to port dashboards with no additional cost between learning environments that implement open specifications (Open Social and Activity Streams) for data access and use widget APIs. We propose to facilitate reuse by sharing the dashboards and widgets via a centralized analytics repository.",
    url = "https://doi.org/10.1109/icalt.2013.126"
}

@article{ref18_bd1ee2,
    author = "Aljohani, Naif and Daud, Ali and Abbasi, Rabeeh and Alowibdi, Jalal and Basheri, Mohammad and Aslam, Muhammad",
    title = "An integrated framework for course adapted student learning analytics dashboard",
    year = "2018",
    issn = "0747-5632",
    doi = "10.1016/j.chb.2018.03.035",
    url = "https://doi.org/10.1016/j.chb.2018.03.035"
}

@article{ref19_ad00de,
    author = "Park, Yeonjeong and Jo, Il‐Hyun",
    title = "Factors that affect the success of learning analytics dashboards",
    year = "2019",
    issn = "1042-1629",
    doi = "10.1007/s11423-019-09693-0",
    url = "https://doi.org/10.1007/s11423-019-09693-0"
}

@article{ref20_6ddd18,
    author = "Bodily, Robert and Ikahihifo, Tarah and Mackley, Benjamin and Graham, Charles",
    title = "The design, development, and implementation of student-facing learning analytics dashboards",
    year = "2018",
    issn = "1042-1726",
    doi = "10.1007/s12528-018-9186-0",
    url = "https://doi.org/10.1007/s12528-018-9186-0"
}

@article{ref21_8bd0d1,
    author = "Aguilar, Stephen and Karabenick, Stuart and Teasley, Stephanie and Baek, Clare",
    title = "Associations between learning analytics dashboard exposure and motivation and self-regulated learning",
    year = "2020",
    issn = "0360-1315",
    doi = "10.1016/j.compedu.2020.104085",
    url = "https://doi.org/10.1016/j.compedu.2020.104085"
}

@article{ref22_bc55dd,
    author = "Lim, Lisa-Angelique and Dawson, Shane and Joksimovíc, Srécko and Gašević, Dragan",
    title = "Exploring students' sensemaking of learning analytics dashboards",
    year = "2019",
    doi = "10.1145/3303772.3303804",
    abstract = "Learning Analytics Dashboards (LAD) are becoming an increasingly popular way to provide students with personalised feedback. Despite the number of LADs being developed, significant research gaps exist around the student perspective, especially how students make sense of graphics provided in LADs, and how they intend to act on the feedback provided therein. This study employed a randomized-controlled trial to examine students' sense-making of LADs showing four different frames of reference, and to what extent the impact of LADs was mediated by baseline self-regulation. Using a mix of quantitative and qualitative data analysis, the results revealed rather distinct patterns in students' sense-making across the four LADs. These patterns involved the intersection of visual salience and planned learning actions. However, collectively, across all four LADs a consistent theme emerged around students planned learning actions. This theme was classified as time and study environment management. A key finding of the study is that the use of LADs as a primary feedback process should be personalized and include training and support to aid student sensemaking.",
    url = "https://doi.org/10.1145/3303772.3303804"
}

@article{ref23_5b5a5e,
    author = "Sun, Kaiwen and Mhaidli, Abraham and Watel, Sonakshi and Brooks, Christopher and Schaub, Florian",
    title = "It's My Data! Tensions Among Stakeholders of a Learning Analytics Dashboard",
    year = "2019",
    doi = "10.1145/3290605.3300824",
    url = "https://doi.org/10.1145/3290605.3300824"
}

@article{ref24_987da6,
    author = "Charleer, Sven and Klerkx, Joris and Duval, Erik and De Laet, Tinne and Verbert, Katrien",
    title = "Creating Effective Learning Analytics Dashboards: Lessons Learnt",
    year = "2016",
    issn = "0302-9743",
    doi = "10.1007/978-3-319-45153-4\_4",
    url = "https://doi.org/10.1007/978-3-319-45153-4\_4"
}

@article{ref25_bc78dd,
    author = "Valle, Natercia and Antonenko, Pavlo and Dawson, Kara and Huggins‐Manley, Anne",
    title = "Staying on target: A systematic literature review on learner‐facing learning analytics dashboards",
    year = "2021",
    issn = "0007-1013",
    doi = "10.1111/bjet.13089",
    url = "https://doi.org/10.1111/bjet.13089"
}

@article{ref26_bba23a,
    author = "Corrin, Linda and De Barba, Paula",
    title = "Exploring students’ interpretation of feedback delivered through learning analytics dashboards",
    year = "2014",
    issn = "2653-665X",
    doi = "10.14742/apubs.2014.1300",
    abstract = "The delivery of feedback to students through learning analytics dashboards is becoming more common in higher education. However, it is not clear what ability students have to interpret this feedback in ways that will benefit their learning. This paper presents the preliminary results of a mixed methods study into students’ interpretation of feedback delivered through learning analytics dashboards and the influence this feedback has on students’ self-regulated learning. The findings from a preliminary analysis of the data from the first two phases will be discussed and the future phases of the research outlined. The outcomes of this research provide new insights into how dashboards can be designed to provide effective feedback in blended learning environments.",
    url = "https://doi.org/10.14742/apubs.2014.1300"
}

@article{ref27_9515e4,
    author = "Aljohani, Naif and Davis, Hugh",
    title = "Learning Analytics and Formative Assessment to Provide Immediate Detailed Feedback Using a Student Centered Mobile Dashboard",
    year = "2013",
    doi = "10.1109/ngmast.2013.54",
    abstract = "The 'immediacy' of feedback on academic performance is a common characteristic shared by both Learning Analytics (LA) and Formative Assessment (FA), and such immediacy could be facilitated by supporting the mobility of learners. However, there is little literature that investigates the significance of combining these two techniques. Therefore, this paper will discuss the analytical application called Quiz My Class Understanding (QMCU) which was purposely developed to investigate the significance of the combination between LA and FA techniques in order to provide students with immediate detailed feedback. Furthermore, it reports on a case study which reflects the role QMCU students' centered mobile dashboard in increasing the students' engagement with the QMCU dashboard.",
    url = "https://doi.org/10.1109/ngmast.2013.54"
}

@article{ref28_1abb81,
    author = "Martínez‐Maldonado, Roberto",
    title = "A handheld classroom dashboard: Teachers’ perspectives on the use of real-time collaborative learning analytics",
    year = "2019",
    issn = "1556-1607",
    doi = "10.1007/s11412-019-09308-z",
    url = "https://doi.org/10.1007/s11412-019-09308-z"
}

@article{ref29_50b452,
    author = "Naranjo, Diana and Prieto, José and Moltó, Germán and Calatrava, Amanda",
    title = "A Visual Dashboard to Track Learning Analytics for Educational Cloud Computing",
    year = "2019",
    issn = "1424-8220",
    doi = "10.3390/s19132952",
    abstract = "Cloud providers such as Amazon Web Services (AWS) stand out as useful platforms to teach distributed computing concepts as well as the development of Cloud-native scalable application architectures on real-world infrastructures. Instructors can benefit from high-level tools to track the progress of students during their learning paths on the Cloud, and this information can be disclosed via educational dashboards for students to understand their progress through the practical activities. To this aim, this paper introduces CloudTrail-Tracker, an open-source platform to obtain enhanced usage analytics from a shared AWS account. The tool provides the instructor with a visual dashboard that depicts the aggregated usage of resources by all the students during a certain time frame and the specific use of AWS for a specific student. To facilitate self-regulation of students, the dashboard also depicts the percentage of progress for each lab session and the pending actions by the student. The dashboard has been integrated in four Cloud subjects that use different learning methodologies (from face-to-face to online learning) and the students positively highlight the usefulness of the tool for Cloud instruction in AWS. This automated procurement of evidences of student activity on the Cloud results in close to real-time learning analytics useful both for semi-automated assessment and student self-awareness of their own training progress.",
    url = "https://doi.org/10.3390/s19132952"
}

@article{ref30_40f9d7,
    author = "Zheng, Juan and Huang, Lingyun and Li, Shan and Lajoie, Susanne and Chen, Yuxin and Hmelo‐Silver, Cindy",
    title = "Self-regulation and emotion matter: A case study of instructor interactions with a learning analytics dashboard",
    year = "2020",
    issn = "0360-1315",
    doi = "10.1016/j.compedu.2020.104061",
    url = "https://doi.org/10.1016/j.compedu.2020.104061"
}

@article{ref31_893953,
    author = "Ramos-Soto, Alejandro and Lama, Manuel and Vázquez-Barreiros, Borja and Bugarín, Alberto and Barro, M.",
    title = "Towards Textual Reporting in Learning Analytics Dashboards",
    year = "2015",
    doi = "10.1109/icalt.2015.96",
    url = "https://doi.org/10.1109/icalt.2015.96"
}

@article{ref32_4302e6,
    author = "Rets, Irina and Herodotou, Christothea and Bayer, Vaclav and Hlosta, Martin and Rienties, Bart",
    title = "Exploring critical factors of the perceived usefulness of a learning analytics dashboard for distance university students",
    year = "2021",
    issn = "2365-9440",
    doi = "10.1186/s41239-021-00284-9",
    abstract = "Learning analytics dashboards (LADs) can provide learners with insights about their study progress through visualisations of the learner and learning data. Despite their potential usefulness to support learning, very few studies on LADs have considered learners' needs and have engaged learners in the process of design and evaluation. Aligning with that, there is a limited understanding of what specific student cohorts, in particular distance and online learners, may seek from LADs to effectively support their studies. In this study, we present findings from 21 interviews with undergraduate distance learners, mainly high performers, that aimed to capture student perceptions about the usefulness of specific LAD features and the factors that explain these perceptions. Our findings revealed that amongst the LAD features favoured by students was the potential to receive study recommendations, whereas comparison with peers was amongst the least favoured elements, unless informed by qualitative information. Factors including information trust, attitudes, age, performance and academic self-confidence were found to explain these perceptions.",
    url = "https://doi.org/10.1186/s41239-021-00284-9"
}

@article{ref33_ac15e8,
    author = "Chen, Li and Lu, Min and Goda, Yoshiko and Yamada, Masanori",
    title = "DESIGN OF LEARNING ANALYTICS DASHBOARD SUPPORTING METACOGNITION",
    year = "2019",
    doi = "10.33965/celda2019\_201911l022",
    abstract = "Metacognition is an aspect in self-regulated learning and is necessary to achieve such learning in an effective andefficient manner. However, it is not always easy and accurate for learners to monitor or assess their own metacognition.In this",
    url = "https://doi.org/10.33965/celda2019\_201911l022"
}

@article{ref34_95cae8,
    author = "Tan, Jennifer and Koh, Elizabeth and Jonathan, Christin and Yang, Simon",
    title = "Learner Dashboards a Double-Edged Sword? Students’ Sense-Making of a Collaborative Critical Reading and Learning Analytics Environment for Fostering 21st Century Literacies",
    year = "2017",
    issn = "1929-7750",
    doi = "10.18608/jla.2017.41.7",
    abstract = "The affordances of learning analytics (LA) tools and solutions are being increasingly harnessed for enhancing 21st century pedagogical and learning strategies and outcomes. However, use cases and empirical understandings of students’ experiences with LA tools and environments aimed at fostering 21st century literacies, especially in the K-12 schooling sector and in Asian education contexts remain relatively scarce in the field. Our paper addresses this knowledge gap in two ways. First, we present a first iteration design of a computer-supported collaborative critical reading and LA environment, WiREAD, and its 16-week implementation in a Singapore high school. Second, we foreground students’ evaluative accounts of the benefits and drawbacks associated with this techno-pedagogical innovation. Our analysis of students’ collective sense-making pointed to a number of potentialities and perils associated with the design and use of LA dashboards. Positives included (1) fostering greater self-awareness, reflective and self-regulatory learning dispositions, (2) enhancing learning motivation and engagement, and (3) nurturing connective literacy among students. The motivational value of peer-referenced LA visualisations for stimulating healthy competition and game-like learning was identified, alongside the perils of these serving to demoralise, pressurise and trigger complacency in learners. By focusing on students’ experiences and interpretations of how the LA dashboard visualizations impacted their learning motivation and outcomes, this paper aims to shed insights into the pedagogical complexities of designing LA that considers the voices of learners as a critical stakeholder group.",
    url = "https://doi.org/10.18608/jla.2017.41.7"
}

@article{ref35_a0fec9,
    author = "Wang, Dongqing and Han, Hou",
    title = "Applying learning analytics dashboards based on process‐oriented feedback to improve students' learning effectiveness",
    year = "2020",
    issn = "0266-4909",
    doi = "10.1111/jcal.12502",
    url = "https://doi.org/10.1111/jcal.12502"
}

@article{ref36_4d5fb4,
    author = "Şahi̇n, Muhittin and Ifenthaler, Dirk",
    title = "Visualizations and Dashboards for Learning Analytics",
    year = "2021",
    issn = "2662-2122",
    doi = "10.1007/978-3-030-81222-5",
    url = "https://doi.org/10.1007/978-3-030-81222-5"
}

@article{ref37_598ed6,
    author = "Williamson, Kimberly and Kizilcec, René",
    title = "A Review of Learning Analytics Dashboard Research in Higher Education: Implications for Justice, Equity, Diversity, and Inclusion",
    year = "2022",
    doi = "10.1145/3506860.3506900",
    url = "https://doi.org/10.1145/3506860.3506900"
}

@article{ref38_485a2b,
    author = "De Laet, Tinne and Millecamp, Martijn and Ortiz‐Rojas, Margarita and Jiménez, Alberto and Maya, Ricardo and Verbert, Katrien",
    title = "Adoption and impact of a learning analytics dashboard supporting the advisor—Student dialogue in a higher education institute in Latin America",
    year = "2020",
    issn = "0007-1013",
    doi = "10.1111/bjet.12962",
    abstract = "This paper presents a case study on the adoption and the impact of new modules in a learning analytics dashboard supporting the dialogue between student advisors and students when advising on a study plan for the next academic semester in Escuela Superior Politecnica del Litoral, a higher education institute in Ecuador. The impact and the adoption of the new dashboard modules were assessed using a mixed‐methods approach. The quantitative approach builds on data of 172 advisors in 34 programs and 4481 advising sessions in 2019 (post) and 4747 advising sessions in 2018 (pre) to assess the adoption and use of the dashboard, the level of support experienced by the advisors, the impact of the new dashboard modules on the difference between the advised study plan and the plan students register for, and students’ academic achievement. The qualitative approach with observations of 14 staged advising dialogues and semi‐structured interviews with eight advisors was used to assess how the dashboard was used and to get deeper understanding of the perceived usefulness and impact of the dashboard. The results show that an institution‐wide deployment of dashboard modules tailored to the needs of the advisors can be achieved and can increase the level of support perceived by the advisors and significantly decrease the gap between the suggested study plans in advising dialogues and the study plans that students actually register for. On the short‐term, however, no significant changes in academic achievement were observed. Practitioner Notes What is already known about this topic? Academic advising can positively impact retention, academic achievement and study completion. Learning analytics dashboards are promising pieces of educational technology for academic advising as they can trigger reflection and sense‐making of educational data. Evaluation of learning analytics dashboards is often still immature and not well‐connected to the actual goals of the dashboards. Large‐scale evaluations looking at impact of dashboards are even scarcer. What this paper adds? This paper adds, to the scarce scientific evidence on academic advising dashboards, a large‐scale case study on a dashboard supporting the advisor student dialogue during the composition of well‐balanced study plans. The paper presents research evidence of the impact of the dashboard on the support advisors experience, the study plans suggested by the advisors and the ones actually registered by the students and students’ academic achievement. Evidence is based on a quantitative analysis, using data of 172 advisors from 34 programs representing more than 9000 advising dialogues, and a qualitative analysis using observations and interviews. Implications for practice and/or policy Dashboards to support academic advising dialogues can be realized institution‐wide at scale. Training of student advisors supports a large scale deployment. Well‐designed dashboards that focus on addressing needs of advisors increase the level of support that advisor experience when advising students. Dashboard accommodating the simulation of study plans and the workload associated with them, succeed in decreasing the variance in suggested plans between advisors and reduce the gap between the study plans that advisors suggest to student and the study plans that students actually register for. Short‐term impact on academic achievement was not observed.",
    url = "https://doi.org/10.1111/bjet.12962"
}

@article{ref39_e7151a,
    author = "Dourado, Raphael and Rodrigues, Rodrigo and Ferreira, Nivan and Mello, Rafael and Gomes, Alex and Verbert, Katrien",
    title = "A Teacher-facing Learning Analytics Dashboard for Process-oriented Feedback in Online Learning",
    year = "2021",
    doi = "10.1145/3448139.3448187",
    abstract = "In online learning, teachers need constant feedback about their students' progress and regulation needs. Learning Analytics Dashboards for process-oriented feedback can be a valuable tool for this purpose. However, few such dashboards have been proposed in literature, and most of them lack empirical validation or grounding in learning theories. We present a teacher-facing dashboard for process-oriented feedback in online learning, co-designed and evaluated through an iterative design process involving teachers and visualization experts. We also reflect on our design process by discussing the challenges, pitfalls, and successful strategies for building this type of dashboard.",
    url = "https://doi.org/10.1145/3448139.3448187"
}

@article{ref40_ba722d,
    author = "Ramaswami, Gomathy and Sušnjak, Teo and Mathrani, Anuradha and Umer, Rahila",
    title = "Use of Predictive Analytics within Learning Analytics Dashboards: A Review of Case Studies",
    year = "2022",
    issn = "2211-1662",
    doi = "10.1007/s10758-022-09613-x",
    abstract = "Abstract Learning analytics dashboards (LADs) provide educators and students with a comprehensive snapshot of the learning domain. Visualizations showcasing student learning behavioral patterns can help students gain greater self-awareness of their learning progression, and at the same time assist educators in identifying those students who may be facing learning difficulties. While LADs have gained popularity, existing LADs are still far behind when it comes to employing predictive analytics into their designs. Our systematic literature review has revealed limitations in the utilization of predictive analytics tools among existing LADs. We find that studies leveraging predictive analytics only go as far as identifying the at-risk students and do not employ model interpretation or explainability capabilities. This limits the ability of LADs to offer data-driven prescriptive advice to students that can offer them guidance on appropriate learning adjustments. Further, published studies have mostly described LADs that are still at prototype stages; hence, robust evaluations of how LADs affect student outcomes have not yet been conducted. The evaluations until now are limited to LAD functionalities and usability rather than their effectiveness as a pedagogical treatment. We conclude by making recommendations for the design of advanced dashboards that more fully take advantage of machine learning technologies, while using suitable visualizations to project only relevant information. Finally, we stress the importance of developing dashboards that are ultimately evaluated for their effectiveness.",
    url = "https://doi.org/10.1007/s10758-022-09613-x"
}

@article{ref41_077f31,
    author = "Revano, Teodoro and Garcia, Manuel",
    title = "Designing Human-Centered Learning Analytics Dashboard for Higher Education Using a Participatory Design Approach",
    year = "2021",
    doi = "10.1109/hnicem54116.2021.9731917",
    url = "https://doi.org/10.1109/hnicem54116.2021.9731917"
}

@article{ref42_f885d1,
    author = "Zamecnik, Andrew and Kovanović, Vitomir and Großmann, Georg and Joksimovíc, Srécko and Jolliffe, Gabrielle and Gibson, David and Pardo, Abelardo",
    title = "Team interactions with learning analytics dashboards",
    year = "2022",
    issn = "0360-1315",
    doi = "10.1016/j.compedu.2022.104514",
    url = "https://doi.org/10.1016/j.compedu.2022.104514"
}

@article{ref43_99f33f,
    author = "Nieto, Gloria and Kitto, Kirsty and Shum, Simon and Martínez‐Maldonado, Roberto",
    title = "Beyond the Learning Analytics Dashboard: Alternative Ways to Communicate Student Data Insights Combining Visualisation, Narrative and Storytelling",
    year = "2022",
    doi = "10.1145/3506860.3506895",
    url = "https://doi.org/10.1145/3506860.3506895"
}

@article{ref44_5d0215,
    author = "Ramos-Soto, Alejandro and Vázquez-Barreiros, Borja and Bugarín, Alberto and Barujel, Adriana and Barro, Senén",
    title = "Evaluation of a Data-To-Text System for Verbalizing a Learning Analytics Dashboard",
    year = "2016",
    issn = "0884-8173",
    doi = "10.1002/int.21835",
    abstract = "The SoftLearn Activity Reporter is a data-to-text service, which automatically generates textual reports about the activity developed by students within the SoftLearn virtual learning environment. In this paper, we describe the conception of the service, its architecture, and its subsequent evaluation by an expert pedagogue, where 20 full reports generated from real data from an undergraduate course supported by the SoftLearn platform were assessed. Results show that the automatically generated reports are a valuable complementary tool for explaining teachers and students the information comprised in a learning analytics dashboard.",
    url = "https://doi.org/10.1002/int.21835"
}

@article{ref45_b6b941,
    author = "Şahi̇n, Muhittin and Ifenthaler, Dirk",
    title = "Visualizations and Dashboards for Learning Analytics: A Systematic Literature Review",
    year = "2021",
    issn = "2662-2122",
    doi = "10.1007/978-3-030-81222-5\_1",
    url = "https://doi.org/10.1007/978-3-030-81222-5\_1"
}

@article{ref46_825d47,
    author = "Kaliisa, Rogers and Jivet, Ioana and Prinsloo, Paul",
    title = "A checklist to guide the planning, designing, implementation, and evaluation of learning analytics dashboards",
    year = "2023",
    issn = "2365-9440",
    doi = "10.1186/s41239-023-00394-6",
    abstract = "Abstract Higher education institutions are moving to design and implement teacher-facing learning analytics (LA) dashboards with the hope that instructors can extract deep insights about student learning and make informed decisions to improve their teaching. While much attention has been paid to developing teacher-facing dashboards, less is known about how they are designed, implemented and evaluated. This paper presents a systematic literature review of existing studies reporting on teacher-facing LA dashboards. Out of the 1968 articles retrieved from several databases, 50 articles were included in the final analysis. Guided by several frameworks, articles were coded based on the following dimensions: purpose, theoretical grounding, stakeholder involvement, ethics and privacy, design, implementation, and evaluation criteria. The findings show that most dashboards are designed to increase teachers’ awareness but with limited actionable insights to allow intervention. Moreover, while teachers are involved in the design process, this is mainly at the exploratory/problem definition stage, with little input beyond this stage. Most dashboards were prescriptive, less customisable, and implicit about the theoretical constructs behind their designs. In addition, dashboards are deployed at prototype and pilot stages, and the evaluation is dominated by self-reports and users’ reactions with limited focus on changes to teaching and learning. Besides, only one study considered privacy as a design requirement. Based on the findings of the study and synthesis of existing literature, we propose a four-dimensional checklist for planning, designing, implementing and evaluating LA dashboards.",
    url = "https://doi.org/10.1186/s41239-023-00394-6"
}

@article{ref47_745fb3,
    author = "Brown, Michael",
    title = "Seeing students at scale: how faculty in large lecture courses act upon learning analytics dashboard data",
    year = "2020",
    issn = "1356-2517",
    doi = "10.1080/13562517.2019.1698540",
    url = "https://doi.org/10.1080/13562517.2019.1698540"
}

@article{ref48_017df5,
    author = "Da Ines, D. and Iksal, Sébastien and Jean-Marie, Gilliot and Madeth, May and Serge, Garlatti",
    title = "Towards Adaptive Dashboards for Learning Analytic - An Approach for Conceptual Design and Implementation",
    year = "2017",
    doi = "10.5220/0006325601200131",
    abstract = "Designing Learning Analytic (LA) dashboards can be a challenging and complex task when dealing with abundant data generated from heterogeneous sources with various uses.On top of that, each dashboard is designed in accordance with the user's needs and their observational objectives.Therefore, understanding the context of LA and its users is compulsory as it is part of the dashboard design approach.Our research effort starts with an exploratory study of different contextual elements that could help us define what an adaptive dashboard is and how it fulfills the user's needs.To do so, we have conducted a needs assessment to characterize the user profiles, their activities, their visualization preferences and objectives when using a dedicated dashboard.In this paper, we introduce a conceptual model, which will be used to generate a variety of LA dashboards.Our main goal is to provide users with adaptive dashboards, generated accordingly to their context of use while satisfying the users' requirements.We also discussed the implementation process of our first prototype as well as further improvements.",
    url = "https://doi.org/10.5220/0006325601200131"
}

@article{ref49_64e5dd,
    author = "Fleur, Damien and van den Bos, Wouter and Bredeweg, Bert",
    title = "Learning Analytics Dashboard for Motivation and Performance",
    year = "2020",
    issn = "0302-9743",
    doi = "10.1007/978-3-030-49663-0\_51",
    url = "https://doi.org/10.1007/978-3-030-49663-0\_51"
}

@article{ref50_90cda9,
    author = "Einhardt, Luan and Tavares, Tatiana and Cechinel, Cristian",
    title = "Moodle analytics dashboard: A learning analytics tool to visualize users interactions in moodle",
    year = "2016",
    doi = "10.1109/laclo.2016.7751805",
    url = "https://doi.org/10.1109/laclo.2016.7751805"
}

@article{ref51_4a725e,
    author = "de Freitas, Sara and Gibson, David and García, Víctor and Irving, Leah and Star, Kam and Charleer, Sven and Verbert, Katrien",
    title = "How to Use Gamified Dashboards and Learning Analytics for Providing Immediate Student Feedback and Performance Tracking in Higher Education",
    year = "2017",
    doi = "10.1145/3041021.3054175",
    url = "https://doi.org/10.1145/3041021.3054175"
}

@article{ref52_91525e,
    author = "Khosravi, Hassan and Shabaninejad, Shiva and Bakharia, Aneesha and Sadiq, Shazia and Indulska, Marta and Gašević, Dragan",
    title = "Intelligent Learning Analytics Dashboards: Automated Drill-Down Recommendations to Support Teacher Data Exploration",
    year = "2021",
    issn = "1929-7750",
    doi = "10.18608/jla.2021.7279",
    abstract = "Learning analytics dashboards commonly visualize data about students with the aim of helping students and educators understand and make informed decisions about the learning process. To assist with making sense of complex and multidimensional data, many learning analytics systems and dashboards have relied strongly on AI algorithms based on predictive analytics. While predictive models have been successful in many domains, there is an increasing realization of the inadequacies of using predictive models in decision-making tasks that affect individuals without human oversight. In this paper, we employ a suite of state-of-the-art algorithms, from the online analytics processing, data mining, and process mining domains, to present an alternative human-in-the-loop AI method to enable educators to identify, explore, and use appropriate interventions for subpopulations of students with the highest deviation in performance or learning process compared to the rest of the class. We demonstrate an application of our proposed approach in an existing learning analytics dashboard (LAD) and explore the recommended drill-downs in a course with 875 students. The demonstration provides an example of the recommendations from real course data and shows how recommendations can lead the user to interesting insights. Furthermore, we demonstrate how our approach can be employed to develop intelligent LADs.",
    url = "https://doi.org/10.18608/jla.2021.7279"
}

@article{ref53_50ae53,
    author = "Fleur, Damien and van den Bos, Wouter and Bredeweg, Bert",
    title = "Social comparison in learning analytics dashboard supporting motivation and academic achievement",
    year = "2023",
    issn = "2666-5573",
    doi = "10.1016/j.caeo.2023.100130",
    abstract = "A promising contribution of Learning Analytics is the presentation of a learner's own learning behaviour and achievements via dashboards, often in comparison to peers, with the goal of improving self-regulated learning. However, there is a lack of empirical evidence on the impact of these dashboards and few designs are informed by theory. Many dashboard designs struggle to translate awareness of learning processes into actual self-regulated learning. In this study we investigate a Learning Analytics dashboard based on existing evidence on social comparison to support motivation, metacognition and academic achievement. Motivation plays a key role in whether learners will engage in self-regulated learning in the first place. Social comparison can be a significant driver in increasing motivation. We performed two randomised controlled interventions in different higher-education courses, one of which took place online due to the COVID-19 pandemic. Students were shown their current and predicted performance in a course alongside that of peers with similar goal grades. The sample of peers was selected in a way to elicit slight upward comparison. We found that the dashboard successfully promotes extrinsic motivation and leads to higher academic achievement, indicating an effect of dashboard exposure on learning behaviour, despite an absence of effects on metacognition. These results provide evidence that carefully designed social comparison, rooted in theory and empirical evidence, can be used to boost motivation and performance. Our dashboard is a successful example of how social comparison can be implemented in Learning Analytics Dashboards.",
    url = "https://doi.org/10.1016/j.caeo.2023.100130"
}

@article{ref54_4b0d62,
    author = "Jayashanka, Rangana and Hettiarachchi, Enosha and Hewagamage, K.",
    title = "Technology Enhanced Learning Analytics Dashboard in Higher Education",
    year = "2022",
    issn = "1479-4403",
    doi = "10.34190/ejel.20.2.2189",
    abstract = "During the COVID-19 pandemic period, all the Sri Lankan universities delivered lectures in fully online mode using Virtual Learning Environments. In fully online mode, students cannot track their performance level, their progress in the course, and their performances compared to the rest of the class. This paper presents research work conducted at the University of Colombo School of Computing (UCSC), Sri Lanka, to solve the above problems and facilitate students learning in fully online and blended learning environments using Learning Analytics. The research objective is to design and create a Technology Enhanced Learning Analytics (TELA) dashboard for improving students’ motivation, engagement, and grades. The Design Science research strategy was followed to achieve the objectives of the research. Initially, a literature survey was conducted analyzing features and limitations in current Learning Analytic dashboards. Then, current Learning Analytic plugins for Moodle were studied to identify their drawbacks. Two surveys with 136 undergraduate students and interviews with 12 lecturers were conducted to determine required features of the TELA system. The system was designed as a Moodle Plugin. Finally, an evaluation of the system was done with third-year undergraduate students of the UCSC. The results showed that the TELA dashboard can improve students' motivation, engagement, and grades. As a result of the system, students could track their current progress and performance compared to the peers, which helps to improve their motivation to engage more in the course. Also, the increased engagement in the course enhances the student’s self-confidence since the student can see continuous improvement of his/her progress and performance which in turn improves the student’s grades.",
    url = "https://doi.org/10.34190/ejel.20.2.2189"
}

@article{ref55_e849d5,
    author = "Knight, David and Brozina, Cory and Stauffer, Éric and Frisina, Chris and Abel, Troy",
    title = "Developing a Learning Analytics Dashboard for Undergraduate Engineering Using Participatory Design",
    year = "2015",
    doi = "10.18260/p.23824",
    abstract = "Abstract Developing a Learning Analytics Dashboard for Undergraduate Engineering Using Participatory DesignA convergence of pressures has led researchers to seek innovative ways to measure and trackstudent learning outcomes and empirically identify the conditions that lead to their development.Learning analytics is an emerging field of inquiry that uses existing student traces to aggregateand illuminate student data through visualizations and dashboards in an attempt to improvelearning outcomes. Though there are currently efforts both in vendor and academic arenas to tryto understand the long-term learning and decision-making effects of such dashboards, thereappears to be a missed opportunity in the development of these dashboards in vivo using human-centered usability practices to develop these new tools for learning. Practices that select relevantdata traces and develop dashboards with learners instead of for learners may lead to strongerstudent self-efficacy, build on existing social learning theory, and benefit from perspectivesfound within human-centered design practices.Our interdisciplinary team of faculty and graduate students from engineering education,computer science, human computer interaction, human centered design, the learning sciences,and visual communications are following a mixed-methods, human-centered approach todashboard development that breaks new ground in learning analytics by involving the end usersthroughout the design and development process. In this paper we report findings from aparticipatory design session held with a group of eight engineering students enrolled in a firstyear general engineering course. The session's protocol was organized to gather the followinginformation from the participants: 1) defining success as a university student, 2) identifyingpotential data streams and information, 3) usefulness of peer benchmarking data, 4) credibilityand ethical issues with learning analytics, and 5) students' use of technology for learning.During the last third of the session, students split into teams and produced designs of a learningdashboard. The entire session was transcribed and coded by each member of our research teamusing NVivo, a qualitative data analysis software package.Results identified the features of learning dashboards that students deem necessary to spur theirinterest and engagement with the systems. Students repeatedly pointed to time elements as beingan important characteristic of a dashboard. They wanted to know how their personal time ontask related to their classmates' time on task and saw value in incorporating scheduling or timemanagement into the dashboard. As these first year students were still early in their programsand adjusting to life on their own, they also pointed to how such a dashboard especially couldhelp with that transition, perhaps more so than with their in-class learning. Students alsodescribed various opt-in scenarios in which they would feel comfortable with the institutionutilizing their individual data in a dashboard learning environment. Findings from this work willinform future participatory design sessions with students and faculty, help our team developinitial wireframes of a student-driven dashboard, and help determine the data traces that willreceive our focus as we carry out learning analytics quantitative modeling.",
    url = "https://doi.org/10.18260/p.23824"
}

@article{ref56_4eeedd,
    author = "Guerra, Julio and Ortiz‐Rojas, Margarita and Zúñiga‐Prieto, Miguel and Scheihing, Eliana and Jiménez, Alberto and Broos, Tom and De Laet, Tinne and Verbert, Katrien",
    title = "Adaptation and evaluation of a learning analytics dashboard to improve academic support at three Latin American universities",
    year = "2020",
    issn = "0007-1013",
    doi = "10.1111/bjet.12950",
    abstract = "Abstract Despite the success of academic advising dashboards in several higher educational institutions (HEI), these dashboards are still under‐explored in Latin American HEI's. To close this gap, three different Latin American universities adapted an existing advising dashboard, originally deployed at the KU Leuven to their own context. In all three cases, the context was the main ruling factor to these adaptations. In this paper, we describe these adaptions using a framework that focuses on four different elements of the context: Objectives, Stakeholders, Key moment and Interactions. Evaluation of the adapted dashboards in the three different Latin American universities is conducted through pilots. This evaluation shows the value of the dashboard approach in different contexts in terms of satisfaction, usefulness and impact in academic decision‐making and advising tasks. The main contribution of this paper is the systematic reporting of the adaptations to an academic advising dashboard and showing the value of an academic advising dashboard on academic decision‐making and advising tasks.",
    url = "https://doi.org/10.1111/bjet.12950"
}

@article{ref57_681c3c,
    author = "Pan, Zilong and Li, Chenglu and Liu, Min",
    title = "Learning Analytics Dashboard for Problem-based Learning",
    year = "2020",
    doi = "10.1145/3386527.3406751",
    url = "https://doi.org/10.1145/3386527.3406751"
}

@article{ref58_2222be,
    author = "Klein, Carrie and Lester, Jaime and Nguyen, Thien and Justen, Abigail and Rangwala, Huzefa and Johri, Aditya",
    title = "Student Sensemaking of Learning Analytics Dashboard Interventions in Higher Education",
    year = "2019",
    issn = "0047-2395",
    doi = "10.1177/0047239519859854",
    abstract = "An instrumental case study was conducted at a large, public research university in the mid-Atlantic region of the United States to understand undergraduate use of learning analytics dashboard (LAD) interventions. Eighty-one undergraduate students participated in focus groups. Scenario-based questions, modeled on current and future LAD interventions, were asked to understand student uses, perceptions, and reactions to these interfaces. Results indicate that student sensemaking is tied to data relevance, accuracy, and context. Further, trust in both data, especially predictive data, and in relationships, especially with faculty, are foundational to the sensemaking of LAD data and interventions. A model of student sensemaking of LAD interventions is provided.",
    url = "https://doi.org/10.1177/0047239519859854"
}

@article{ref59_d3cbf7,
    author = "Xin, Ong and Singh, Dalbir",
    title = "Development of Learning Analytics Dashboard based on Moodle Learning Management System",
    year = "2021",
    issn = "2156-5570",
    doi = "10.14569/ijacsa.2021.0120793",
    abstract = "Digitalization catalyzes drastic changes to a particular subject or area. Digitalization is an operational structure transformation process, such as in the educational domain. Digitalization in the academic field has brought the classroom to the users' fingertips with the prevalence of e-learning applications, learning management systems, etc. However, with the increasing number of digital learning platform users, educators find it hard to monitor their students' progress. Analytics that analyze data generated from the usage pattern of the users contribute to giving the educators an insight regarding the performance of their students. With that, they can apply early intervention and modification of their delivery method to suit the students' needs and, at the same time, increase the quality of the content. This study illustrates the development of a learning analytics dashboard that can improve learning outcomes for educators and students.",
    url = "https://doi.org/10.14569/ijacsa.2021.0120793"
}

@article{ref60_4ea8d3,
    author = "Kaliisa, Rogers and Dolonen, Jan",
    title = "CADA: a teacher-facing learning analytics dashboard to foster teachers’ awareness of students’ participation and discourse patterns in online discussions",
    year = "2022",
    issn = "2211-1662",
    doi = "10.1007/s10758-022-09598-7",
    abstract = "Abstract Despite the potential of learning analytics (LA) to support teachers’ everyday practice, its adoption has not been fully embraced due to the limited involvement of teachers as co-designers of LA systems and interventions. This is the focus of the study described in this paper. Following a design-based research (DBR) approach and guided by concepts from the socio-cultural perspective and human-computer interaction (HCI), we design, test, and evaluate a teacher-facing LA dashboard, the Canvas Discussion Analytics Dashboard (CADA), in real educational settings. The goal of this dashboard is to support teachers’ roles in online environments through insights into students’ participation and discourse patterns. We evaluate CADA through 10 in-depth interviews with university teachers to examine their experiences using CADA in seven blended undergraduate and graduate courses over a one-year period. The findings suggest that engaging teachers throughout the analytics tool design process and giving them control/agency over LA tools can favour their adoption in practice. Additionally, the alignment of dashboard metrics with relevant theoretical constructs allows teachers to monitor the learning designs and make course design changes on the fly. The teachers in this study emphasise the need for LA dashboards to provide actionable insights by moving beyond what things are towards how things should b e. This study has several contributions. First, we make an artefact contribution (e.g. CADA), an LA dashboard to support teachers with insights into students’ online discussions. Second, by leveraging theory, and working with the teachers to develop and implement a dashboard in authentic teaching environments, we make an empirical, theoretical and methodological contribution to the field of learning analytics and technology enhanced learning. We synthesise these through practical design and implementation considerations for researchers, dashboard developers, and higher education institutions.",
    url = "https://doi.org/10.1007/s10758-022-09598-7"
}

@article{ref61_f78a9e,
    author = "Valle, Natercia and Antonenko, Pavlo and Valle, Denis and Dawson, Kara and Huggins‐Manley, Anne and Baiser, Benjamin",
    title = "The influence of task-value scaffolding in a predictive learning analytics dashboard on learners' statistics anxiety, motivation, and performance",
    year = "2021",
    issn = "0360-1315",
    doi = "10.1016/j.compedu.2021.104288",
    url = "https://doi.org/10.1016/j.compedu.2021.104288"
}

@article{ref62_ca9eb9,
    author = "Shabaninejad, Shiva and Khosravi, Hassan and Indulska, Marta and Bakharia, Aneesha and Isaías, Pedro",
    title = "Automated insightful drill-down recommendations for learning analytics dashboards",
    year = "2020",
    doi = "10.1145/3375462.3375539",
    url = "https://doi.org/10.1145/3375462.3375539"
}

@article{ref63_984418,
    author = "Amo, Daniel and Alier, Marc and Guerrero, María",
    title = "The student’s progress snapshot a hybrid text and visual learning analytics dashboard",
    year = "2018",
    issn = "0949-149X"
}

@article{ref64_ada9c9,
    author = "Leitner, Philipp and Ebner, Martin",
    title = "Development of a Dashboard for Learning Analytics in Higher Education",
    year = "2017",
    issn = "0302-9743",
    doi = "10.1007/978-3-319-58515-4\_23",
    url = "https://doi.org/10.1007/978-3-319-58515-4\_23"
}

@article{ref65_c97134,
    author = "Cha, Hyunjin and Park, Taejung",
    title = "Applying and Evaluating Visualization Design Guidelines for a MOOC Dashboard to Facilitate Self-Regulated Learning Based on Learning Analytics",
    year = "2019",
    issn = "1976-7277",
    doi = "10.3837/tiis.2019.06.002",
    abstract = "With the help of learning analytics, MOOCs have wider potential to succeed in learning through promoting self-regulated learning (SRL).The current study aims to apply and validate visualization design guidelines for a MOOC dashboard to enhance such SRL capabilities based on learning analytics.To achieve the research objective, a MOOC dashboard prototype, LM-Dashboard, was designed and developed, reflecting the visualization design guidelines to promote SRL.Then, both expert and learner participants evaluated LM-Dashboard through iterations to validate the visualization design guidelines and perceived SRL effectiveness.The results of expert and learner evaluations indicated that most of the visualization design guidelines on LM-Dashboard were valid and some perceived SRL aspects such as monitoring a student's learning progress and assessing their achievements with time management were beneficial.However, some features on LM-Dashboard should be improved to enhance SRL aspects related to achieving their learning goals with persistence.The findings suggest that it is necessary to offer appropriate feedback or tips as well as to visualize learner behaviors and activities in an intuitive and efficient way for the successful cycle of SRL.Consequently, this study contributes to establishing a basis for the visual design of a MOOC dashboard for optimizing each learner's SRL.",
    url = "https://doi.org/10.3837/tiis.2019.06.002"
}

@article{ref66_1f3f2a,
    author = "Valle, Natercia and Antonenko, Pavlo and Valle, Denis and Sommer, Max and Huggins‐Manley, Anne and Dawson, Kara and Kim, Dongho and Baiser, Benjamin",
    title = "Predict or describe? How learning analytics dashboard design influences motivation and statistics anxiety in an online statistics course",
    year = "2021",
    issn = "1042-1629",
    doi = "10.1007/s11423-021-09998-z",
    url = "https://doi.org/10.1007/s11423-021-09998-z"
}

@article{ref67_36b574,
    author = "Park, Eunsung and Ifenthaler, Dirk and Clariana, Roy",
    title = "Adaptive or adapted to: Sequence and reflexive thematic analysis to understand learners' self‐regulated learning in an adaptive learning analytics dashboard",
    year = "2022",
    issn = "0007-1013",
    doi = "10.1111/bjet.13287",
    url = "https://doi.org/10.1111/bjet.13287"
}

@article{ref68_c50c7e,
    author = "Divjak, Blaženka and Svetec, Barbi and Horvat, Damir",
    title = "Learning analytics dashboards: What do students actually ask for?",
    year = "2023",
    doi = "10.1145/3576050.3576141",
    url = "https://doi.org/10.1145/3576050.3576141"
}

@article{ref69_87dbae,
    author = "Alam, Imtiajul and Malone, Lauren and Nadolny, Larysa and Brown, Michael and Cervato, Cinzia",
    title = "Investigating the impact of a gamified learning analytics dashboard: Student experiences and academic achievement",
    year = "2023",
    issn = "0266-4909",
    doi = "10.1111/jcal.12853",
    abstract = "Abstract Background The substantial growth in gamification research has connected gamified learning to enhanced engagement, improved performance, and greater motivation. Similar to gamification, personalized learning analytics dashboards can enhance student engagement. Objectives This study explores the student experiences and academic achievements using a gamified dashboard in a large, introductory STEM course. Methods We examined two groups of students enrolled in different sections of a one‐semester‐long physical geology course with a total enrollment of 223 students. The only difference between the groups was that one had access to the dashboard. The data collection included students' assignments, overall performances, and exam scores. Students in both sections completed a Science Literacy Concept Inventory survey at the beginning and end of the term. Additionally, students completed an end‐of‐term survey containing open‐ended questions on their experience and interactions with specific elements. Results Students shared mostly positive comments about their experience with the dashboard, and the final grade of students with access to the dashboard was 13\% higher, on average, compared to their peers in the non‐dashboard section. Conclusion With low costs and little time invested, gamified dashboards could have a significant impact on student performance in large STEM lecture courses.",
    url = "https://doi.org/10.1111/jcal.12853"
}

@article{ref70_a3f549,
    author = "Ulfa, Saida and Fattawi, Izzull and Surahman, Ence and Hayashi, Yusuke",
    title = "Investigating Learners' Perception of Learning Analytics Dashboard to Improve Learning Interaction in Online Learning System",
    year = "2019",
    doi = "10.1109/icet48172.2019.8987229",
    url = "https://doi.org/10.1109/icet48172.2019.8987229"
}

@article{ref71_67d59b,
    author = "Bao, Haogang and Li, Yanyan and Su, You and Xing, Shuang and Chen, Nian‐Shing and Rosé, Carolyn",
    title = "The effects of a learning analytics dashboard on teachers’ diagnosis and intervention in computer-supported collaborative learning",
    year = "2021",
    issn = "1475-939X",
    doi = "10.1080/1475939x.2021.1902383",
    url = "https://doi.org/10.1080/1475939x.2021.1902383"
}

@article{ref72_dfa098,
    author = "Williamson, Kimberly and Kizilcec, René",
    title = "Learning Analytics Dashboard Research Has Neglected Diversity, Equity and Inclusion",
    year = "2021",
    doi = "10.1145/3430895.3460160",
    url = "https://doi.org/10.1145/3430895.3460160"
}

@article{ref73_5a820f,
    author = "Owatari, Takuro and Shimada, Atsushi and Minematsu, Tsubasa and Hori, Maiya and Taniguchi, Rin-ichiro",
    title = "Real-Time Learning Analytics Dashboard for Students in Online Classes",
    year = "2020",
    doi = "10.1109/tale48869.2020.9368340",
    url = "https://doi.org/10.1109/tale48869.2020.9368340"
}

@article{ref74_e2fd68,
    author = "Masiello, Italo and Mohseni, Zeynab and Palma, Francis and Nordmark, Susanna and Augustsson, Hanna and Rundquist, Rebecka",
    title = "A Current Overview of the Use of Learning Analytics Dashboards",
    year = "2024",
    issn = "2227-7102",
    doi = "10.3390/educsci14010082",
    abstract = "The promise of Learning Analytics Dashboards in education is to collect, analyze, and visualize data with the ultimate ambition of improving students’ learning. Our overview of the latest systematic reviews on the topic shows a number of research trends: learning analytics research is growing rapidly; it brings to the front inequality and inclusiveness measures; it reveals an unclear path to data ownership and privacy; it provides predictions which are not clearly translated into pedagogical actions; and the possibility of self-regulated learning and game-based learning are not capitalized upon. However, as learning analytics research progresses, greater opportunities lie ahead, and a better integration between information science and learning sciences can bring added value of learning analytics dashboards in education.",
    url = "https://doi.org/10.3390/educsci14010082"
}

@article{ref75_4b159f,
    author = "Yoo, Mina and Jin, Sung-Hee",
    title = "Development and Evaluation of Learning Analytics Dashboards to Support Online Discussion Activities.",
    year = "2020",
    issn = "1176-3647"
}

@article{ref76_b6c038,
    author = "Yousef, Ahmed and Khatiry, Ahmed",
    title = "Cognitive versus behavioral learning analytics dashboards for supporting learner’s awareness, reflection, and learning process",
    year = "2021",
    issn = "1049-4820",
    doi = "10.1080/10494820.2021.2009881",
    url = "https://doi.org/10.1080/10494820.2021.2009881"
}

@article{ref77_e61ac1,
    author = "Gruzd, Anatoliy and Conroy, Nadia",
    title = "Learning Analytics Dashboard for Teaching with Twitter",
    year = "2020",
    issn = "1530-1605",
    doi = "10.24251/hicss.2020.330",
    abstract = "As social media takes root in our society, more University instructors are incorporating platforms like Twitter into their classroom. However, few of the current Learning Analytics (LA) systems process social media data for instructional interventions and evaluation. As a result, instructors who are using social media cannot easily assess their students’ learning progress or use the data to adjust their lessons in real time. We surveyed 54 university instructors to better understand how they use social media in the classroom; we then used these results to design and evaluate our own Twitter-centric LA dashboard. The overarching goals for this project were to 1) assist instructors in determining whether their particular use of Twitter met their teaching objectives, and 2) help system designers navigate the nuance of designing LA dashboards for social media platforms.",
    url = "https://doi.org/10.24251/hicss.2020.330"
}

@article{ref78_a2a3b1,
    author = "Hasnine, Mohammad and Nguyen, Ho and Tran, Thuy and Bui, Huyen and Akçapınar, Gökhan and Ueda, Hiroshi",
    title = "A Real-Time Learning Analytics Dashboard for Automatic Detection of Online Learners’ Affective States",
    year = "2023",
    issn = "1424-8220",
    doi = "10.3390/s23094243",
    abstract = "Students’ affective states describe their engagement, concentration, attitude, motivation, happiness, sadness, frustration, off-task behavior, and confusion level in learning. In online learning, students’ affective states are determinative of the learning quality. However, measuring various affective states and what influences them is exceedingly challenging for the lecturer without having real interaction with the students. Existing studies primarily use self-reported data to understand students’ affective states, while this paper presents a novel learning analytics system called MOEMO (Motion and Emotion) that could measure online learners’ affective states of engagement and concentration using emotion data. Therefore, the novelty of this research is to visualize online learners’ affective states on lecturers’ screens in real-time using an automated emotion detection process. In real-time and offline, the system extracts emotion data by analyzing facial features from the lecture videos captured by the typical built-in web camera of a laptop computer. The system determines online learners’ five types of engagement (“strong engagement”, “high engagement”, “medium engagement”, “low engagement”, and “disengagement”) and two types of concentration levels (“focused” and “distracted”). Furthermore, the dashboard is designed to provide insight into students’ emotional states, the clusters of engaged and disengaged students’, assistance with intervention, create an after-class summary report, and configure the automation parameters to adapt to the study environment.",
    url = "https://doi.org/10.3390/s23094243"
}

@article{ref79_864043,
    author = "Dabbebi, Inès and Gilliot, Jean-Marie and Iksal, Sébastien",
    title = "User Centered Approach for Learning Analytics Dashboard Generation",
    year = "2019",
    doi = "10.5220/0007693102600267",
    abstract = "HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not.The documents may come from teaching and research institutions in France or abroad, or from public or private research centers.L'archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.",
    url = "https://doi.org/10.5220/0007693102600267"
}

@article{ref80_5a054c,
    author = "Gkontzis, Andreas and Karachristos, ‪Christoforos and Lazarinis, Fotis and Stavropoulos, Elias and Verykios, Vassilios",
    title = "Assessing Student Performance by Learning Analytics Dashboards",
    year = "2017",
    issn = "2459-4210",
    doi = "10.12681/icodl.1096",
    abstract = "Στο σημερινό ανταγωνιστικό επιχειρηματικό περιβάλλον, πολλές και διαφορετικές εφαρμογές παράγουν ένα τεράστιο όγκο δεδομένων, τα οποία συνήθως αναφέρονται ως μεγάλα δεδομένα. Τα συστήματα Επιχειρηματικής Ευφυίας και Ανάλυσης εφαρμόζονται όλο και περισσότερο από τον δημόσιο και τον ιδιωτικό τομέα για την παρακολούθηση σύνθετων δεδομένων σε Ταμπλό και την αποτελεσματική υποστήριξη λήψης αποφάσεων. Τα Επιχειρησιακά Ταμπλό, ως πλήρη συστήματα εκτέλεσης πληροφοριών, επιτρέπουν την SECTION A: theoretical papers, original research and scientific articles 102 εξέταση των επιχειρηματικών επιδόσεων προς καθορισμένους στόχους, σε ένα οπτικό διαδραστικό περιβάλλον. Τα ιδρύματα της εξ’ αποστάσεως εκπαίδευσης εφαρμόζουν παρόμοιες μεθοδολογίες για τη διαχείριση της έκρηξης εκπαιδευτικών δεδομένων χρησιμοποιώντας τεχνικές εξόρυξης δεδομένων και απεικόνισης. Η έλλειψη καθημερινής ανάλυσης δεδομένων σε πραγματικό χρόνο και η καθυστέρηση στην άμεση παρουσίαση κρίσιμων πληροφοριών, περιορίζουν τις πρόωρες αποφάσεις των εκπαιδευτικών. Με την εμφάνιση των Ταμπλό Ανάλυσης Εκπαιδευτικών Δεδομένων ως ένα πλήρες σύστημα μέτρησης, συλλογής, ανάλυσης και ταυτόχρονης απεικόνισης ζωτικών γνώσεων σε ένα προσαρμόσιμο περιβάλλον εργασίας, παρέχεται στους διδάσκοντες η έγκαιρη επίγνωση της τρέχουσας προόδου των φοιτητών. Η μελέτη αυτή διερευνά συγκεκριμένα LAD στην πλατφόρμα Moodle, καθώς τα εφαρμόζει σε ένα σεμινάριο που προσφέρεται από το Εργαστήριο Μεθοδολογίας και Τεχνολογίας Εκπαιδευτικού Περιεχομένου στο Ελληνικό Ανοικτό Πανεπιστήμιο και απεικονίζει τα ευρήματα της παρακολούθησης των αλληλεπιδράσεων των φοιτητών. Η συμβολή της εργασίας αφορά την ενίσχυση της λήψης αποφάσεων από τους εκπαιδευτικούς και τον αυτο-προβληματισμό των φοιτητών παρακολουθώντας τη δραστηριότητά τους και παρέχοντας εγκαίρως ακριβή ανατροφοδότηση.",
    url = "https://doi.org/10.12681/icodl.1096"
}

@article{ref81_265052,
    author = "Ramaswami, Gomathy and Sušnjak, Teo and Mathrani, Anuradha",
    title = "Effectiveness of a Learning Analytics Dashboard for Increasing Student Engagement Levels",
    year = "2023",
    issn = "1929-7750",
    doi = "10.18608/jla.2023.7935",
    abstract = "Learning Analytics Dashboards (LADs) are gaining popularity as a platform for providing students with insights into their learning behaviour patterns in online environments. Existing LAD studies are mainly centred on displaying students’ online behaviours with simplistic descriptive insights. Only a few studies have integrated predictive components, while none possess the ability to explain how the predictive models work and how they have arrived at specific conclusions for a given student. A further gap exists within existing LADs with respect to prescriptive analytics that generate data-driven feedback to students on how to adjust their learning behaviour. The LAD in this study attempts to address this gap and integrates a full spectrum of current analytics technologies for sense-making while anchoring them within theoretical educational frameworks. This study’s LAD (SensEnablr) was evaluated for its effectiveness in impacting learning in a student cohort at a tertiary institution. Our findings demonstrate that student engagement with learning technologies and course resources increased significantly immediately following interactions with the dashboard. Meanwhile, results showed that the dashboard boosted the respondents’ learning motivation levels and that the novel analytics insights drawn from predictive and prescriptive analytics were beneficial to their learning. This study, therefore, has implications for future research when investigating student outcomes and optimizing student learning using LAD technologies.",
    url = "https://doi.org/10.18608/jla.2023.7935"
}

@article{ref82_469792,
    author = "Ullmann, Thomas and De Liddo, Anna and Bachler, Michelle",
    title = "A Visualisation Dashboard for Contested Collective Intelligence Learning Analytics to Improve Sensemaking of Group Discussion",
    year = "2019",
    issn = "1138-2783",
    doi = "10.5944/ried.22.1.22294",
    abstract = "La habilidad para participar y contribuir a los debates es importante para el aprendizaje informal y formal. Especialmente cuando se abordan temas altamente complejos, puede ser difícil apoyar a los alumnos que participan en una discusión grupal efectiva y mantenerse al tanto de toda la información generada colectivamente durante la discusión. La tecnología puede ayudar con el compromiso y razonamiento en debates tan grandes, por ejemplo, puede monitorear cuán saludable es un debate y proporcionar indicadores sobre la distribución de la participación. Un marco especial que pretende aprovechar la inteligencia de grupos de pequeños a muy grandes con el apoyo de herramientas de discurso y argumentación estructuradas es la Inteligencia Colectiva Controvertida (ICC). Las herramientas de CCI proporcionan una fuente rica de datos semánticos que, si se procesan de manera adecuada, pueden generar un sofisticado análisis del discurso en línea. Este estudio presenta un panel de visualización con varios análisis visuales que muestran aspectos importantes de los debates en línea que han sido facilitados por las herramientas de discusión de CCI. El tablero de instrumentos fue diseñado para mejorar la creación de sentidos y la participación en los debates en línea y se ha evaluado con dos estudios, un experimento de laboratorio y un estudio de campo, en el contexto de dos institutos de educación superior. Este artículo informa sobre los resultados de una evaluación de usabilidad del panel de visualización. Los hallazgos descriptivos sugieren que los participantes con poca experiencia en el uso de visualizaciones analíticas pudieron desempeñarse bien en determinadas tareas. Esto constituye un resultado prometedor para la aplicación de tales tecnologías de visualización, ya que las interfaces analíticas de aprendizaje centradas en el discurso pueden ayudar a apoyar el compromiso de los alumnos y su razonamiento en debates en línea complejos.",
    url = "https://doi.org/10.5944/ried.22.1.22294"
}

@article{ref83_444a34,
    author = "Shabaninejad, Shiva and Khosravi, Hassan and Leemans, Sander and Sadiq, Shazia and Indulska, Marta",
    title = "Recommending Insightful Drill-Downs Based on Learning Processes for Learning Analytics Dashboards",
    year = "2020",
    issn = "0302-9743",
    doi = "10.1007/978-3-030-52237-7\_39",
    url = "https://doi.org/10.1007/978-3-030-52237-7\_39"
}

@article{ref84_9b1de9,
    author = "Wiley, Korah and Dimitriadis, Yannis and Linn, Marcia",
    title = "<scp>A human‐centred</scp> learning analytics approach for developing contextually scalable K‐12 teacher dashboards",
    year = "2023",
    issn = "0007-1013",
    doi = "10.1111/bjet.13383",
    abstract = "Abstract This paper describes a Human‐Centred Learning Analytics (HCLA) design approach for developing learning analytics (LA) dashboards for K‐12 classrooms that maintain both contextual relevance and scalability—two goals that are often in competition. Using mixed methods, we collected observational and interview data from teacher partners and assessment data from their students' engagement with the lesson materials. This DBR‐based, human‐centred design process resulted in a dashboard that supported teachers in addressing their students' learning needs. To develop the dashboard features that could support teachers, we found that a design refinement process that drew on the insights of teachers with varying teaching experience, philosophies and teaching contexts strengthened the resulting outcome. The versatile nature of the approach, in terms of student learning outcomes, makes it useful for HCLA design efforts across diverse K‐12 educational contexts. Practitioner notes What is already known about this topic Learning analytics that are aligned to both a learning theory and learning design support student learning. LA dashboards that support users to understand the associated learning analytics data provide actionable insight. Design‐based research is a promising methodology for Human‐Centred Learning Analytics design, particularly in the K‐12 educational context. What this paper adds Leveraging a longstanding, yet fluid, research‐practice partnership is an effective design‐based research adaptation for addressing the high variation in instructional practices that characterize K‐12 education. Using both quantitative and qualitative data that reflects students' developing knowledge effectively supports teachers' inquiry into student learning. Teachers' use of learning analytics dashboards is heavily influenced by their perspectives on teaching and learning. Implications for practice and/or policy Impact on student learning outcomes, alongside usability and feasibility, should be included as a necessary metric for the effectiveness of LA design. LA dashboard developers should both leverage learning data that reflect students' developing knowledge and position teachers to take responsive pedagogical action to support student learning. LA researchers and developers should utilize a long‐term, yet fluid, research‐practice partnership to form a multi‐stakeholder, multidisciplinary design team for Human‐Centred Learning Analytics design.",
    url = "https://doi.org/10.1111/bjet.13383"
}

@article{ref85_cebe25,
    author = "Al Hadhrami, Ghaniya",
    title = "Learning Analytics Dashboard to Improve Students’ Performance and Success",
    year = "2017",
    issn = "2320-737X",
    doi = "10.9790/7388-0701053945",
    abstract = {Several learning researches have been conducted recently in order to improve the students' performance.The new learning system has benefited from numerical environment by presenting an online educational system.Structured approach in design, implementation, and student's assessment has been studied.The present project aims to develop a dashboard for "Learning Analytics Techniques to Improve Students' Performance and Success" through a web Application.The model proposed is a result oriented and aims to improve the online educational systems for both teachers and students.It includes more accurate assessment and more effective evaluation of the learning process.Learning analytics is focusing on data that coming from Learning Management System (LMS).The data coming from LMS includes the number of times the student has logged into the page, the assessment of the student according to course prerequisites, the time spent by the student in reading the course materials and the time taken to complete the module assessments and attending discussions on forum.This technique will show a visual report and explain where the student is lacking.This framework will display the learning capacity of students to the teacher as well as the self-performance to the student.This project is designed with Data Mining techniques to identify student's risk and forecast student's results.},
    url = "https://doi.org/10.9790/7388-0701053945"
}

@article{ref86_7775a5,
    author = "Majumdar, Rwitajit and Akçapınar, Arzu and Akçapınar, Gökhan and Ogata, Hiroaki and Flanagan, Brendan",
    title = "LAView: Learning Analytics Dashboard Towards Evidence-based Education",
    year = "2019"
}

@article{ref87_6abe4d,
    author = "Sedrakyan, Gayane and Järvelä, Sanna and Kirschner, Paul",
    title = "Conceptual framework for feedback automation and personalization for designing learning analytics dashboards",
    year = "2016"
}

@article{ref88_96a92f,
    author = "Chen, Li and Lu, Min and Goda, Yoshiko and Shimada, Atsushi and Yamada, Masanori",
    title = "FACTORS OF THE USE OF LEARNING ANALYTICS DASHBOARD THAT AFFECT METACOGNITION",
    year = "2020",
    doi = "10.33965/celda2020\_202014l038",
    abstract = "In this study, we used a learning analytics dashboard (LAD) in a higher education course to support students'metacognition and evaluated the effects of its use. The LAD displays students' reading path and specific behaviors whenviewing digital",
    url = "https://doi.org/10.33965/celda2020\_202014l038"
}

@article{ref89_829615,
    author = "Leitner, Philipp and Maier, Karin and Ebner, Martin",
    title = "Web Analytics as Extension for a Learning Analytics Dashboard of a Massive Open Online Platform",
    year = "2020",
    issn = "2662-2122",
    doi = "10.1007/978-3-030-47392-1\_19",
    url = "https://doi.org/10.1007/978-3-030-47392-1\_19"
}

@article{ref90_0bfb94,
    author = "Paulsen, Lucas and Lindsay, Euan",
    title = "Learning analytics dashboards are increasingly becoming about learning and not just analytics - A systematic review",
    year = "2024",
    issn = "1360-2357",
    doi = "10.1007/s10639-023-12401-4",
    abstract = "Abstract This systematic review explores the emerging themes in the design and implementation of student-facing learning analytics dashboards in higher education. Learning Analytics has long been criticised for focusing too much on the analytics, and not enough on the learning. The review is then guided by an interest in whether these dashboards are still primarily analytics-driven or if they have become pedagogically informed over time. By mapping the identified themes of technological maturity, informing frameworks, affordances, data sources, and analytical levels over publications per year, the review identifies an emerging trajectory towards student-focused dashboards. These dashboards are informed by theory-oriented frameworks, designed to incorporate affordances that supporting student learning, and realised through integration of more than just activity data from learning management systems – allowing the dashboards to better support students' learnings processes. Based on this emerging trajectory, the review provides a series of design recommendations for student-focused dashboards that are connected to learning sciences as well as analytics.",
    url = "https://doi.org/10.1007/s10639-023-12401-4"
}

@article{ref91_6a592c,
    author = "Kaliisa, Rogers and Misiejuk, Kamila and López‐Pernas, Sonsoles and Khalil, Mohammad and Saqr, Mohammed",
    title = "Have Learning Analytics Dashboards Lived Up to the Hype? A Systematic Review of Impact on Students' Achievement, Motivation, Participation and Attitude",
    year = "2024",
    doi = "10.1145/3636555.3636884",
    abstract = "While learning analytics dashboards (LADs) are the most common form of LA intervention, there is limited evidence regarding their impact on students' learning outcomes. This systematic review synthesizes the findings of 38 research studies to investigate the impact of LADs on students' learning outcomes, encompassing achievement, participation, motivation, and attitudes. As we currently stand, there is no evidence to support the conclusion that LADs have lived up to the promise of improving academic achievement. Most studies reported negligible or small effects, with limited evidence from well-powered controlled experiments. Many studies merely compared users and non-users of LADs, confounding the dashboard effect with student engagement levels. Similarly, the impact of LADs on motivation and attitudes appeared modest, with only a few exceptions demonstrating significant effects. Small sample sizes in these studies highlight the need for larger-scale investigations to validate these findings. Notably, LADs showed a relatively substantial impact on student participation. Several studies reported medium to large effect sizes, suggesting that LADs can promote engagement and interaction in online learning environments. However, methodological shortcomings, such as reliance on traditional evaluation methods, self-selection bias, the assumption that access equates to usage, and a lack of standardized assessment tools, emerged as recurring issues. To advance the research line for LADs, researchers should use rigorous assessment methods and establish clear standards for evaluating learning constructs. Such efforts will advance our understanding of the potential of LADs to enhance learning outcomes and provide valuable insights for educators and researchers alike.",
    url = "https://doi.org/10.1145/3636555.3636884"
}

@article{ref92_d54096,
    author = "Ramaswami, Gomathy and Sušnjak, Teo and Mathrani, Anuradha",
    title = "Capitalizing on Learning Analytics Dashboard for Maximizing Student Outcomes",
    year = "2019",
    doi = "10.1109/csde48274.2019.9162357",
    url = "https://doi.org/10.1109/csde48274.2019.9162357"
}

@article{ref93_1a316d,
    author = "Nguyen, Ha and Campos, Fabio and Ahn, June",
    title = "Discovering Generative Uncertainty in Learning Analytics Dashboards",
    year = "2021",
    issn = "2662-2122",
    doi = "10.1007/978-3-030-81222-5\_21",
    url = "https://doi.org/10.1007/978-3-030-81222-5\_21"
}

@article{ref94_f8ceff,
    author = "Chen, Li and Lu, Min and Goda, Yoshiko and Shimada, Atsushi and Yamada, Masanori",
    title = "Learning Analytics Dashboard Supporting Metacognition",
    year = "2021",
    issn = "2662-5628",
    doi = "10.1007/978-3-030-65657-7\_8",
    url = "https://doi.org/10.1007/978-3-030-65657-7\_8"
}

@article{ref95_73d3f5,
    author = "van Leeuwen, Anouschka and Rummel, Nikol",
    title = "Teacher regulation of collaborative learning: research directions for learning analytics dashboards",
    year = "2017"
}

@article{ref96_47a9f8,
    author = "Chaudy, Yaëlle and Connolly, Thomas",
    title = "Specification and evaluation of an assessment engine for educational games: Empowering educators with an assessment editor and a learning analytics dashboard",
    year = "2018",
    issn = "1875-9521",
    doi = "10.1016/j.entcom.2018.07.003",
    url = "https://doi.org/10.1016/j.entcom.2018.07.003"
}

@article{ref97_04bc2e,
    author = "Duan, Xiaojing and Wang, Chaoli and Rouamba, Guieswende",
    title = "Designing a Learning Analytics Dashboard to Provide Students with Actionable Feedback and Evaluating Its Impacts",
    year = "2022",
    doi = "10.5220/0011116400003182",
    url = "https://doi.org/10.5220/0011116400003182"
}

@article{ref98_068b72,
    author = "Haynes, Carl",
    title = "The Role of Self-Regulated Learning in the Design, Implementation, and Evaluation of Learning Analytics Dashboards",
    year = "2020",
    doi = "10.1145/3386527.3406732",
    url = "https://doi.org/10.1145/3386527.3406732"
}

@article{ref99_971c99,
    author = "Dipace, Anna and Fazlagic, Bojan and Minerva, Tommaso",
    title = "The Design of a Learning Analytics Dashboard: EduOpen Mooc platform redefinition procedures",
    year = "2019",
    doi = "10.20368/1971-8829/1135044",
    url = "https://doi.org/10.20368/1971-8829/1135044"
}

@article{ref100_564abd,
    author = "de Vreugd, Lars and Jansen, Renée and van Leeuwen, Anouschka and van der Schaaf, Marieke",
    title = "The role of reference frames in learners’ internal feedback generation with a learning analytics dashboard",
    year = "2023",
    issn = "0191-491X",
    doi = "10.1016/j.stueduc.2023.101303",
    abstract = "Being able to self-regulate can positively impact learners' academic achievement. An inherent catalyst of Self-Regulated Learning (SRL) is internal feedback, the new knowledge which is generated when comparing current knowledge against reference information. Learners may not always generate internal feedback, hampering further SRL. Supporting SRL can be done with a Learning Analytics Dashboard (LAD), in which reference frames allow for comparisons and facilitate internal feedback generation. This study explores internal feedback generation using a LAD and the effect of reference frame availability. A multiple method design examined the interplay of reference frames, comparison processes, internal feedback generation and preparatory activities engagement. Differences between three conditions were explored using Bain ANOVA's. Results showed that reference frames almost exclude other external comparators and are used in parallel with an internal comparator. A peer reference frame leads to most verbalizations of internal feedback, and potentially to most verbalizations of preparatory activities.",
    url = "https://doi.org/10.1016/j.stueduc.2023.101303"
}

@article{ref101_8143c4,
    author = "Seaton, Jennifer and Chang, Maiga and Graf, Sabine",
    title = "Integrating a Learning Analytics Dashboard in an Online Educational Game",
    year = "2019",
    issn = "2522-0888",
    doi = "10.1007/978-981-32-9335-9\_7",
    url = "https://doi.org/10.1007/978-981-32-9335-9\_7"
}

@article{ref102_c1cdc0,
    author = "Calvo-Morata, Antonio and Alonso‐Fernández, Cristina and Pérez-Colado, Iván and Freire, Manuel and Martínez‐Ortiz, Iván and Fernández‐Manjón, Baltasar",
    title = "Improving Teacher Game Learning Analytics Dashboards through ad-hoc Development",
    year = "2019",
    doi = "10.3217/jucs-025-12-1507",
    url = "https://doi.org/10.3217/jucs-025-12-1507"
}

@article{ref103_ff96e2,
    author = "Liu, Min and Han, Songhee and Shao, Peixia and Cai, Ying and Pan, Zilong",
    title = "The Current Landscape of Research and Practice on Visualizations and Dashboards for Learning Analytics",
    year = "2021",
    issn = "2662-2122",
    doi = "10.1007/978-3-030-81222-5\_2",
    url = "https://doi.org/10.1007/978-3-030-81222-5\_2"
}

@article{ref104_cae68a,
    author = "Akçapınar, Gökhan and Hasnine, Mohammad",
    title = "Discovering the effects of learning analytics dashboard on students’ behavioral patterns using differential sequence mining",
    year = "2022",
    issn = "1877-0509",
    doi = "10.1016/j.procs.2022.09.443",
    abstract = "Interventions based on learning analytics have a very important place in closing the learning analytics loop. However, data-driven studies that test the effects of learning analytics-based interventions on students' online learning behaviors are very limited. In this study, the effect of the student-facing learning analytics dashboard (LAD) on the learning behavior of students in the online learning environment was investigated by using the differential pattern mining method. In a completely remote course, the learning behaviors of the participants before the introduction of the dashboard were compared with the learning behaviors they exhibited after the dashboard was introduced. In this way, it has become possible to analyze the behavior changes after the dashboard intervention. Wilcoxon signed-rank test was used to test whether these behavioral changes were statistically significant or not. According to the Wilcoxon signed-rank test results, while there is no significant difference in terms of students' assignment and quiz interactions, it is seen that there is a statistically significant increase in terms of students' forum-related activities such as reading other students' posts, starting a new discussion, and replying others' posts. Students' SCORM interactions (e.g, launch, complete) were also increased after engaging with the LAD. In addition, it was found that the overall interaction of students in the online learning environment increased by 57\% when the LAD was used.",
    url = "https://doi.org/10.1016/j.procs.2022.09.443"
}

@article{ref105_63e0df,
    author = "Gilliot, Jean-Marie and Iksal, Sébastien and Medou, Daniel and Dabbebi, Inès",
    title = "Participatory design of learning analytics dashboards",
    year = "2018",
    doi = "10.1145/3286689.3286693",
    abstract = "While the field of Learning Analytics is in full development, potential users are just discovering the opportunities offered by these tools. One of the major difficulties consists in proposing a visual data representation which makes sense to the users. After refining our method to identify user needs in terms of visualization, and to define the main dimensions of a learning dashboard, we propose a tool to support participatory design. This tool is based on a canvas and cards to help dashboards' creation using Learning Analytics. It allows users to support creativity around decision making, characterize their context, and draw a dashboard's mockup using existing content.",
    url = "https://doi.org/10.1145/3286689.3286693"
}

@article{ref106_be91c9,
    author = "Peraić, Ivan and Grubišić, Ani",
    title = "Development and Evaluation of a Learning Analytics Dashboard for Moodle Learning Management System",
    year = "2022",
    issn = "0302-9743",
    doi = "10.1007/978-3-031-22131-6\_30",
    url = "https://doi.org/10.1007/978-3-031-22131-6\_30"
}

@article{ref107_895877,
    author = "Karademir, Onur and Ahmad, Atezaz and Schneider, Ján and Di Mitri, Daniele and Jivet, Ioana and Drachsler, Hendrik",
    title = "Designing the Learning Analytics Cockpit - A Dashboard that Enables Interventions",
    year = "2021",
    issn = "2367-3389",
    doi = "10.1007/978-3-030-86618-1\_10",
    url = "https://doi.org/10.1007/978-3-030-86618-1\_10"
}

@article{ref108_9185ef,
    author = "Il-Hyun, Jo and Park, Yeonjeong",
    title = "Design and Application of Visual Dashboard Based on Learning Analytics*",
    year = "2014",
    issn = "1229-7291",
    doi = "10.15833/kafeiam.20.2.3",
    url = "https://doi.org/10.15833/kafeiam.20.2.3"
}

@article{ref109_5a054c,
    author = "Gkontzis, Andreas and Karachristos, ‪Christoforos and Lazarinis, Fotis and Stavropoulos, Elias and Verykios, Vassilios",
    title = "Assessing Student Performance by Learning Analytics Dashboards",
    year = "2017"
}

@article{ref110_972172,
    author = "Fernandez‐Nieto, Gloria and Martínez‐Maldonado, Roberto and Echeverría, Vanessa and Kitto, Kirsty and Gašević, Dragan and Shum, Simon",
    title = "Data Storytelling Editor: A Teacher-Centred Tool for Customising Learning Analytics Dashboard Narratives",
    year = "2024",
    doi = "10.1145/3636555.3636930",
    abstract = "Dashboards are increasingly used in education to provide teachers and students with insights into learning. Yet, existing dashboards are often criticised for their failure to provide the contextual information or explanations necessary to help students interpret these data. Data Storytelling (DS) is emerging as an alternative way to communicate insights providing guidance and context to facilitate students' interpretations. However, while data stories have proven effective in prompting students' reflections, to date, it has been necessary for researchers to craft the stories rather than enabling teachers to do this by themselves. This can make this approach more feasible and scalable while also respecting teachers' agency. Based on the notion of DS, this paper presents a DS editor for teachers. A study was conducted in two universities to examine whether the editor could enable teachers to create stories adapted to their learning designs. Results showed that teachers appreciated how the tool enabled them to contextualise automated feedback to their teaching needs, generating data stories to support student reflection.",
    url = "https://doi.org/10.1145/3636555.3636930"
}

@article{ref111_15e6a0,
    author = "Rahimi, Seyedahmad and Shute, Valerie",
    title = "Learning Analytics Dashboards in Educational Games",
    year = "2021",
    issn = "2662-2122",
    doi = "10.1007/978-3-030-81222-5\_24",
    url = "https://doi.org/10.1007/978-3-030-81222-5\_24"
}

@article{ref112_cf8328,
    author = "Khulbe, Manisha and Tammets, Kairit",
    title = "Mediating Teacher Professional Learning with a Learning Analytics Dashboard and Training Intervention",
    year = "2023",
    issn = "2211-1662",
    doi = "10.1007/s10758-023-09642-0",
    url = "https://doi.org/10.1007/s10758-023-09642-0"
}

@article{ref113_e8358c,
    author = "Hilliger, Isabel and Miranda, Constanza and Schuit, Gregory and Duarte, Fernando and Anselmo, Martin and Parra, Denis",
    title = "Evaluating a Learning Analytics Dashboard to Visualize Student Self-Reports of Time-on-task",
    year = "2021",
    doi = "10.1145/3448139.3448203",
    url = "https://doi.org/10.1145/3448139.3448203"
}

@article{ref114_0080d0,
    author = "Vaclavek, Jonas and Kužílek, Jakub and Skočilas, Jan and Zdráhal, Zdeněk and Fuglík, Viktor",
    title = "Learning Analytics Dashboard Analysing First-Year Engineering Students",
    year = "2018",
    issn = "0302-9743",
    doi = "10.1007/978-3-319-98572-5\_48",
    url = "https://doi.org/10.1007/978-3-319-98572-5\_48"
}

@article{ref115_1094f8,
    author = "Vigentini, Lorenzo and Clayphan, Andrew and Zhang, Xia and Chitsaz, Mahsa",
    title = "Overcoming the MOOC Data Deluge with Learning Analytic Dashboards",
    year = "2017",
    issn = "2198-4182",
    doi = "10.1007/978-3-319-52977-6\_6",
    url = "https://doi.org/10.1007/978-3-319-52977-6\_6"
}

@article{ref116_60923f,
    author = "CORBU, EMILIA and Edelhauser, Eduard",
    title = "Responsive Dashboard as a Component of Learning Analytics System for Evaluation in Emergency Remote Teaching Situations",
    year = "2021",
    issn = "1424-8220",
    doi = "10.3390/s21237998",
    abstract = "The pandemic crisis has forced the development of teaching and evaluation activities exclusively online. In this context, the emergency remote teaching (ERT) process, which raised a multitude of problems for institutions, teachers, and students, led the authors to consider it important to design a model for evaluating teaching and evaluation processes. The study objective presented in this paper was to develop a model for the evaluation system called the learning analytics and evaluation model (LAEM). We also validated a software instrument we designed called the EvalMathI system, which is to be used in the evaluation system and was developed and tested during the pandemic. The optimization of the evaluation process was accomplished by including and integrating the dashboard model in a responsive panel. With the dashboard from EvalMathI, six online courses were monitored in the 2019/2020 and 2020/2021 academic years, and for each of the six monitored courses, the evaluation of the curricula was performed through the analyzed parameters by highlighting the percentage achieved by each course on various components, such as content, adaptability, skills, and involvement. In addition, after collecting the data through interview guides, the authors were able to determine the extent to which online education during the COVID 19 pandemic has influenced the educational process. Through the developed model, the authors also found software tools to solve some of the problems raised by teaching and evaluation in the ERT environment.",
    url = "https://doi.org/10.3390/s21237998"
}

@article{ref117_c838b1,
    author = "Becerra, Álvaro and Daza, Roberto and Cobos, Ruth and Morales, Aythami and Cukurova, Mutlu and Fiérrez, Julián",
    title = "M2LADS: A System for Generating MultiModal Learning Analytics Dashboards",
    year = "2023",
    doi = "10.1109/compsac57700.2023.00241",
    abstract = "In this article, we present a Web-based System called M2LADS, which supports the integration and visualization of multimodal data recorded in learning sessions in a MOOC in the form of Web-based Dashboards. Based on the edBB platform, the multimodal data gathered contains biometric and behavioral signals including electroencephalogram data to measure learners' cognitive attention, heart rate for affective measures, visual attention from the video recordings. Additionally, learners' static background data and their learning performance measures are tracked using LOGCE and MOOC tracking logs respectively, and both are included in the Web-based System. M2LADS provides opportunities to capture learners' holistic experience during their interactions with the MOOC, which can in turn be used to improve their learning outcomes through feedback visualizations and interventions, as well as to enhance learning analytics models and improve the open content of the MOOC.",
    url = "https://doi.org/10.1109/compsac57700.2023.00241"
}

@article{ref118_03ce32,
    author = "Kaliisa, Rogers and Gillespie, Anna and Herodotou, Christothea and Kluge, Anders and Rienties, Bart",
    title = "Teachers’ Perspectives on the Promises, Needs and Challenges of Learning Analytics Dashboards: Insights from Institutions Offering Blended and Distance Learning",
    year = "2021",
    issn = "2662-2122",
    doi = "10.1007/978-3-030-81222-5\_16",
    url = "https://doi.org/10.1007/978-3-030-81222-5\_16"
}

@article{ref119_808c69,
    author = "Droit, Alena and Rieger, Bodo",
    title = "Learning Analytics in the Flipped Classroom – Learning Dashboards from the Students' Perspective",
    year = "2020",
    issn = "1530-1605",
    doi = "10.24251/hicss.2020.013",
    abstract = "Blended learning courses offer the opportunity to collect large amounts of learning data that can help students to improve their performance. The presentation of learning data often takes place in the form of Learning Analytics dashboards, which are already in use at some universities. Students, who are the primary data providers and at the same time the main users, should be involved in the process of developing Learning Analytics dashboards from the beginning. Since there are only a few guidelines for designing these dashboards in literature, we conducted a study with 139 business and information systems students who, in addition to answering a questionnaire, also designed their dashboards with the help of a case study. The dashboard analysis provides detailed insights into the design of the functional and information scope, as well as the presentation of the data for Learning Analytics dashboards.",
    url = "https://doi.org/10.24251/hicss.2020.013"
}

@article{ref120_f12e3f,
    author = "Yan, Lixiang and Zhao, Linxuan and Echeverría, Vanessa and Jin, Yueqiao and Alfredo, Riordan and Li, Xinyu and Gašević, Dragan and Martínez‐Maldonado, Roberto",
    title = "VizChat: Enhancing Learning Analytics Dashboards with Contextualised Explanations Using Multimodal Generative AI Chatbots",
    year = "2024",
    issn = "0302-9743",
    doi = "10.1007/978-3-031-64299-9\_13",
    url = "https://doi.org/10.1007/978-3-031-64299-9\_13"
}

@article{ref121_533a1b,
    author = "Park, Yeonjeong and Jo, Il‐Hyun",
    title = {"Need Analysis for Learning Analytics Dashboard in LMS: Applying Activity Theory as an Analytic and design Tool"},
    year = "2014",
    issn = "1225-424X",
    doi = "10.17232/kset.30.2.221",
    url = "https://doi.org/10.17232/kset.30.2.221"
}

@article{ref122_6e61ab,
    author = "Gruzd, Anatoliy and Conroy, Nadia",
    title = "Designing a learning analytics dashboard for twitter-facilitated teaching",
    year = "2018",
    doi = "10.1145/3231644.3231704",
    abstract = "Social media sites are increasingly being adopted to support teaching practice in higher education. Learning Analytics (LA) dashboards can be used to reveal how students engage with course material and others in the class. However, research on the best practices of designing, developing, and evaluating such dashboards to support teaching and learning with social media has been limited. Considering the increasing use of Twitter for both formal and informal learning processes, this paper presents our design process and a LA prototype dashboard developed based on a comprehensive literature review and an online survey among 54 higher education instructors who have used Twitter in their teaching.",
    url = "https://doi.org/10.1145/3231644.3231704"
}

@article{ref123_20c1d8,
    author = "Oliver-Quelennec, Katia and Bouchet, François and Carron, Thibault and Casalino, Kathy and Pinçon, Claire",
    title = "Adapting Learning Analytics Dashboards by and for University Students",
    year = "2022",
    issn = "0302-9743",
    doi = "10.1007/978-3-031-16290-9\_22",
    url = "https://doi.org/10.1007/978-3-031-16290-9\_22"
}

@article{ref124_3e0f90,
    author = "Tretow-Fish, Tobias and Khalid, Saifuddin",
    title = "Methods for Evaluating Learning Analytics and Learning Analytics Dashboards in Adaptive Learning Platforms: A Systematic Review",
    year = "2023",
    issn = "1479-4403",
    doi = "10.34190/ejel.21.5.3088",
    abstract = "This research paper highlights and addresses the lack of a systematic review of the methods used to evaluate Learning Analytics (LA) and Learning Analytics Dashboards (LAD) of Adaptive Learning Platforms (ALPs) in the current literature. Addressing this gap, the authors built upon the work of Tretow-Fish and Khalid (2022) and analyzed 32 papers, which were grouped into six categories (C1-6) based on their themes. The categories include C1) the evaluation of LA and LAD design and framework, C2) the evaluation of user performance with LA and LAD, C3) the evaluation of adaptivity, C4) the evaluation of ALPs through perceived value, C5) the evaluation of Multimodal methods, and C6) the evaluation of the pedagogical implementation of ALP’s LA and LAD. The results include a tabular summary of the papers including the categories, evaluation unit(s), methods, variables and purpose. While there are numerous studies in categories C1-4 that focus on the design, development, and impact assessment of ALP's LA and LAD, there are only a few studies in categories C5 and C6. For the category of C5), very few studies applied any evaluation methods assessing the multimodal features of LA and LADs on ALPs. Especially for C6), evaluating the pedagogical implementation of ALP's LA and LAD, the three dimensions of signature pedagogy are used to assess the level of pedagogy evaluation. Findings showed that no studies focus on evaluating the deep or implicit structure of ALP's LA. All studies examine the structural surface dimension of learning activities and interactions between students, teachers, and ALP's LA and LAD, as examined in categories C2-C5. No studies were exclusively categorized as a C6 category, indicating that all studies evaluate ALP's LA and LAD on the surface structure dimension of signature pedagogy. This review highlights the lack of pedagogical methodology and theory in ALP's LA and LAD, which are recommended to be emphasized in future research and ALP development and implementation.",
    url = "https://doi.org/10.34190/ejel.21.5.3088"
}

@article{ref125_39cfe1,
    author = "Scheneider, Thais and Lemos, Robson",
    title = "Use of Learning Analytics Interactive Dashboards in Serious Games",
    year = "2020",
    doi = "10.31686/ijier.vol8.iss3.2220",
    abstract = "The learning analytics in serious games, corresponds to a subject in increasing demand in the educational field. In this context, there is a need to study how data visualizations found in the literature are adopted in learning analytics in serious games. This paper presents a Systematic Literature Review (SLR) on how the evolution of studies associated with the use of learning analytics interactive dashboards in serious games is processed, seeking to investigate the characteristics of using dashboards for viewing educational data. A bibliometric analysis was carried out in which 75 relevant studies were selected from the Scopus, Web of Science, and IEEExplore databases. From the data analysis, it was observed that in the current literature there is a reduced number of studies containing the main actors in the learning process, as follows: teachers/instructors, students/participants, game developers/designers, and managers/researchers. In the vast majority of investigated studies, data visualization algorithms are used, where the main focus takes into account only actors, such as teachers/instructors and students/participants.",
    url = "https://doi.org/10.31686/ijier.vol8.iss3.2220"
}

@article{ref126_ac645b,
    author = "Giannakos, Michail",
    title = "Educational Data, Learning Analytics and Dashboards",
    year = "2022",
    issn = "2196-498X",
    doi = "10.1007/978-3-031-14350-2\_4",
    abstract = "Abstract When learners interact with technologies and the learning context, a large amount of data is created. The collection, analysis, and utilization of those educational data has provided opportunities for learning technology (and CCI) research. In this chapter, we will discuss how learning systems produce and utilize educational data. In particular, we will discuss contemporary developments in the fields of learning analytics, educational data mining, and learner modelling; and how those advancements have impacted the design and functionalities of learning technologies.",
    url = "https://doi.org/10.1007/978-3-031-14350-2\_4"
}

@article{ref127_5624b9,
    author = "Uysal, Mehmet and Horzum, Mehmet",
    title = "Designing and Developing a Learning Analytics Dashboard to Support Self-Regulated Learning",
    year = "2021",
    issn = "2662-2122",
    doi = "10.1007/978-3-030-81222-5\_22",
    url = "https://doi.org/10.1007/978-3-030-81222-5\_22"
}

@article{ref128_b5909b,
    author = "Tsoni, Rozita and Kalles, Dimitris and Verykios, Vassilios",
    title = "A Data Pipeline Approach for Building Learning Analytics Dashboards",
    year = "2022",
    doi = "10.1145/3549737.3549774",
    url = "https://doi.org/10.1145/3549737.3549774"
}

@article{ref129_630bc5,
    author = "Kesselbacher, Max and Wiltschnig, Kevin and Bollin, Andreas",
    title = "Block-based learning analytics repository and dashboard",
    year = "2020",
    doi = "10.1145/3421590.3421662",
    url = "https://doi.org/10.1145/3421590.3421662"
}

@article{ref130_ca1c2a,
    author = "Eickholt, Jesse and Weible, Jennifer and Teasley, Stephanie",
    title = "Student-facing Learning Analytics Dashboard: Profiles of Student Use",
    year = "2022",
    doi = "10.1109/fie56618.2022.9962531",
    url = "https://doi.org/10.1109/fie56618.2022.9962531"
}

@article{ref131_0d2eba,
    author = "Fu, Qian and Bai, Xue and Zheng, Yafeng and Du, Runsheng and Wang, Dongqing and Zhang, Tianyi",
    title = "VisOJ: real-time visual learning analytics dashboard for online programming judge",
    year = "2022",
    issn = "0178-2789",
    doi = "10.1007/s00371-022-02586-z",
    url = "https://doi.org/10.1007/s00371-022-02586-z"
}

@article{ref132_c714fc,
    author = "Chen, Li and Geng, Xuewang and Lu, Min and Shimada, Atsushi and Yamada, Masanori",
    title = "How Students Use Learning Analytics Dashboards in Higher Education: A Learning Performance Perspective",
    year = "2023",
    issn = "2158-2440",
    doi = "10.1177/21582440231192151",
    abstract = "Developed to maximize learning performance, learning analytics dashboards (LAD) are becoming increasingly commonplace in education. An LAD’s effectiveness depends on how it is used and varies according to users’ academic levels. In this study, two LADs and a learning support system were used in a higher education course to support students’ cognitive and self-regulated learning (SRL) strategies. A total of 54 students’ learning logs on three systems and their learning performance scores were collected; descriptive statistics of learning behaviors, Mann-Whitney U test, and lag sequential analysis were used to explore how students with different learning performances used LADs to support their learning. Compared to low-performers, high-performers used the LADs more frequently during preview and review phases and conducted more monitoring and reflection strategies to support their learning. Finally, some practical implications for improving the design and use of LADs were provided.",
    url = "https://doi.org/10.1177/21582440231192151"
}

@article{ref133_fd206a,
    author = "Liu, Songran and Mouri, Kousuke and Ogata, Hiroaki",
    title = "Learning Analytics Data Flow and Visualizing for Ubiquitous Learning Logs in LMS and Learning Analytics Dashboard",
    year = "2020",
    issn = "0302-9743",
    doi = "10.1007/978-3-030-50344-4\_39",
    url = "https://doi.org/10.1007/978-3-030-50344-4\_39"
}

@article{ref134_566085,
    author = "Cechinel, Cristian and Dos Santos, Mateus and Barrozo, Caio and Schardosim, Jesiel and de Vila, Eduardo and Ramos, Vinícius and Primo, Tiago and Muñoz, Roberto and Queiroga, Emanuel",
    title = "A Learning Analytics Dashboard for Moodle: Implementing Machine Learning Techniques to Early Detect Students at Risk of Failure",
    year = "2021",
    doi = "10.1109/laclo54177.2021.00019",
    url = "https://doi.org/10.1109/laclo54177.2021.00019"
}

@article{ref135_d5ba69,
    author = "Pozdniakov, Stanislav and Martínez‐Maldonado, Roberto and Singh, Shaveen and Chen, Peter and Richardson, Dan and Bartindale, Tom and Olivier, Patrick and Gašević, Dragan",
    title = "Question-driven Learning Analytics: Designing a Teacher Dashboard for Online Breakout Rooms",
    year = "2021",
    doi = "10.1109/icalt52272.2021.00060",
    url = "https://doi.org/10.1109/icalt52272.2021.00060"
}

@article{ref136_3c533a,
    author = "Jivet, Ioana",
    title = "The Dashboard That Loved Me: Designing adaptive learning analytics for self-regulated learning",
    year = "2021"
}

@article{ref137_d192dc,
    author = "Vovides, Yianna and Inman, Sarah",
    title = "Elusive Learning—Using Learning Analytics to Support Reflective Sensemaking of Ill-Structured Ethical Problems: A Learner-Managed Dashboard Solution",
    year = "2016",
    issn = "1999-5903",
    doi = "10.3390/fi8020026",
    abstract = "Since the turn of the 21st century, we have seen a surge of studies on the state of U.S. education addressing issues such as cost, graduation rates, retention, achievement, engagement, and curricular outcomes. There is an expectation that graduates should be able to enter the workplace equipped to take on complex and “messy” or ill-structured problems as part of their professional and everyday life. In the context of online learning, we have identified two key issues that are elusive (hard to capture and make visible): learning with ill-structured problems and the interaction of social and individual learning. We believe that the intersection between learning and analytics has the potential, in the long-term, to minimize the elusiveness of deep learning. A proposed analytics model is described in this article that is meant to capture and also support further development of a learner’s reflective sensemaking.",
    url = "https://doi.org/10.3390/fi8020026"
}

@article{ref138_0062e6,
    author = "Silvola, Anni and Sjöblom, Amanda and Näykki, Piia and Gedrimiene, Egle and Muukkonen, Hanni",
    title = "Learning analytics for academic paths: student evaluations of two dashboards for study planning and monitoring",
    year = "2023",
    issn = "2295-3159",
    doi = "10.14786/flr.v11i2.1277",
    abstract = "An in-depth understanding of student experiences and evaluations of learning analytics dashboards (LADs) is needed to develop supportive learning analytics tools. This study investigates how students (N = 140) evaluated two student-facing LADs as a support for academic path-level self-regulated learning (SRL) through the concrete processes of planning and monitoring studies. Aim of the study was to gain new understanding about student perspectives for LAD use on academic path-level context. The study specifically focused on the student evaluations of the dashboard support and challenges, and the differences of student evaluations based on their self-efficacy beliefs and resource management strategies. The findings revealed that students evaluated dashboard use helpful for their study planning and monitoring, while the challenge aspects mostly included further information needs and development ideas. Students with higher self-efficacy evaluated the dashboards as more helpful for study planning than those with lower self-efficacy, and students with lower help seeking skills evaluated the dashboards as more helpful for study monitoring than those with higher help seeking skills. The results indicate that the design of LAD can help students to focus on different aspects of study planning and monitoring and that students with different beliefs and capabilities might benefit from different LAD designs and use practices. The study provides theory-informed approach for investigating LAD use in academic path-level context and extends current understanding of students as users of LADs.",
    url = "https://doi.org/10.14786/flr.v11i2.1277"
}

@article{ref139_24c14a,
    author = "Τέγος, Στέργιος and Tsiatsos, Τhrasyvoulos and Psathas, Georgios and Demetriadis, Stavros",
    title = "Towards a Learning Analytics Dashboard for Collaborative Conversational Agent Activities in MOOCs",
    year = "2020",
    issn = "2194-5365",
    doi = "10.1007/978-3-030-49932-7\_65",
    url = "https://doi.org/10.1007/978-3-030-49932-7\_65"
}

@article{ref140_667b4a,
    author = "Cohen, Eli and Toohey, Danny and McGill, Tanya and Berkelaar, Chad and Kadekodi, Ananth and Kamińska, Dominika and Lianto, Melisa and Power, Nathan",
    title = "Do Students Really Want to Know? Investigating the Relationship between Learning Analytics Dashboards and Student Motivation",
    year = "2019",
    issn = "1535-0703",
    doi = "10.28945/4352",
    abstract = "Aim/Purpose: The aim of this project was to explore the perceptions of information technology students about student-facing learning analytics dashboards that display ranking information, and whether they perceive that their motivation to study would be influenced by the use of dashboards that display their performance relative to other students. Background: While there has been a focus on the use of learning analytics dashboards by academics to inform their teaching, there has not been as much exploration of the use of student-facing dashboards, nor on the effect that students believe these dashboards will have on their motivation to study. Methodology: The research surveyed students enrolled in Information Technology courses at an Australian university. Data about students’ academic motivation was gathered using a short, online survey. Contribution: The paper adds to knowledge of the impact on students of student-facing learning analytics dashboards. Findings: A majority of students (63\%) would like to see their cohort-ranking via a dashboard, though a large majority (91\%) preferred that their ranking not be made available to other students. Students who were highly motivated to study were more likely to wish to have their ranking made available via dashboards. Those students who viewed a dashboard showing them as highly ranked relative to the unit average for an assignment were significantly more likely to be more motivated to study in this unit than those who were shown to be ranked well below the average. Recommendations for Practitioners: Although students were generally in favor of their cohort ranking being made available using dashboards, universities should proceed with caution when implementing these student-facing dashboards because of the potential for demotivating students. Recommendations for Researchers: Further investigation of the reasons why students do not wish to have their rankings made available via dashboards is needed. Impact on Society: This research contributes to the body of knowledge regarding student motivation and its relationship with student-facing learning analytics dashboards. Future Research Given the complexity of the issues investigated, more research is needed in this area.",
    url = "https://doi.org/10.28945/4352"
}

@article{ref141_8f3226,
    author = "Schwendimann, Beat and Rodríguez‐Triana, María and Vozniuk, Andrii and Prieto, Luis and Boroujeni, Mina and Holzer, Adrian and Gillet, Denis and Dillenbourg, Pierre",
    title = "Perceiving Learning at a Glance: A Systematic Literature Review of Learning Dashboard Research",
    year = "2016",
    issn = "1939-1382",
    doi = "10.1109/tlt.2016.2599522",
    url = "https://doi.org/10.1109/tlt.2016.2599522"
}

@article{ref142_274439,
    author = "Macfadyen, Leah and Myers, Alison",
    title = "The “IKEA Model” for pragmatic development of a custom learning analytics dashboard",
    year = "2023",
    issn = "2653-665X",
    doi = "10.14742/apubs.2023.465",
    abstract = "Many educators and learning analytics practitioners find themselves in ‘learning analytics limbo’, with access only to simplistic one-size-fits-all vendor-driven LA dashboards, as they wait for development of possible future LA solutions that would allow customizations that genuinely cater to differences in learning design and educator skills. We present here a simple and pragmatically oriented project that allows individual educators to build and customize an LA solution ‘at home’ with relatively simple tools. This open-source project takes advantage of data available to an educator via the LMS, and allows them to develop and customize an educator-facing dashboard that meets their teaching and learning design needs. This small-scale solution allows local educators and practitioners to continue to build their data literacy and LA-informed teaching skills, and to contribute to ongoing institutional learning through sharing their experience with institutional LA teams.",
    url = "https://doi.org/10.14742/apubs.2023.465"
}

@article{ref143_43f2cd,
    author = "Huang, Lingyun and Zheng, Juan and Lajoie, Susanne and Chen, Yuxin and Hmelo‐Silver, Cindy and Wang, Minhong",
    title = "Examining university teachers’ self-regulation in using a learning analytics dashboard for online collaboration",
    year = "2023",
    issn = "1360-2357",
    doi = "10.1007/s10639-023-12131-7",
    url = "https://doi.org/10.1007/s10639-023-12131-7"
}

@article{ref144_2d9d9a,
    author = "Toyokawa, Yuko and Majumdar, Rwitajit and Kondo, Taisho and Horikoshi, Izumi and Ogata, Hiroaki",
    title = "Active reading dashboard in a learning analytics enhanced language-learning environment: effects on learning behavior and performance",
    year = "2023",
    issn = "2197-9995",
    doi = "10.1007/s40692-023-00267-x",
    url = "https://doi.org/10.1007/s40692-023-00267-x"
}

@article{ref145_4458f2,
    author = "Munim, Ziaul and Schramm, Hans‐Joachim and Krabbel, Helene and Nyairo, F. and Haavardtun, Per and Kim, T-E. and Bustgaard, Morten",
    title = "User Requirements for Learning Analytics Dashboard in Maritime Simulator Training",
    year = "2023",
    doi = "10.1109/ieem58616.2023.10406321",
    url = "https://doi.org/10.1109/ieem58616.2023.10406321"
}

@article{ref146_2f6f85,
    author = "Ouatiq, Amina and Riyami, Bouchaïb and Mansouri, Khalifa and Qbadou, Mohammed",
    title = "The Preferences and Expectation of Moroccan Teachers from Learning Analytics Dashboards in a Blended Learning Environment: Empirical Study",
    year = "2022",
    issn = "2367-3389",
    doi = "10.1007/978-3-030-91738-8\_27",
    url = "https://doi.org/10.1007/978-3-030-91738-8\_27"
}

@article{ref147_8b7361,
    author = "Majumdar, Rwitajit and Akçapınar, Arzu and Akçapınar, Gökhan and Flanagan, Brendan and Ogata, Hiroaki",
    title = "Learning Analytics Dashboard Widgets to Author Teaching-Learning Cases for Evidence-based Education",
    year = "2019"
}

@article{ref148_07cac9,
    author = "Aljohani, Naif and Davis, Hugh and Ally, Mohamed",
    title = "Student-centred learning analytics dashboards to empower students to take responsibility for their learning",
    year = "2016",
    doi = "10.4324/9781315621586-19",
    url = "https://doi.org/10.4324/9781315621586-19"
}

@article{ref149_9bfd95,
    author = "Becerra, Álvaro and Daza, Roberto and Cobos, Ruth and Morales, Aythami and Cukurova, Mutlu and Fiérrez, Julián",
    title = "M2LADS: A System for Generating MultiModal Learning Analytics Dashboards in Open Education",
    year = "2023",
    doi = "10.48550/arxiv.2305.12561",
    abstract = "In this article, we present a Web-based System called M2LADS, which supports the integration and visualization of multimodal data recorded in learning sessions in a MOOC in the form of Web-based Dashboards. Based on the edBB platform, the multimodal data gathered contains biometric and behavioral signals including electroencephalogram data to measure learners' cognitive attention, heart rate for affective measures, visual attention from the video recordings. Additionally, learners' static background data and their learning performance measures are tracked using LOGCE and MOOC tracking logs respectively, and both are included in the Web-based System. M2LADS provides opportunities to capture learners' holistic experience during their interactions with the MOOC, which can in turn be used to improve their learning outcomes through feedback visualizations and interventions, as well as to enhance learning analytics models and improve the open content of the MOOC.",
    url = "https://doi.org/10.48550/arxiv.2305.12561"
}

@article{ref150_646f31,
    author = "Munim, Ziaul and Kim, Taeeun",
    title = "A Review of Learning Analytics Dashboard and a Novel Application in Maritime Simulator Training",
    year = "2023",
    issn = "2771-0718",
    doi = "10.54941/ahfe1003158",
    abstract = "Developing a Learning Analytics Dashboard (LAD) to evaluate maritime simulation training performance based on key performance indicators (KPIs) of maritime navigational competence can improve learning efficiency and effectiveness. Relevant data needs to be fed from simulation training logs and other sources, analysed using appropriate visualization and artificial intelligence approaches, and reported in a single window with valuable insights for trainees and instructors. This study provides a Systematic Literature Review (SLR) of published literature on LADs using scientometric tools and techniques. The findings reveal six research clusters and publication trends in LAD research. An example of a novel application of Automated Machine Learning (AutoML) analysing data from maritime desktop simulator training is presented for future maritime LAD development.",
    url = "https://doi.org/10.54941/ahfe1003158"
}

@article{ref151_cbda99,
    author = "Reid, David and Drysdale, Timothy",
    title = "Student-Facing Learning Analytics Dashboard for Remote Lab Practical Work",
    year = "2024",
    issn = "1939-1382",
    doi = "10.1109/tlt.2024.3354128",
    abstract = {The designs of many student-facing learning analytics (SFLA) dashboards are insufficiently informed by educational research and lack rigorous evaluation in authentic learning contexts, including during remote laboratory practical work. We present and evaluate an SFLA dashboard designed using the principles of formative assessment to provide feedback to students during remote lab activities. Feedback is based upon graphical visualisations of student actions performed during lab tasks and comparison to expected procedures using TaskCompare - our custom, asymmetric graph dissimilarity measure that distinguishes students who miss expected actions from those who perform additional actions, a capability missing in existing graph distance (symmetrical dissimilarity) measures. Using a total of <inline-formula xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><tex-math notation="LaTeX">$N = 235$</tex-math></inline-formula> student graphs collected during authentic learning in two different engineering courses, we describe the validation of TaskCompare and evaluate the impact of the SFLA dashboard on task completion during remote lab activities. Additionally, we use components of the Motivated Strategies for Learning Questionnaire (MSLQ) as covariates for propensity score matching (PSM) to account for potential bias in self-selection of use of the dashboard. We find that those students who used the SFLA dashboard achieved significantly better task completion rate (nearly double) than those who did not, with a significant difference in TaskCompare score between the two groups (Mann-Whitney <inline-formula xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><tex-math notation="LaTeX">$U = 453.5$</tex-math></inline-formula> , <inline-formula xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><tex-math notation="LaTeX">$p \&lt; 0.01$</tex-math></inline-formula> , Cliff's <inline-formula xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><tex-math notation="LaTeX">$\delta = 0.43$</tex-math></inline-formula> , large effect size). This difference remains after accounting for self-selection. We also report students' positive rating of usefulness of the SFLA dashboard for completing lab work is significantly above a neutral response ( <inline-formula xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><tex-math notation="LaTeX">$S = 21.0$</tex-math></inline-formula> , <inline-formula xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><tex-math notation="LaTeX">$p \&lt; 0.01$</tex-math></inline-formula> ). These findings provide evidence that our our SFLA dashboard is an effective means of providing formative assessment during remote laboratory activities.},
    url = "https://doi.org/10.1109/tlt.2024.3354128"
}

@article{ref152_d897c7,
    author = "Gallagher, Timothy and Slof, Bert and van der Schaaf, Marieke and Toyoda, Ryo and Tehreem, Yusra and Fracaro, Sofia and Kester, Liesbeth",
    title = "Reference frames for learning analytics dashboards: The progress and social reference frame and occupational <scp>self‐efficacy</scp>",
    year = "2023",
    issn = "0266-4909",
    doi = "10.1111/jcal.12912",
    abstract = "Abstract Background The potential of learning analytics dashboards in virtual reality simulation‐based training environments to influence occupational self‐efficacy via self‐reflection phase processes in the Chemical industry is still not fully understood. Learning analytics dashboards provide feedback on learner performance and offer points of comparison (i.e., comparison with one's own past performance or comparison with peer performance) to help learners make sense of their feedback. Objectives We present a theoretical framework for describing learning analytics reference frames and investigate the impact of feedback delivered through dashboards with different reference frames on occupational self‐efficacy, while controlling for workplace self‐reflection. Methods This experimental study engaged 42 chemical operator employees, aged between 18 and 55 years, each with at least one year of experience. We utilised a two‐group design to ask two research question each with three competing hypotheses related to changes in occupational self‐efficacy, employing Bayesian informative hypothesis evaluation. Results and Conclusions Results for the primary research question suggest that dashboards with progress reference frames do not elicit greater change to self‐efficacy than those with social reference frames, however, they may elicit equal change. Furthermore, dashboards with social reference frames may elicit greater change to self‐efficacy than those with progress reference frames. Exploratory results found that dashboards with progress reference frames may elicit greater positive directional change than those with social reference frames and that they may elicit equal directional change. These findings contribute to the understanding of self‐efficacy beliefs within the Chemical industry, with potential impacts on skill development. The research may inform the design of targeted interventions and training programs to influence self‐efficacy. From a practical perspective, this research suggests that careful consideration is needed when choosing reference frames in learning analytics dashboards due to their potential consequences on the formation of learner self‐efficacy.",
    url = "https://doi.org/10.1111/jcal.12912"
}

@article{ref153_1a76b0,
    author = "Putra, Faisal and Santoso, Harry and Aji, Rizal",
    title = "Evaluation of learning analytics metrics and dashboard in a software engineering project course",
    year = "2018",
    issn = "1324-5821"
}

@article{ref154_27cd83,
    author = "Pozdniakov, Stanislav and Martínez‐Maldonado, Roberto and Tsai, Yi‐Shan and Srivastava, Namrata and Liu, Yuchen and Gašević, Dragan",
    title = "Single or Multi-page Learning Analytics Dashboards? Relationships Between Teachers’ Cognitive Load and Visualisation Literacy",
    year = "2023",
    issn = "0302-9743",
    doi = "10.1007/978-3-031-42682-7\_23",
    url = "https://doi.org/10.1007/978-3-031-42682-7\_23"
}

@article{ref155_dc2b28,
    author = "Yang, Christopher and Wu, Jiun‐Yu and Ogata, Hiroaki",
    title = "Learning analytics dashboard-based self-regulated learning approach for enhancing students’ e-book-based blended learning",
    year = "2024",
    issn = "1360-2357",
    doi = "10.1007/s10639-024-12913-7",
    url = "https://doi.org/10.1007/s10639-024-12913-7"
}

@article{ref156_660d27,
    author = "Jaramillo‐Morillo, Daniel and Ruipérez‐Valiente, José and Astaiza, Claudia and Solarte, Mario and Ramírez-González, Gustavo and Alexandron, Giora",
    title = "Evaluating a learning analytics dashboard to detect dishonest behaviours: A case study in small private online courses with academic recognition",
    year = "2022",
    issn = "0266-4909",
    doi = "10.1111/jcal.12734",
    url = "https://doi.org/10.1111/jcal.12734"
}

@article{ref157_b1d4b1,
    author = "Hu, Yung-Hsiang and Liao, Bo‐Kai and Hsieh, Chieh-Lun",
    title = "Effectiveness of a gamified learning analytics dashboard with coregulation mechanism for self-regulated learning in college ethics courses",
    year = "2023",
    issn = "1049-4820",
    doi = "10.1080/10494820.2023.2277741",
    url = "https://doi.org/10.1080/10494820.2023.2277741"
}

@article{ref158_813c9e,
    author = "Taibi, Davide and Bianchi, Francesca and Kemkes, Philipp and Marenzi, Ivana",
    title = "A Learning Analytics Dashboard to Analyse Learning Activities in Interpreter Training Courses",
    year = "2019",
    issn = "1865-0929",
    doi = "10.1007/978-3-030-21151-6\_14",
    url = "https://doi.org/10.1007/978-3-030-21151-6\_14"
}

@article{ref159_d85bff,
    author = "Akçapınar, Gökhan and Hasnine, Mohammad",
    title = "Development and Evaluation of a Student-Facing Gamified Learning Analytics Dashboard",
    year = "2021",
    issn = "2662-2122",
    doi = "10.1007/978-3-030-81222-5\_13",
    url = "https://doi.org/10.1007/978-3-030-81222-5\_13"
}

@article{ref160_128620,
    author = "Sloan-Lynch, Jay and Morse, R.",
    title = "Equity-Forward Learning Analytics: Designing a Dashboard to Support Marginalized Student Success",
    year = "2024",
    doi = "10.1145/3636555.3636844",
    abstract = "Student outcomes in US higher education exhibit deep and persistent inequities. The continued underperformance of historically marginalized students remains a serious concern across higher education, reflected in increasing efforts among institutions to infuse diversity, equity, and inclusion into their academic and social communities. Yet despite widespread recognition of these inequities, few studies in the learning analytics literature engage in practical ways with issues of educational equity or DEI considerations. In this paper, we share our work supporting a large college's strategic DEI goals through the creation of a Course Diversity Dashboard informed by research into how students' study behaviors and performance interact with their gender and ethnic identities to impact course outcomes. The dashboard enables users to explore inequalities in course outcomes and take concrete actions to improve student study strategies, time management, and prior knowledge. Results from our research revealed the existence of previously hidden learner inequities in all courses included in our study as well as critical differences in underrepresented minority students' prior knowledge. And while we did not find evidence of meaningful differences in the study behaviors of student subgroups, our findings further validate the effectiveness of evidence-informed study strategies in an authentic educational setting.",
    url = "https://doi.org/10.1145/3636555.3636844"
}

@article{ref161_71d011,
    author = "Kallweit, Michael and Rolka, Katrin",
    title = "Learning Analytics in Mathematiklehrveranstaltungen – adaptive und interaktive Handlungsempfehlungen in Dashboards",
    year = "2024",
    issn = "2524-6380",
    doi = "10.1007/978-3-658-42993-5\_4",
    url = "https://doi.org/10.1007/978-3-658-42993-5\_4"
}

@article{ref162_9a5ef3,
    author = "Bennett, Liz and Folley, Sue",
    title = "Dashboard Literacy: Understanding Students’ Response to Learning Analytic Dashboards",
    year = "2020",
    issn = "2570-4532",
    doi = "10.1007/978-3-030-36911-8\_5",
    url = "https://doi.org/10.1007/978-3-030-36911-8\_5"
}

@article{ref163_13211d,
    author = "Freitag, Natascha and Serafin, Adam and Schmidt, Sebastian",
    title = "Learning Analytics Dashboards for Online Collaboration Whiteboards: Feasibility Check of an Activity Dashboard to Support the Evaluation of Student Activity within Miro",
    year = "2022",
    issn = "2232-5697",
    doi = "10.53615/2232-5697.11.207-214",
    abstract = "Purpose: Current developments due to ongoing socioeconomic transformations have increasingly boosted the digitalisation process and thus, have shifted the work domain closer to the internet. This includes the education sector, which results in the rising demand for suitable online learning opportunities. In the university context, students are frequently challenged with test performances based on collaborative work in joint research projects. The paradigm shift also affects the tutor turning him into an e-tutor. In the scope of e-tutoring, tracking and assessment of group-specific or individual-based work performance can be guaranteed by a suitable social analytics solution.\&\#x0D; Study design/methodology/approach: In this work, the design and implementation of a learning analytics dashboard tool for Online Collaborative Whiteboards, especially Miro, is presented.\&\#x0D; Findings: The requirements were derived from focus group interviews with key users.\&\#x0D; Originality/value: The tool features real-time tracking and quantitative evaluation of group and individual user actions.",
    url = "https://doi.org/10.53615/2232-5697.11.207-214"
}

@article{ref164_d5a73b,
    author = "Calvo-Morata, Antonio and Alonso‐Fernández, Cristina and Pérez-Colado, Iván and Freire, Manuel and Martínez‐Ortiz, Iván and Fernández‐Manjón, Baltasar",
    title = "Improving Teacher Game Learning Analytics Dashboards through ad-hoc Development.",
    year = "2019",
    issn = "0948-6968"
}

@article{ref165_bf4759,
    author = "Silva, Gabriel and de Carvalho, Janaína and Maciel, Alexandre",
    title = "Desenvolvimento de um Learning Analytics Dashboard a partir de Modelos de Mineração de Dados Educacionais",
    year = "2021",
    issn = "2525-4251",
    doi = "10.25286/repa.v6i3.1688",
    abstract = "Este trabalho propõe o desenvolvimento de um Learning Analytics Dashboard para análise de dados a partir dos resultados de mineração de dados educacionais, através da predição do desempenho de alunos. Assim, o objetivo geral deste trabalho é a criação de um mecanismo de visualização que pretende permitir aos professores identificar os alunos dos quais o desempenho predito é não satisfatório, fornecendo informações relativas à trajetória dos alunos no curso e assim possibilitando ao professor a intervenção pedagógica e motivacional. Durante o processo de desenvolvimento da arquitetura, três estágios foram considerados: consciência, onde o foco é a visualização dos dados de diferentes formas; reflexão, cujo objetivo é avaliar a relevância dos dados presentes nas visualizações, por meio de questões feitas pelos próprios professores; sensemaking, que diz respeito à análise e reflexão das respostas dadas às perguntas feitas, gerando novos insights. Com isso, foi desenvolvida uma ferramenta cujo objetivo é apresentar predições de desempenho de alunos de uma forma amigável para os professores, de modo a contribuir com o processo de ensino-aprendizagem e potencializar a interação entre alunos, professores e recursos em ambientes virtuais de aprendizagem.",
    url = "https://doi.org/10.25286/repa.v6i3.1688"
}

@article{ref166_4b4991,
    author = "Padakanti, Dhatri and Moraes, Márcia",
    title = "Learning Analytics Dashboard to Support Instructors: A Literature Review",
    year = "2023",
    doi = "10.4995/head23.2023.16119",
    abstract = {The purpose of this study is to conduct an evaluation of the literature on learning analytics dashboards in order to address the following research questions: "How do instructors use learning analytics dashboards to send notifications?" and "How can a learning analytics dashboard help students get reminded of the due dates for assignments, quizzes, and exams?". A total of 20 papers were analyzed. Although the majority of them discussed how students utilize dashboards to track their progress or compare their progress with their peers, none of them mentioned how dashboards may be utilized to automatically notify students or send reminders, reducing the amount of work instructors have to do. This can be taken into account when developing a dashboard for learning analytics in the future.},
    url = "https://doi.org/10.4995/head23.2023.16119"
}

@article{ref167_b32479,
    author = "Gruzd, Anatoliy and Conroy, Nadia",
    title = "Designing a learning analytics dashboard for Twitter-facilitated teaching",
    year = "2021",
    doi = "10.32920/14637885.v1",
    url = "https://doi.org/10.32920/14637885.v1"
}

@article{ref168_35981a,
    author = "Biedermann, Daniel and Schneider, Ján and Drachsler, Hendrik",
    title = "Implementation and Evaluation of a Trusted Learning Analytics Dashboard.",
    year = "2018"
}

@article{ref169_a04632,
    author = "Gujju, Krishnavamsi and Bandi, Sahithi and Moraes, Márcia",
    title = "Review of Learning Analytics Dashboards for Students",
    year = "2024",
    issn = "2367-3389",
    doi = "10.1007/978-3-031-65522-7\_27",
    url = "https://doi.org/10.1007/978-3-031-65522-7\_27"
}

@article{ref170_68791c,
    author = "Mohseni, Zeynab and Masiello, Italo and Martins, Rafael",
    title = "Co-Developing an Easy-to-Use Learning Analytics Dashboard for Teachers in Primary/Secondary Education: A Human-Centered Design Approach",
    year = "2023",
    issn = "2227-7102",
    doi = "10.3390/educsci13121190",
    abstract = "Learning Analytics Dashboards (LADs) can help provide insights and inform pedagogical decisions by supporting the analysis of large amounts of educational data, obtained from sources such as Digital Learning Materials (DLMs). Extracting requirements is a crucial step in developing a LAD, as it helps identify the underlying design problem that needs to be addressed. In fact, determining the problem that requires a solution is one of the primary objectives of requirements extraction. Although there have been studies on the development of LADs for K12 education, these studies have not specifically emphasized the use of a Human-Centered Design (HCD) approach to better comprehend the teachers’ requirements and produce more stimulating insights. In this paper we apply prototyping, which is widely acknowledged as a successful way for rapidly implementing cost-effective designs and efficiently gathering stakeholder feedback, to elicit such requirements. We present a three-step HCD approach, involving a design cycle that employs paper and interactive prototypes to guide the systematic and effective design of LADs that truly meet teacher requirements in primary/secondary education, actively engaging them in the design process. We then conducted interviews and usability testing to co-design and develop a LAD that can be used in classroom’s everyday learning activities. Our results show that the visualizations of the interactive prototype were easily interpreted by the participants, verifying our initial goal of co-developing an easy-to-use LAD.",
    url = "https://doi.org/10.3390/educsci13121190"
}

@article{ref171_5ab729,
    author = "Kitto, Simon and Chiang, Michelle and Ng, Olivia and Cleland, Jennifer",
    title = "More, better feedback please: are learning analytics dashboards (LAD) the solution to a wicked problem?",
    year = "2024",
    issn = "1382-4996",
    doi = "10.1007/s10459-024-10358-8",
    abstract = "Abstract There is a long-standing lack of learner satisfaction with quality and quantity of feedback in health professions education (HPE) and training. To address this, university and training programmes are increasingly using technological advancements and data analytic tools to provide feedback. One such educational technology is the Learning Analytic Dashboard (LAD), which holds the promise of a comprehensive view of student performance via partial or fully automated feedback delivered to learners in real time. The possibility of displaying performance data visually, on a single platform, so users can access and process feedback efficiently and constantly, and use this to improve their performance, is very attractive to users, educators and institutions. However, the mainstream literature tends to take an atheoretical and instrumentalist view of LADs, a view that uncritically celebrates the promise of LAD’s capacity to provide a ‘technical fix’ to the ‘wicked problem’ of feedback in health professions education. This paper seeks to recast the discussion of LADs as something other than a benign material technology using the lenses of Miller and Rose’s technologies of government and Barry’s theory of Technological Societies, where such technical devices are also inherently agentic and political. An examination of the purpose, design and deployment of LADs from these theoretical perspectives can reveal how these educational devices shape and govern the HPE learner body in different ways, which in turn, may produce a myriad of unintended– and ironic– effects on the feedback process. In this Reflections article we wish to encourage health professions education scholars to examine the practices and consequences thereof of the ever-expanding use of LADs more deeply and with a sense of urgency.",
    url = "https://doi.org/10.1007/s10459-024-10358-8"
}

@article{ref172_d3362b,
    author = "Thiruppugal, Mithila",
    title = "An Investigation Into the Design of Learning Analytic Dashboards (LAD) for the Enhancement of Motivation, Engagement and Achievements in an E-Learning Environment",
    year = "2020",
    issn = "1477-9358",
    doi = "10.14236/ewic/hci20dc.11",
    abstract = "The current scenario of higher education reflects an increased permeation of digital technology, with this integration has influenced both learning and teaching practices. One of the main impacts has been in the advancement of Learning Management Systems (LMS). By facilitating synchronous and asynchronous communication and interactions linked to a virtual environment, LMSs have become integral to higher education. Learning Analytic Dashboards (LAD) are the digital platforms used by educational institutions for collecting, measuring, analysing and reporting data concerning learners and their activities and achievements. Related to that, as an area of research and development, learning Analytics is a rapidly growing field of LMS and of particular significance to LAD design. Many universities are utilising LADs, but guidelines to support effective design underpinned by research are limited. This study aims to assess the role of LAD design in optimising learning by influencing factors such as student motivation, engagement and achievement. The significance of this study is that it will offer novel insights on principles that should be adopted while designing LADs capable of leading students towards better learning experience and performance. Based on implications from previous studies as well as an empirical investigation on the need for such tools, this study will develop a prototype of a LAD that befits with student-facing demands.",
    url = "https://doi.org/10.14236/ewic/hci20dc.11"
}

@article{ref173_19f226,
    author = "Jivet, Ioana and Scheffel, Maren and Specht, Marcus and Drachsler, Hendrik",
    title = "License to evaluate",
    year = "2018",
    doi = "10.1145/3170358.3170421",
    abstract = "Learning analytics can bridge the gap between learning sciences and data analytics, leveraging the expertise of both fields in exploring the vast amount of data generated in online learning environments. A typical learning analytics intervention is the learning dashboard, a visualisation tool built with the purpose of empowering teachers and learners to make informed decisions about the learning process. Related work has investigated learning dashboards, yet none have explored the theoretical foundation that should inform the design and evaluation of such interventions. In this systematic literature review, we analyse the extent to which theories and models from learning sciences have been integrated into the development of learning dashboards aimed at learners. Our analysis revealed that very few dashboard evaluations take into account the educational concepts that were used as a theoretical foundation for their design. Furthermore, we report findings suggesting that comparison with peers, a common reference frame for contextualising information on learning analytics dashboards, was not perceived positively by all learners. We summarise the insights gathered through our literature review in a set of recommendations for the design and evaluation of learning analytics dashboards for learners.",
    url = "https://doi.org/10.1145/3170358.3170421"
}

@article{ref174_505a5f,
    author = "Mouaici, Mohamed and Vignollet, Laurence and Galez, Christine and Etienne, M.",
    title = "Learning Analytics Dashboards for Professional Training - Challenges and Proposal",
    year = "2018"
}

@article{ref175_5af6db,
    author = "Park, Yeonjeong and Jo, Il‐Hyun",
    title = "Correction to: Factors that affect the success of learning analytics dashboards",
    year = "2019",
    issn = "1042-1629",
    doi = "10.1007/s11423-019-09703-1",
    url = "https://doi.org/10.1007/s11423-019-09703-1"
}

@article{ref176_a21335,
    author = "Safsouf, Yassine and Mansouri, Khalifa and Poirier, Franck",
    title = "Understand the influence of learning analytics dashboards on learner self-regulation and academic success",
    year = "2022",
    doi = "10.1109/educon52537.2022.9766741",
    abstract = "Since the beginning of the COVID-19 pandemic, many countries have adopted online education as an alternative to face-to-face courses. This has increased awareness of the importance of analyzing learning data left by students to improve and evaluate the learning process. This article presents a new tool, named TaBAT, created to work with different LMSs in the form of dashboards accessible online and allowing teachers to monitor the progress of their learners and at the same time and allow teachers to track the progress of their learners, while allowing learners to develop self-regulation skills and visualize their learning process. The results of a study conducted show that TaBAT helped learners to increase their progress and spend more time in the online course.",
    url = "https://doi.org/10.1109/educon52537.2022.9766741"
}

@article{ref177_b11dac,
    author = "Kaveri, Anceli and Silvola, Anni and Muukkonen, Hanni",
    title = "Supporting Student Agency with a Student-Facing Learning Analytics Dashboard",
    year = "2023",
    issn = "1929-7750",
    doi = "10.18608/jla.2023.7729",
    abstract = "Learning analytics dashboard (LAD) development has been criticized for being too data-driven and for developers lacking an understanding of the nontechnical aspects of learning analytics (LA). The ability of developers to address their understanding of learners as well as systematic efforts to involve students in the development process are central to creating pedagogically grounded student-facing dashboards. However, limited research is available about developer perceptions on supporting students with LA. We examined an interdisciplinary LA development team’s (IDT) perceptions of and intentions to support student agency, and the student-facing LAD development process. Qualitative content analysis supported by a social cognitive theory framework was conducted on interviews (N = 12) to analyze the IDT’s perceptions of student agency. IDT members had differing conceptions of student agency but agreed that it manifests in strategic study progression and planning, as well as in active interpretation and use of LA-based feedback. IDT members had differing views on student involvement in the LAD development process. Communication challenges within an IDT and limited resources were mentioned, impeding development work. The results of this study highlight the importance of fostering communication among IDT members about guiding pedagogical design principles and the systematic use of educational concepts in LA development processes.",
    url = "https://doi.org/10.18608/jla.2023.7729"
}

@article{ref178_3857ad,
    author = "Khelifi, Tesnim and Rabah, Nourhène and Le Grand, Bénédicte and Daoudi, Ibtissem",
    title = "EX-LAD: Explainable Learning Analytics Dashboard in Higher Education",
    year = "2024",
    issn = "2398-7340",
    doi = "10.29007/dsxd",
    abstract = "This paper introduces an EXplainable Learning Analytic Dashboard (EX-LAD) that presents learning analytics data on student performance, engagement, and perseverance in a clear and easily understandable manner. The main goal of this study is to make this information accessible to both teachers and students, who may not possess extensive knowledge in data analysis, and demonstrate the effectiveness of the relationship between performance, engagement, and perseverance in identifying student difficulties. This dashboard enables teachers to gain valuable information about their student’s progress, identify at-risk learners, and provide targeted support. Similarly, students can use this dashboard to track their own learning journey, identify their strengths and weaknesses, and make informed decisions to improve their academic performance. It integrates visualizations to represent various aspects of student learning, such as performance, engagement, and perseverance. To demonstrate the effectiveness of our dashboard, we conducted a case study using real data collected from ESIEE-IT, an engineering school in France, during the academic year 2021-2022. This case study serves as concrete evidence of the impact and values our dashboard brings to the educational context.",
    url = "https://doi.org/10.29007/dsxd"
}

@article{ref179_e261cb,
    author = "Duval, Erik",
    title = "Attention please!",
    year = "2011",
    doi = "10.1145/2090116.2090118",
    abstract = {This paper will present the general goal of and inspiration for our work on learning analytics, that relies on attention metadata for visualization and recommendation. Through information visualization techniques, we can provide a dashboard for learners and teachers, so that they no longer need to "drive blind". Moreover, recommendation can help to deal with the "paradox of choice" and turn abundance from a problem into an asset for learning.},
    url = "https://doi.org/10.1145/2090116.2090118"
}

@article{ref180_319cb3,
    author = "Lim, SungTae and Kim, Eunhee",
    title = "The Design of Dashboard for Instructor Feedback Support Based on Learning Analytics",
    year = "2017",
    issn = "1598-5016",
    doi = "10.32431/kace.2017.20.6.001",
    url = "https://doi.org/10.32431/kace.2017.20.6.001"
}

@article{ref181_c49ccf,
    author = "Sapsai, Iryna and Usme, Yeimy and Abke, Jörg",
    title = "Learning Analytics Dashboard for Educators: Proposed Project to Design with Pedagogical Background",
    year = "2023",
    doi = "10.1145/3593663.3593686",
    url = "https://doi.org/10.1145/3593663.3593686"
}

@article{ref182_c153d5,
    author = "Kannan, Priya and Zapata‐Rivera, Diego",
    title = "Facilitating the Use of Data From Multiple Sources for Formative Learning in the Context of Digital Assessments: Informing the Design and Development of Learning Analytic Dashboards",
    year = "2022",
    issn = "2504-284X",
    doi = "10.3389/feduc.2022.913594",
    abstract = "Learning analytic dashboards (LADs) are data visualization systems that use dynamic data in digital learning environments to provide students, teachers, and administrators with a wealth of information about student’s engagement, experiences, and performance on tasks. LADs have become increasingly popular, particularly in formative learning contexts, and help teachers make data-informed decisions about a student’s developing skills on a topic. LADs afford the possibility for teachers to obtain real-time data on student performance, response processes, and progress on academic learning tasks. However, data presented on LADs are often not based on an evaluation of stakeholder needs, and have been found to not be clearly interpretable and actionable for teachers to readily adapt their pedagogical actions based on these insights. We elaborate on how insights from research focused on interpretation and use of Score Reporting systems and research on open learner models (OLMs) can be used to inform a research agenda aimed at exploring the design and evaluation of LADs.",
    url = "https://doi.org/10.3389/feduc.2022.913594"
}

@article{ref183_531dda,
    author = "Kodumuru, Saketh and Lucas, Brendan and Sabanwar, Vivek and Patil, Sachin and Avudiappan, Deepa and Parikh, Parth and Arya, Kavi",
    title = "Towards developing a learning analytics dashboard for a massive online robotics competition",
    year = "2022",
    doi = "10.1109/educon52537.2022.9766808",
    url = "https://doi.org/10.1109/educon52537.2022.9766808"
}

@article{ref184_5b1d8e,
    author = "Ahmad, Atezaz and Schneider, Ján and Drachsler, Hendrik",
    title = "OpenLAIR an Open Learning Analytics Indicator Repository Dashboard",
    year = "2020",
    issn = "0302-9743",
    doi = "10.1007/978-3-030-57717-9\_46",
    url = "https://doi.org/10.1007/978-3-030-57717-9\_46"
}

@article{ref185_5cae17,
    author = "Il-Hyun, Jo and Kunhee, Ha and Park, Yeonjeong",
    title = "Measuring Information Perception in Learning Analytics Dashboard: Use of Eye-Tracking System",
    year = "2015",
    issn = "1229-7291",
    doi = "10.15833/kafeiam.21.3.441",
    url = "https://doi.org/10.15833/kafeiam.21.3.441"
}

@article{ref186_4f5823,
    author = "Broos, Tom and Verbert, Katrien and Langie, Greet and Van Soom, Carolien and De Laet, Tinne",
    title = "Low-Investment, Realistic-Return Business Cases for Learning Analytics Dashboards: Leveraging Usage Data and Microinteractions",
    year = "2018",
    issn = "0302-9743",
    doi = "10.1007/978-3-319-98572-5\_30",
    url = "https://doi.org/10.1007/978-3-319-98572-5\_30"
}

@article{ref187_38581c,
    author = "Akçayır, Gökçe and Wanderley, Leticia and Epp, Carrie and Hewitt, Jim and Mahmoudi-Nejad, Athar",
    title = "Learning Analytics Dashboard Use in Online Courses: Why and How Instructors Interpret Discussion Data",
    year = "2021",
    issn = "2662-2122",
    doi = "10.1007/978-3-030-81222-5\_17",
    url = "https://doi.org/10.1007/978-3-030-81222-5\_17"
}

@article{ref188_cdb0fc,
    author = "Martins, Ludmila and Molins, Laia and Cano, Elena and Puertas, Eloi",
    title = "LEARNING ANALYTICS DASHBOARD FOR SELF-REGULATION LEARNING: STUDENTS’ PERCEPTIONS OF LEARNING AND SATISFACTION",
    year = "2023",
    issn = "2340-1117",
    doi = "10.21125/edulearn.2023.0233",
    url = "https://doi.org/10.21125/edulearn.2023.0233"
}

@article{ref189_73f0a7,
    author = "Kaur, Amanpreet and Chahal, Kuljit",
    title = "A learning analytics dashboard for data-driven recommendations on influences of non-cognitive factors in introductory programming",
    year = "2023",
    issn = "1360-2357",
    doi = "10.1007/s10639-023-12125-5",
    url = "https://doi.org/10.1007/s10639-023-12125-5"
}

@article{ref190_a4995d,
    author = "Bodily, Robert",
    title = "Designing, Developing, and Implementing Real-Time Learning Analytics Student Dashboards",
    year = "2018"
}

@article{ref191_15c571,
    author = "Becerra, Álvaro and Daza, Roberto and Cobos, Ruth and Morales, Aythami and Fiérrez, Julián",
    title = "User Experience Study using a System for Generating MultiModal Learning Analytics Dashboards",
    year = "2023",
    doi = "10.1145/3612783.3612813",
    url = "https://doi.org/10.1145/3612783.3612813"
}

@article{ref192_be8ee1,
    author = "Herde, Nils",
    title = "The Vent Learning Analytics Dashboard - and VSON container format for visualization data",
    year = "2018"
}

@article{ref193_3e0ccd,
    author = "Joseph‐Richard, Paul",
    title = "Students as Lead Designers of Learning Analytics Dashboards: Lessons learned in a Northern Irish University",
    year = "2020",
    doi = "10.33422/2nd.rteconf.2020.03.47",
    abstract = "There is an increasing scholarly and practitioner interest in developing user-centred, personalized learning analytics dashboards (LADs) in higher education institutions, to support student success and improve learning and teaching.In most implementation efforts, however, a teacher-centric, institutional view tends to drive dashboard designs, while using students only as data providers.We stretched our engagement approach by empowering them to be the lead-designers of LADs to learn what data they would like presented in a dashboard, and how.Using a novel cord sorting technique, we asked 42 Northern Irish university students to construct dashboards that reflect their priorities.Using observation, photography and semi-structured interviews, we collected data on student-constructed dashboards.Content analysis of students' constructions revealed a strong preference for the inclusion of personal financial data (money spent so far vs resources utilized), among others, and exclusion of social media data.Thematic analysis of qualitative data uncovered within-group variations in students' LAD-related assumptions, particularly between undergraduates and postgraduates, and between international and home students.Participants challenged institutional overreliance on measurable digital footprints as proxies for academic success and emphasised the need for including success stories of their peers and seniors, in future dashboards.In advocating story-integrated LADs, we call for designs that better reflect learners' everyday needs and priorities.We provide a caution that offering genuine control and oversight to students for designing LADs, despite being useful, might be more complex than currently assumed in the literature.We discuss pedagogical implications for teachers and LA designers in advancing student-led LA designs.",
    url = "https://doi.org/10.33422/2nd.rteconf.2020.03.47"
}

@article{ref194_e0fd6e,
    author = "Buvari, Sebastian and Viberg, Olga and Iop, Alessandro and Romero, Mario",
    title = "A Student-Centered Learning Analytics Dashboard Towards Course Goal Achievement in STEM Education",
    year = "2023",
    issn = "0302-9743",
    doi = "10.1007/978-3-031-42682-7\_64",
    abstract = "Abstract Online learning has become an everyday form of learning for many students across different disciplines, including STEM subjects in the setting of higher education. Studying in these settings requires students to self-regulate their learning to a higher degree as compared to campus-based education. A vital aspect of self-regulated learning is the application of goal-setting strategies. Universities act to support students’ goal-setting through the achievement of course learning outcomes, which work both as a promise and metric of academic achievement. However, a lack of clear integration between course activities and course learning outcomes leaves a dissonance between students’ study efforts and the course progress. This demo study presents a student-centered learning analytics dashboard aimed at assisting students in their achievement of course learning goals in the setting of STEM higher education. The dashboard was designed using a design science methodological approach. Thirty-seven students have contributed to its development and evaluation during different stages of the design process, including the conceptual iterative design and prototyping. The preliminary results show that students found the tool to be easy to use and useful for the achievement of the course goals.",
    url = "https://doi.org/10.1007/978-3-031-42682-7\_64"
}

@article{ref195_cfa9b0,
    author = "da Silva, Rogério and de Souza Gimenes, Itana and Maldonado, José",
    title = "Analyzing learners' behavior and discourse within large online communities: a Social Learning Analytics Dashboard",
    year = "2021",
    doi = "10.5753/sbie.2021.217468",
    abstract = "Online Learning Communities (OLC) are nowadays one of the most important producers of Big Data in education. However, the investigation of such environments is underrepresented in educational research. There is a lack of methods and tools that characterize the massive learning associated with the student participation in large OLC. This paper presents a Social Learning Analytics Dashboard (SLAD) to analyze temporal trend models that outline the evolution of learners behavior over time. Such models suggest that ongoing collaboration and positive emotion have a fundamental role for knowledge creation and sharing in large scale social learning. These findings can be used to take actions in order to enhance and regulate social interaction within large OLC.",
    url = "https://doi.org/10.5753/sbie.2021.217468"
}

@article{ref196_2ee599,
    author = "Liu, Yiming and Huang, Lingyun and Doleck, Tenzin",
    title = "How teachers’ self-regulation, emotions, perceptions, and experiences predict their capacities for learning analytics dashboard: A Bayesian approach",
    year = "2023",
    issn = "1360-2357",
    doi = "10.1007/s10639-023-12163-z",
    url = "https://doi.org/10.1007/s10639-023-12163-z"
}

@article{ref197_657551,
    author = "Liu, Yiming and Hu, Xiao and Ng, Jeremy and Ma, Zhengyang and Lai, Xiaoyan",
    title = "Ready or not? Investigating in-service teachers’ integration of learning analytics dashboard for assessing students’ collaborative problem solving in K–12 classrooms",
    year = "2024",
    issn = "1360-2357",
    doi = "10.1007/s10639-024-12842-5",
    abstract = "Abstract Collaborative problem solving (CPS) has emerged as a crucial 21st century competence that benefits students’ studies, future careers, and general well-being, prevailing across disciplines and learning approaches. Given the complex and dynamic nature of CPS, teacher-facing learning analytics dashboards (LADs) have increasingly been adopted to support teachers’ CPS assessments by analysing and visualising various dimensions of students’ CPS. However, there is limited research investigating K-12 teachers’ integration of LADs for CPS assessments in authentic classrooms. In this study, a LAD was implemented to assist K-12 teachers in assessing students’ CPS skills in an educational game. Based on the person-environment fit theory, this study aimed to (1) examine the extent to which teachers’ environmental and personal factors influence LAD usage intention and behaviour and (2) identify personal factors mediating the relationships between environmental factors and LAD usage intention and behaviour. Survey data of 300 in-service teachers from ten Chinese K-12 schools were collected and analysed using partial least squares structural equation modelling (PLS-SEM). Results indicated that our proposed model showed strong in-sample explanatory power and out-of-sample predictive capability. Additionally, subjective norms affected technological pedagogical content knowledge (TPACK) and self-efficacy, while school support affected technostress and self-efficacy. Moreover, subjective norms, technostress, and self-efficacy predicted behavioural intention, while school support, TPACK, and behavioural intention predicted actual behaviour. As for mediation effects, school support indirectly affected behavioural intention through self-efficacy, while subjective norms indirectly affected behavioural intention through self-efficacy and affected actual behaviour through TPACK. This study makes theoretical, methodological, and practical contributions to technology integration in general and LAD implementation in particular.",
    url = "https://doi.org/10.1007/s10639-024-12842-5"
}

@article{ref198_d7a4bd,
    author = "de Vreugd, Lars and van Leeuwen, Anouschka and Jansen, Renée and van der Schaaf, Marieke",
    title = "Learning Analytics Dashboard Design and Evaluation to Support Student Self-Regulation of Study Behaviour",
    year = "2024",
    issn = "1929-7750",
    doi = "10.18608/jla.2024.8529",
    abstract = "For university students, self-regulation of study behaviour is important. However, students are not always capable of effective self-regulation. Providing study behaviour information via a learning analytics dashboard (LAD) may support phases within self-regulated learning (SRL). However, it is unclear what information a LAD should provide, how to present information in a usable manner, and what the information’s perceived usefulness is in supporting self-regulation of study behaviour. This study entails a sequential mixed design: assessing information needs in focus groups (n=7), exploring usability via think-aloud interviews (n=8), assessing usability with the System Usability Scale (n=42), and assessing perceived usefulness via interviews (n=16). Results showed that students and tutors agreed on the relevance of the constructs chosen from literature but differed in ranking the importance of new constructs. Usability exploration led to several design improvements. Perceived usefulness assessment showed the LAD supported the appraisal of study behaviour. A need for reference frames to facilitate data interpretation was vocalized. Impacts on study behaviour varied, possibly because preparatory activities were not used. Impact could be improved by further integrating the LAD into existing learning processes.",
    url = "https://doi.org/10.18608/jla.2024.8529"
}

@article{ref199_b4c6a9,
    author = "Gallagher, Timothy and Slof, Bert and van der Schaaf, Marieke and Arztmann, Michaela and Fracaro, Sofia and Kester, Liesbeth",
    title = "Learning analytics dashboard design: Workplace learner preferences for reference frames in immersive training in practice",
    year = "2024",
    issn = "0266-4909",
    doi = "10.1111/jcal.13042",
    abstract = "Abstract Background Learning analytics dashboards are increasingly being used to communicate feedback to learners. However, little is known about learner preferences for dashboard designs and how they differ depending on the self‐regulated learning (SRL) phases the dashboards are presented (i.e., forethought, performance, and self‐reflection phases) and SRL skills. Insight into design preferences for dashboards with different reference frames (i.e., progress, social, internal achievement and external achievement) is important because the effectiveness of feedback can depend upon how a learner perceives it. Objective This study examines workplace learner preferences for four dashboard designs for each SRL phase and how SRL skills relate to these preferences. Methods Seventy participants enrolled in a chemical process apprenticeship program took part in the study. Preferences were determined using a method of adaptive comparative judgement and SRL skills were measured using a questionnaire. Preferences were tested on four dashboard designs informed by social and temporal comparison theory and goal setting theory. Multinomial logistic regressions were used to examine the relationship between dashboard preferences and SRL. Results and Conclusions Results show that the progress reference frame is more preferred before and after task performance, and the social reference frame is less preferred before and after task performance. It was found that the higher the SRL skill score the higher the probability a learner preferred the progress reference frame compared to having no preference before task performance. The results are consistent with other findings, which suggest caution when using social comparison in designing dashboards which provide feedback.",
    url = "https://doi.org/10.1111/jcal.13042"
}

@article{ref200_2e73f6,
    author = "Alabi, Halimat",
    title = "Sensemaking with learning analytics visualizations: Investigating dashboard comprehension and effects on learning strategy",
    year = "2021"
}

@article{ref201_a34550,
    author = "Safsouf, Yassine and Mansouri, Khalifa and Poirier, Franck",
    title = "Enhanced Online Academic Success and Self-Regulation Through Learning Analytics Dashboards",
    year = "2023",
    issn = "1868-4238",
    doi = "10.1007/978-3-031-43393-1\_30",
    url = "https://doi.org/10.1007/978-3-031-43393-1\_30"
}

@article{ref202_e5733e,
    author = "Joseph‐Richard, Paul and Uhomoibhi, James",
    title = "Which Data Sets Are Preferred by University Students in Learning Analytics Dashboards? A Situated Learning Theory Perspective",
    year = "2023",
    issn = "1532-0545",
    doi = "10.1287/ited.2023.0289",
    abstract = "Scholarly interests in developing personalized learning analytics dashboards (LADs) in universities have been increasing. LADs are data visualization tools for both teachers and learners that allow them to support student success and improve teaching and learning. In most LADs, however, a teacher-centric, institutional view drives their designs, treating students only as passive end-users, which results in LADs being less useful to students. To address this limitation, we used a card-sorting technique and asked 42 students at a university in Northern Ireland to construct dashboards that reflect their priorities. Using a situated theory of learning as a lens and with the help of multiple qualitative methods, we collected data on what constitutes useful dashboards. Findings suggest that situated learning data sets, such as information on how students learn by talking and listening to others in their communities, need to be integrated into LADs. Students preferred to see the inclusion of qualitative narratives, self-directed learning data and financial information (money spent versus resources utilized) in LADs. As well as raising new questions on how such LADs could be designed, this study challenges institutional overreliance on measurable digital footprints as proxies for academic success. We call for recognizing the wider social learning that happens in landscapes of practice so that LADs become more useful to students.",
    url = "https://doi.org/10.1287/ited.2023.0289"
}

@article{ref203_506822,
    author = "Bayer, Vaclav and Mulholland, Paul and Hlosta, Martin and Frey, T. and Herodotou, Christothea and Fernández, Miriam",
    title = "Co‐creating an equality diversity and inclusion learning analytics dashboard for addressing awarding gaps in higher education",
    year = "2024",
    issn = "0007-1013",
    doi = "10.1111/bjet.13509",
    abstract = "Abstract Educational outcomes from traditionally underrepresented groups are generally worse than for their more advantaged peers. This problem is typically known as the awarding gap (we use the term awarding gap over ‘attainment gap’ as attainment places the responsibility on students to attain at equal levels) and continues to pose a challenge for educational systems across the world. While Learning Analytics (LA) dashboards help identify patterns contributing to the awarding gap, they often lack stakeholder involvement, offering very little support to institutional Equality, Diversity and Inclusion (EDI) leads or educators to pinpoint and address these gaps. This paper introduces an innovative EDI LA dashboard, co‐created with diverse stakeholders. Rigorously evaluated, the dashboard provides fine‐grained insights and course‐level analysis, empowering institutions to effectively address awarding gaps and contribute to a diverse and inclusive higher education landscape. Practitioners notes What is already known about this topic Traditionally underrepresented groups face educational disparities, commonly known as the awarding gap. Underachievement is a complex multi‐dimensional problem and cannot be solely attributable to individual student deficiencies. LA dashboards targeting this specific problem are often not public, there is little research about them, and are frequently designed with little involvement of educational stakeholders. What this paper adds Pioneers the introduction of a dashboard specifically designed to address the awarding gap problem. Emphasises the significant data needs of educational stakeholders in tackling awarding gaps. Expands the design dimensions of Learning Analytics (LA) by introducing a specific design approach rooted in established user experience (UX) design methods. Implications for practice and/or policy Insights from this study will guide practitioners, designers, and developers in creating AI‐based educational systems to effectively target the awarding gap problem.",
    url = "https://doi.org/10.1111/bjet.13509"
}

@article{ref204_6498df,
    author = "Ahrar, Arash and Doroodian, Mohammadreza and Hatala, Marek",
    title = "Exploring Eye-tracking Features to Understand Students' Sensemaking of Learning Analytics Dashboards",
    year = "2025",
    doi = "10.1145/3706468.3706543",
    url = "https://doi.org/10.1145/3706468.3706543"
}

@article{ref205_3cf55e,
    author = "Tretow-Fish, Tobias and Khalid, Saifuddin",
    title = "Applying Kano’s two-factor theory to prioritize learning analytics dashboard features for learning technology designers",
    year = "2024",
    issn = "1309-517X",
    doi = "10.30935/cedtech/14286",
    abstract = "Existing methods for software requirements elicitation, five-point Likert scales and voting methods for requirements prioritization, and usability and user experience evaluation methods do not enable prioritizing the learning analytics dashboard requirements. Inspired by management and product design field, this research applies Kano’s two-factor theory to prioritize the features of learning analytics dashboards (LADs) of adaptive learning platform (ALP) called Rhapsode\&lt;sup\&gt;TM\&lt;/sup\&gt; learner, based on students’ perceived usefulness to support designers’ decision-making. Comparing usability and user experience methods for evaluating LAD features, this paper contributes with the protocol and a case applying Kano method for evaluating the perceived importance of the dashboards in ALP. The paper applies Kano’s two-factor questionnaire on the 13 LADs features of Rhapsode\&lt;sup\&gt;TM\&lt;/sup\&gt; learner. Responses from 17 students are collected using a questionnaire, which is used to showcase the strength of the two-factor theory through five tabular and graphical techniques. Through these five tabular and graphical techniques, we demonstrate the application and usefulness of the method as designers and management are often carried away by the possibilities of insights instead of actual usefulness. The results revealed a variation in the categorization of LADs depending on the technique employed. As the complexity of the techniques increases, additional factors that indicate data uncertainty are gradually incorporated, clearly highlighting the growing requirement for data. In the case of RhapsodeTM learner platform, results based on the students responses show that 11 of 13 LADs being excluded due to low significance level in categorization (technique 1) and low response rate.",
    url = "https://doi.org/10.30935/cedtech/14286"
}

@article{ref206_206ab6,
    author = "Oliver-Quelennec, Katia and Bouchet, François and Carron, Thibault and Pinçon, Claire",
    title = "CAN A LEARNING ANALYTICS DASHBOARD PARTICIPATIVE DESIGN APPROACH BE TRANSPOSED TO AN ONLINE-ONLY CONTEXT?",
    year = "2021",
    doi = "10.33965/celda2021\_202108l008",
    abstract = "In-person sessions of participative design are commonly used in the field of Learning Analytics, but to reach students notalways available on-site (e.g. during a pandemic), they have to be adapted to online-only context. Card-based tools are",
    url = "https://doi.org/10.33965/celda2021\_202108l008"
}

@article{ref207_a8048c,
    author = "Jin, Yueqiao and Yang, Kaixun and Yan, Lixiang and Echeverría, Vanessa and Zhao, Linxuan and Alfredo, Riordan and Milesi, Mikaela and Fan, Jie and Li, Xinyu and Gašević, Dragan and Martínez‐Maldonado, Roberto",
    title = "Chatting with a Learning Analytics Dashboard: The Role of Generative AI Literacy on Learner Interaction with Conventional and Scaffolding Chatbots",
    year = "2024",
    doi = "10.48550/arxiv.2411.15597",
    abstract = "Learning analytics dashboards (LADs) simplify complex learner data into accessible visualisations, providing actionable insights for educators and students. However, their educational effectiveness has not always matched the sophistication of the technology behind them. Explanatory and interactive LADs, enhanced by generative AI (GenAI) chatbots, hold promise by enabling dynamic, dialogue-based interactions with data visualisations and offering personalised feedback through text. Yet, the effectiveness of these tools may be limited by learners' varying levels of GenAI literacy, a factor that remains underexplored in current research. This study investigates the role of GenAI literacy in learner interactions with conventional (reactive) versus scaffolding (proactive) chatbot-assisted LADs. Through a comparative analysis of 81 participants, we examine how GenAI literacy is associated with learners' ability to interpret complex visualisations and their cognitive processes during interactions with chatbot-assisted LADs. Results show that while both chatbots significantly improved learner comprehension, those with higher GenAI literacy benefited the most, particularly with conventional chatbots, demonstrating diverse prompting strategies. Findings highlight the importance of considering learners' GenAI literacy when integrating GenAI chatbots in LADs and educational technologies. Incorporating scaffolding techniques within GenAI chatbots can be an effective strategy, offering a more guided experience that reduces reliance on learners' GenAI literacy.",
    url = "https://doi.org/10.48550/arxiv.2411.15597"
}

@article{ref208_d83a98,
    author = "Nasseif, Halah",
    title = "Learning analytics and dashboards, examining course design and students’ behavior:A case study in Saudi Arabian Higher Education",
    year = "2021"
}

@article{ref209_55be66,
    author = "Deshpande, Kirti and Asbe, Shubham and Lugade, Akanksha and More, Yash and Bhalerao, Dipali and Partudkar, Anuradha",
    title = "Learning Analytics Powered Teacher Facing Dashboard to Visualize, Analyze Students’ Academic Performance and give Key DL(Deep Learning) Supported Key Recommendations for Performance Improvement.",
    year = "2023",
    doi = "10.1109/iconat57137.2023.10080832",
    url = "https://doi.org/10.1109/iconat57137.2023.10080832"
}

@article{ref210_5d11b6,
    author = "Tretow-Fish, Tobias and Khalid, Saifuddin and Leweke, Victor",
    title = "Prototyping the Learning Analytics Dashboards of an Adaptive Learning Platform: Faculty Perceptions Versus Designers’ Intentions",
    year = "2023",
    issn = "0302-9743",
    doi = "10.1007/978-3-031-34411-4\_36",
    url = "https://doi.org/10.1007/978-3-031-34411-4\_36"
}

@article{ref211_9c8d2e,
    author = "Yang, Yuhua and Hussin, Norriza and Zheng, Maoxing and Wang, Dan",
    title = "Investigation and the development of learning analytics dashboard in open and distance learning using big data mining",
    year = "2024",
    issn = "2630-5046",
    doi = "10.32629/jai.v7i5.919",
    abstract = {\&lt;p class="Abstract"\&gt;The main aim of this study is to provide universities with a way of examining and predicting student performance. The fundamental aim and purpose of this study is to help academic institutions to analyse and predict student performance. The credibility and accuracy of the model was examined by comparing the predicted results of the model with the observed values. And educational data mining techniques were used to create student profiles. Weighted gain, classification analysis, decision tree and rule induction were used in this study. The results of the study showed that the level of students' academic performance varied according to criteria such as academic structure, faculty, mode of enrolment and gender. In order to determine the relative importance of variables, the information weight gain technique was used after generating rule induction parameters and hidden rules between data. Using data mining techniques, we can obtain both guidelines to instruct students and information to help us identify them.\&lt;/p\&gt;},
    url = "https://doi.org/10.32629/jai.v7i5.919"
}

@article{ref212_b7977d,
    author = "Wang, Chao and Hu, Xiao and López, Nora and Ng, Jeremy",
    title = "Needs Analysis of Learning Analytics Dashboard for College Teacher Online Professional Learning in an International Training Initiative for the Global South",
    year = "2024",
    doi = "10.1145/3636555.3636932",
    url = "https://doi.org/10.1145/3636555.3636932"
}

@article{ref213_a2bdd6,
    author = "Jonathan, Christin and Tan, Jennifer and Koh, Elizabeth and Caleon, Imelda and Tay, Siu",
    title = "Enhancing students’ critical reading fluency, engagement and self-efficacy using self-referenced learning analytics dashboard visualizations",
    year = "2017"
}

@article{ref214_a8048c,
    author = "Jin, Yueqiao and Yang, Kaixun and Yan, Lixiang and Echeverría, Vanessa and Zhao, Linxuan and Alfredo, Riordan and Milesi, Mikaela and Fan, Jie and Li, Xinyu and Gašević, Dragan and Martínez‐Maldonado, Roberto",
    title = "Chatting with a Learning Analytics Dashboard: The Role of Generative AI Literacy on Learner Interaction with Conventional and Scaffolding Chatbots",
    year = "2025",
    doi = "10.1145/3706468.3706545",
    url = "https://doi.org/10.1145/3706468.3706545"
}

@article{ref215_3215ed,
    author = "Teasley, Stephanie and POPOV, V. and Bae, Jin-Seo and Elkins, Shannon",
    title = "Self-Regulated Learning Theory and Epistemic Network Analysis: Understanding University Students' Use of a Learning Analytics Dashboard",
    year = "2023",
    issn = "0749-7423",
    doi = "10.1108/s0749-742320230000022015",
    url = "https://doi.org/10.1108/s0749-742320230000022015"
}

@article{ref216_871676,
    author = "Colling, Leona and Kholin, Mareike and Meurers, Detmar",
    title = "A Learning Analytics Dashboard for K-12 English Teachers - Bridging the Gap Between Student Process Data and Teacher Needs",
    year = "2024",
    doi = "10.1145/3631700.3665228",
    url = "https://doi.org/10.1145/3631700.3665228"
}

@article{ref217_623245,
    author = "Santos, José and Govaerts, Sten and Verbert, Katrien and Duval, Erik",
    title = "Goal-oriented visualizations of activity tracking",
    year = "2012",
    doi = "10.1145/2330601.2330639",
    abstract = "Increasing motivation of students and helping them to reflect on their learning processes is an important driver for learning analytics research. This paper presents our research on the development of a dashboard that enables self-reflection on activities and comparison with peers. We describe evaluation results of four iterations of a design based research methodology that assess the usability, use and usefulness of different visualizations. Lessons learned from the different evaluations performed during each iteration are described. In addition, these evaluations illustrate that the dashboard is a useful tool for students. However, further research is needed to assess the impact on the learning process.",
    url = "https://doi.org/10.1145/2330601.2330639"
}

@article{ref218_4f6652,
    author = "Molenaar, Inge and Campen, C.A.N.",
    title = "How Teachers Make Dashboard Information Actionable",
    year = "2018",
    issn = "1939-1382",
    doi = "10.1109/tlt.2018.2851585",
    abstract = "This study investigates how teachers use dashboards in primary school classrooms. While learners practice on a tablet real-time data indicating learner progress and performance is displayed on teacher dashboards. This study examines how teachers use the dashboards, applying Verberts' learning analytics process model. Teacher dashboard consultations and resulting pedagogical actions were observed in 38 mathematics lessons. In stimulated recall interviews, the 38 teachers were asked to elaborate on how they reflect on and make sense of the information on the dashboard. The results showed that teachers consulted the dashboard on average 8.3 times per lesson. Teachers activated existing knowledge about students and the class to interpret dashboard information. Task and process feedback were the pedagogical actions most often used following dashboard consultation. Additionally, teachers who consulted the dashboard more often activated more and more diverse pedagogical knowledge to interpret the data and, consequently, gave more and more diverse feedback. These results indicated that teacher dashboards were indeed influencing teachers' pedagogical actions in their daily classroom activities. This study provided the first evidence that dashboards progressively impact teaching practice and initiate more profound behavioral changes as teachers become more proficient in using them.",
    url = "https://doi.org/10.1109/tlt.2018.2851585"
}

@article{ref219_1ebfdd,
    author = "Santos, José and Verbert, Katrien and Govaerts, Sten and Duval, Erik",
    title = "Addressing learner issues with StepUp!",
    year = "2013",
    doi = "10.1145/2460296.2460301",
    abstract = "This paper reports on our research on the use of learning analytics dashboards to support awareness, self-reflection, sensemaking and impact for learners. So far, little research has been done to evaluate such dashboards with students and to assess their impact on learning. In this paper, we present the results of an evaluation study of our dashboard, called StepUp!, and the extent to which it addresses issues and needs of our students. Through brainstorming sessions with our students, we identified and prioritized learning issues and needs. In a second step, we deployed StepUp! during one month and we evaluated to which extent our dashboard addresses the issues and needs identified earlier in different courses. The results show that our tool has potentially higher impact for students working in groups and sharing a topic than students working individually on different topics.",
    url = "https://doi.org/10.1145/2460296.2460301"
}

@article{ref220_6a5fdd,
    author = "Jivet, Ioana and Scheffel, Maren and Schmitz, Marcel and Robbers, Stefan and Specht, Marcus and Drachsler, Hendrik",
    title = "From students with love: An empirical study on learner goals, self-regulated learning and sense-making of learning analytics in higher education",
    year = "2020",
    issn = "1096-7516",
    doi = "10.1016/j.iheduc.2020.100758",
    abstract = "Unequal stakeholder engagement is a common pitfall of adoption approaches of learning analytics in higher education leading to lower buy-in and flawed tools that fail to meet the needs of their target groups. With each design decision, we make assumptions on how learners will make sense of the visualisations, but we know very little about how students make sense of dashboard and which aspects influence their sense-making. We investigated how learner goals and self-regulated learning (SRL) skills influence dashboard sense-making following a mixed-methods research methodology: a qualitative pre-study followed-up with an extensive quantitative study with 247 university students. We uncovered three latent variables for sense-making: transparency of design, reference frames and support for action. SRL skills are predictors for how relevant students find these constructs. Learner goals have a significant effect only on the perceived relevance of reference frames. Knowing which factors influence students' sense-making will lead to more inclusive and flexible designs that will cater to the needs of both novice and expert learners.",
    url = "https://doi.org/10.1016/j.iheduc.2020.100758"
}

@article{ref221_bc70b9,
    author = "Corrin, Linda and De Barba, Paula",
    title = "How do students interpret feedback delivered via dashboards?",
    year = "2015",
    doi = "10.1145/2723576.2723662",
    url = "https://doi.org/10.1145/2723576.2723662"
}

@article{ref222_b6a312,
    author = "Williamson, Ben",
    title = "Policy networks, performance metrics and platform markets: Charting the expanding data infrastructure of higher education",
    year = "2019",
    issn = "0007-1013",
    doi = "10.1111/bjet.12849",
    abstract = "Abstract Digital data are transforming higher education (HE) to be more student‐focused and metrics‐centred. In the UK, capturing detailed data about students has become a government priority, with an emphasis on using student data to measure, compare and assess university performance. The purpose of this paper is to examine the governmental and commercial drivers of current large‐scale technological efforts to collect and analyse student data in UK HE. The result is an expanding data infrastructure which includes large‐scale and longitudinal datasets, learning analytics services, student apps, data dashboards and digital learning platforms powered by artificial intelligence (AI). Education data scientists have built positive pedagogic cases for student data analysis, learning analytics and AI. The politicization and commercialization of the wider HE data infrastructure is translating them into performance metrics in an increasingly market‐driven sector, raising the need for policy frameworks for ethical, pedagogically valuable uses of student data in HE. Practitioner Notes What is already known about this topic Learning analytics, education data science and artificial intelligence are opening up new ways of collecting and analysing student data in higher education. UK government policies emphasize the use of student data for improvements to teaching and learning. What this paper adds A conceptual framework from “infrastructure studies” demonstrates how political objectives and commercial aims are fused to HE data systems, with data infrastructure becoming a key tool of government reform. A critical infrastructure analysis shows that student data processing technologies are being developed and deployed to measure university performance through student data. Implications for practice and/or policy Educators and managers in universities need to prepare robust institutional frameworks to govern their use of student data. Learning analytics practitioners, data scientists, learning scientists and social science researchers need to collaborate with the policy community and education technology developers on new policy frameworks to challenge narrow uses of student data as performance metrics.",
    url = "https://doi.org/10.1111/bjet.12849"
}

@article{ref223_f90608,
    author = "Jones, Kyle",
    title = "Learning analytics and higher education: a proposed model for establishing informed consent mechanisms to promote student privacy and autonomy",
    year = "2019",
    issn = "2365-9440",
    doi = "10.1186/s41239-019-0155-0",
    abstract = "By tracking, aggregating, and analyzing student profiles along with students' digital and analog behaviors captured in information systems, universities are beginning to open the black box of education using learning analytics technologies. However, the increase in and usage of sensitive and personal student data present unique privacy concerns. I argue that privacy-as-control of personal information is autonomy promoting, and that students should be informed about these information flows and to what ends their institution is using them. Informed consent is one mechanism by which to accomplish these goals, but Big Data practices challenge the efficacy of this strategy. To ensure the usefulness of informed consent, I argue for the development of Platform for Privacy Preferences (P3P) technology and assert that privacy dashboards will enable student control and consent mechanisms, while providing an opportunity for institutions to justify their practices according to existing norms and values.",
    url = "https://doi.org/10.1186/s41239-019-0155-0"
}

@article{ref224_c1758c,
    author = "Kokoç, Mehmet and Altun, Arif",
    title = "Effects of learner interaction with learning dashboards on academic performance in an e-learning environment",
    year = "2019",
    issn = "0144-929X",
    doi = "10.1080/0144929x.2019.1680731",
    url = "https://doi.org/10.1080/0144929x.2019.1680731"
}

@article{ref225_55bec8,
    author = "Wise, Alyssa and Jung, Yeonji",
    title = "Teaching with Analytics: Towards a Situated Model of Instructional Decision-Making",
    year = "2019",
    issn = "1929-7750",
    doi = "10.18608/jla.2019.62.4",
    abstract = "\&\#x0D; \&\#x0D; \&\#x0D; The process of using analytic data to inform instructional decision-making is acknowledged to be complex; however, details of how it occurs in authentic teaching contexts have not been fully unpacked. This study investigated five university instructors’ use of a learning analytics dashboard to inform their teaching. The existing literature was synthesized to create a template for inquiry that guided interviews, and inductive qualitative analysis was used to identify salient emergent themes in how instructors 1) asked questions, 2) interpreted data, 3) took action, and 4) checked impact. Findings showed that instructors did not always come to analytics use with specific questions, but rather with general areas of curiosity. Questions additionally emerged and were refined through interaction with the analytics. Data interpretation involved two distinct activities, often along with affective reactions to data: reading data toidentify noteworthy patterns and explaining their importance in the course using contextual knowledge. Pedagogical responses to the analytics included whole-class scaffolding, targeted scaffolding, and revising course design, as well two new non-action responses: adopting a wait-and-see posture and engaging in deep reflection on pedagogy. Findings were synthesized into a model of instructor analytics use that offers useful categories of activities for future study and support \&\#x0D; \&\#x0D; \&\#x0D;",
    url = "https://doi.org/10.18608/jla.2019.62.4"
}

@article{ref226_8c4a07,
    author = "Boscardin, Christy and Fergus, Kirkpatrick and Hellevig, Bonnie and Hauer, Karen",
    title = "Twelve tips to promote successful development of a learner performance dashboard within a medical education program",
    year = "2017",
    issn = "0142-159X",
    doi = "10.1080/0142159x.2017.1396306",
    url = "https://doi.org/10.1080/0142159x.2017.1396306"
}

@article{ref227_9bdfa1,
    author = "Afzaal, Muhammad and Nouri, Jalal and Zia, Aayesha and Papapetrou, Panagiotis and Fors, Uno and Wu, Yongchao and Li, Xiu and Weegar, Rebecka",
    title = "Explainable AI for Data-Driven Feedback and Intelligent Action Recommendations to Support Students Self-Regulation",
    year = "2021",
    issn = "2624-8212",
    doi = "10.3389/frai.2021.723447",
    abstract = "Formative feedback has long been recognised as an effective tool for student learning, and researchers have investigated the subject for decades. However, the actual implementation of formative feedback practices is associated with significant challenges because it is highly time-consuming for teachers to analyse students' behaviours and to formulate and deliver effective feedback and action recommendations to support students' regulation of learning. This paper proposes a novel approach that employs learning analytics techniques combined with explainable machine learning to provide automatic and intelligent feedback and action recommendations that support student's self-regulation in a data-driven manner, aiming to improve their performance in courses. Prior studies within the field of learning analytics have predicted students' performance and have used the prediction status as feedback without explaining the reasons behind the prediction. Our proposed method, which has been developed based on LMS data from a university course, extends this approach by explaining the root causes of the predictions and by automatically providing data-driven intelligent recommendations for action. Based on the proposed explainable machine learning-based approach, a dashboard that provides data-driven feedback and intelligent course action recommendations to students is developed, tested and evaluated. Based on such an evaluation, we identify and discuss the utility and limitations of the developed dashboard. According to the findings of the conducted evaluation, the dashboard improved students' learning outcomes, assisted them in self-regulation and had a positive effect on their motivation.",
    url = "https://doi.org/10.3389/frai.2021.723447"
}

@article{ref228_87aa62,
    author = "Khan, Imran and Pardo, Abelardo",
    title = "Data2U",
    year = "2016",
    doi = "10.1145/2883851.2883911",
    url = "https://doi.org/10.1145/2883851.2883911"
}

@article{ref229_114ecf,
    author = "Schwendimann, Beat and Rodríguez‐Triana, María and Vozniuk, Andrii and Prieto, Luis and Boroujeni, Mina and Holzer, Adrian and Gillet, Denis and Dillenbourg, Pierre",
    title = "Understanding learning at a glance",
    year = "2016",
    doi = "10.1145/2883851.2883930",
    abstract = "Research on learning dashboards aims to identify what data is meaningful to different stakeholders in education, and how data can be presented to support sense-making processes. This paper summarizes the main outcomes of a systematic literature review on learning dashboards, in the fields of Learning Analytics and Educational Data Mining. The query was run in five main academic databases and enriched with papers coming from GScholar, resulting in 346 papers out of which 55 were included in the final analysis. Our review distinguishes different kinds of research studies as well as different aspects of learning dashboards and their maturity in terms of evaluation. As the research field is still relatively young, many of the studies are exploratory and proof-of-concept. Among the main open issues and future lines of work in the area of learning dashboards, we identify the need for longitudinal research in authentic settings, as well as studies that systematically compare different dashboard design options.",
    url = "https://doi.org/10.1145/2883851.2883930"
}

@article{ref230_a6e32c,
    author = "Knight, David and Brozina, Cory and Novoselich, Brian",
    title = "An Investigation of First-Year Engineering Student and Instructor Perspectives of Learning Analytics Approaches",
    year = "2016",
    issn = "1929-7750",
    doi = "10.18608/jla.2016.33.11",
    abstract = "This paper investigates how first-year engineering undergraduates and their instructors describe the potential for learning analytics approaches to contribute to students’ success. Results of qualitative data collection in a first-year engineering course indicated that both students and instructors emphasized a preference for learning analytics systems to focus on aggregate as opposed to individual data. Another consistent theme across students and instructors was an interest in bringing data related to time (e.g., how time is spent outside of class) into learning analytics products. Students’ and instructors’ viewpoints diverged in the “level” at which they would find a learning analytics dashboard useful—instructors remained focused on a specific class, but students drove the conversation to a much broader scope at the major or university level but in a discipline-specific manner. Such practices that select relevant data and develop models with learners and teachers instead of for learners and teachers should better inform development of and, ultimately, sustainable use of learning analytics-based models and dashboards.",
    url = "https://doi.org/10.18608/jla.2016.33.11"
}

@article{ref231_aeb18f,
    author = "Swenson, Jenni",
    title = "Establishing an ethical literacy for learning analytics",
    year = "2014",
    doi = "10.1145/2567574.2567613",
    url = "https://doi.org/10.1145/2567574.2567613"
}

@article{ref232_6a2fa0,
    author = "Reimers, Gabriel and Neovesky, Anna",
    title = "Student Focused Dashboards - An Analysis of Current Student Dashboards and What Students Really Want",
    year = "2015",
    doi = "10.5220/0005475103990404",
    url = "https://doi.org/10.5220/0005475103990404"
}

@article{ref233_d12bdf,
    author = "Zheng, Lanqin and Zhong, Lu and Niu, Jiayu",
    title = "Effects of personalised feedback approach on knowledge building, emotions, co-regulated behavioural patterns and cognitive load in online collaborative learning",
    year = "2021",
    issn = "0260-2938",
    doi = "10.1080/02602938.2021.1883549",
    url = "https://doi.org/10.1080/02602938.2021.1883549"
}

@article{ref234_7e0c9b,
    author = "Herodotou, Christothea and Hlosta, Martin and Boroowa, Avinash and Rienties, Bart and Zdráhal, Zdeněk and Mangafa, Chrysoula",
    title = "Empowering online teachers through predictive learning analytics",
    year = "2019",
    issn = "0007-1013",
    doi = "10.1111/bjet.12853",
    url = "https://doi.org/10.1111/bjet.12853"
}

@article{ref235_cfbf8b,
    author = "Kia, Fatemeh and Teasley, Stephanie and Hatala, Marek and Karabenick, Stuart and Kay, Matthew",
    title = "How patterns of students dashboard use are related to their achievement and self-regulatory engagement",
    year = "2020",
    doi = "10.1145/3375462.3375472",
    url = "https://doi.org/10.1145/3375462.3375472"
}

@article{ref236_9a14ab,
    author = "Hellings, J.A. and Haelermans, Carla",
    title = "The effect of providing learning analytics on student behaviour and performance in programming: a randomised controlled experiment",
    year = "2020",
    issn = "0018-1560",
    doi = "10.1007/s10734-020-00560-z",
    abstract = "Abstract We use a randomised experiment to study the effect of offering half of 556 freshman students a learning analytics dashboard and a weekly email with a link to their dashboard, on student behaviour in the online environment and final exam performance. The dashboard shows their online progress in the learning management systems, their predicted chance of passing, their predicted grade and their online intermediate performance compared with the total cohort. The email with dashboard access, as well as dashboard use, has positive effects on student behaviour in the online environment, but no effects are found on student performance in the final exam of the programming course. However, we do find differential effects by specialisation and student characteristics.",
    url = "https://doi.org/10.1007/s10734-020-00560-z"
}

@article{ref237_21d3a5,
    author = "Kotsiantis, Sotiris and Τσέλιος, Νικόλαος and Filippidi, Andromahi and Komis, Vassilis",
    title = "Using learning analytics to identify successful learners in a blended learning course",
    year = "2013",
    issn = "1753-5255",
    doi = "10.1504/ijtel.2013.059088",
    url = "https://doi.org/10.1504/ijtel.2013.059088"
}

@article{ref238_cf7bb6,
    author = "Tarmazdi, Hamid and Vivian, Rebecca and Szabo, Claudia and Falkner, Katrina and Falkner, Nickolas",
    title = "Using Learning Analytics to Visualise Computer Science Teamwork",
    year = "2015",
    doi = "10.1145/2729094.2742613",
    url = "https://doi.org/10.1145/2729094.2742613"
}

@article{ref239_9cdac8,
    author = "Pijeira‐Díaz, Héctor and Drachsler, Hendrik and Järvelä, Sanna and Kirschner, Paul",
    title = "Investigating collaborative learning success with physiological coupling indices based on electrodermal activity",
    year = "2016",
    doi = "10.1145/2883851.2883897",
    url = "https://doi.org/10.1145/2883851.2883897"
}

@article{ref240_75a353,
    author = "van Alphen, Erik and Bakker, Saskia",
    title = "Lernanto",
    year = "2016",
    doi = "10.1145/2851581.2892524",
    url = "https://doi.org/10.1145/2851581.2892524"
}

@article{ref241_671252,
    author = "Broos, Tom and Pinxten, Maarten and Delporte, Margaux and Verbert, Katrien and De Laet, Tinne",
    title = "Learning dashboards at scale: early warning and overall first year experience",
    year = "2019",
    issn = "0260-2938",
    doi = "10.1080/02602938.2019.1689546",
    abstract = "In this study, we present a case study involving two self-service dashboards providing feedback on learning and study skills and on academic achievement. These dashboards were offered to first-year university students in several study programmes in Flanders, Belgium. Data for this study were collected using usage tracking (N = 2875) and a survey taken at the beginning of the second year before (N = 484) and after (N = 538) the introduction of the dashboards. We found that early dashboard usage is related to academic achievement later in the academic year and that students' review of the feedback received in the first year improved. Although these results are modest in comparison to how high the bar is sometimes set for learning analytics applications, we argue that low-cost deployments of self-service dashboards are an interesting approach to start building experience with similar tools and to start paving the way for future developments.",
    url = "https://doi.org/10.1080/02602938.2019.1689546"
}

@article{ref242_3a0789,
    author = "Bennett, Liz and Folley, Sue",
    title = "Four design principles for learner dashboards that support student agency and empowerment",
    year = "2019",
    issn = "1758-1184",
    doi = "10.1108/jarhe-11-2018-0251",
    url = "https://doi.org/10.1108/jarhe-11-2018-0251"
}

@article{ref243_fa57bc,
    author = "de Quincey, Ed and Briggs, Chris and Kyriacou, Theocharis and Waller, Richard",
    title = "Student Centred Design of a Learning Analytics System",
    year = "2019",
    doi = "10.1145/3303772.3303793",
    abstract = "Current Learning Analytics (LA) systems are primarily designed with University staff members as the target audience; very few are aimed at students, with almost none being developed with direct student involvement and undertaking a comprehensive evaluation. This paper describes a HEFCE funded project that has employed a variety of methods to engage students in the design, development and evaluation of a student facing LA dashboard. LA was integrated into the delivery of 4 undergraduate modules with 169 student sign-ups. The design of the dashboard uses a novel approach of trying to understand the reasons why students want to study at university and maps their engagement and predicted outcomes to these motivations, with weekly personalised notifications and feedback. Students are also given the choice of how to visualise the data either via a chart-based view or to be represented as themselves. A mixed-methods evaluation has shown that students' feelings of dependability and trust of the underlying analytics and data is variable. However, students were mostly positive about the usability and interface design of the system and almost all students once signed-up did interact with their LA. The majority of students could see how the LA system could support their learning and said that it would influence their behaviour. In some cases, this has had a direct impact on their levels of engagement. The main contribution of this paper is the transparent documentation of a User Centred Design approach that has produced forms of LA representation, recommendation and interaction design that go beyond those used in current similar systems and have been shown to motivate students and impact their learning behaviour.",
    url = "https://doi.org/10.1145/3303772.3303793"
}

@article{ref244_9e9e1f,
    author = "Charleer, Sven and Klerkx, Joris and Santos, José and Duval, Erik",
    title = "Improving awareness and reflection through collaborative, interactive visualizations of badges.",
    year = "2013"
}

@article{ref245_f86be6,
    author = "Mejía, Carolina and Florián, Beatriz and Vatrapu, Ravi and Bull, Susan and Gómez, Sergio and Fabregat, Ramón",
    title = "A Novel Web-Based Approach for Visualization and Inspection of Reading Difficulties on University Students",
    year = "2016",
    issn = "1939-1382",
    doi = "10.1109/tlt.2016.2626292",
    url = "https://doi.org/10.1109/tlt.2016.2626292"
}

@article{ref246_210279,
    author = "Teasley, Stephanie",
    title = "Learning analytics: where information science and the learning sciences meet",
    year = "2018",
    issn = "2398-5348",
    doi = "10.1108/ils-06-2018-0045",
    url = "https://doi.org/10.1108/ils-06-2018-0045"
}

@article{ref247_31720c,
    author = "Olivares, Daniel",
    title = "Exploring Learning Analytics for Computing Education",
    year = "2015",
    doi = "10.1145/2787622.2787746",
    url = "https://doi.org/10.1145/2787622.2787746"
}

@article{ref248_b1e75d,
    author = "Gruzd, Anatoliy and Conroy, Nadia",
    title = "Learning Analytics Dashboard for Twitter",
    year = "2021",
    doi = "10.32920/ryerson.14639625.v1",
    abstract = "\&lt;p\&gt;Considering the increasing use of Twitter for both formal and informal learning, the primary goal of this project is to design a Learning Analytics (LA) dashboard to support instructors’ evaluation of Twitter-based teaching. To achieve this goal, we conducted an online survey involving 54 higher education instructors who have used Twitter in their past teaching. The main purpose was to identify why instructors use Twitter and what types of analytics they would consider valuable. The results of the survey evidence that instructors use Twitter to help students engage with class material, promote discussion, and build learning communities. Instructors expressed interest in analytical tools to help them quantitatively and qualitatively interpret Twitter data. Coupled with an in-depth literature review in this area, we relied on the survey data to prototype a Learning Analytics dashboard (https://dashboard.socialmediadata.org/educhat). Our online dashboard uses a simple, easy-to-read interface in accordance with previous successful dashboard implementations. Graphical visualizations allow instructors to monitor discussion patterns, such as the frequency and times of posting. Visual content breakdowns by number of retweets, original posts, and topics in the form of hashtags and named entities reveal the constituents of students’ posts. The dashboard provides additional analysis in the form of sentiment and subjectivity ranking as a way to contextually aid qualitative assessment. To support instructors’ awareness of class participation, we incorporated two visualizations that highlight the most active users and individuals who are most frequently mentioned in others’ tweets. Instructors can use the dashboard to gauge the participation at the individual- or classroom-level, and further discover what topics and links students discuss and share on Twitter. Three instructors piloted the LA dashboard over a 4-month semester in the Fall of 2017. Following their use, we conducted evaluation interviews with these instructors. Instructor evaluations confirmed that the proposed design is aligned with their pedagogical needs; they favored an intuitive interface that combined summative metrics for the entire class and personalized assessment of individual students. Based on instructors’ feedback, our future work will iteratively refine the design by integrating additional interactive features to adjust time scales of the output, investigate source data, collect data from lists of Twitter users (as opposed to a single hashtag), and further integrate the dashboard with other LMS (Learning Management System) data.\&lt;/p\&gt;",
    url = "https://doi.org/10.32920/ryerson.14639625.v1"
}

@article{ref249_7c9428,
    author = "Fu, Xinyu and Shimada, Atsushi and Ogata, Hiroaki and Taniguchi, Yuta and Suehiro, Daiki",
    title = "Real-time learning analytics for C programming language courses",
    year = "2017",
    doi = "10.1145/3027385.3027407",
    url = "https://doi.org/10.1145/3027385.3027407"
}

@article{ref250_0d3df9,
    author = "Kuhnel, Matthias and Seiler, Luisa and Honal, Andrea and Ifenthaler, Dirk",
    title = "Mobile learning analytics in higher education: usability testing and evaluation of an app prototype",
    year = "2018",
    issn = "1741-5659",
    doi = "10.1108/itse-04-2018-0024",
    abstract = "Purpose The purpose of the study was to test the usability of the MyLA app prototype by its potential users. Furthermore, the Web app will be introduced in the framework of “Mobile Learning Analytics”, a cooperation project between the Cooperative State University Mannheim and University of Mannheim. The participating universities focus on the support of personalized and self-regulated learning. MyLA collects data such as learning behavior, as well as personality traits. Last but not least, the paper will contribute to the topic of learning analytics and mobile learning in higher education. Design/methodology For the empirical investigation, a mixed-method design was chosen. While 105 participants took part in the conducted online survey, after testing the app prototype, seven students joined an additional eye tracking study. For the quantitative part, a selected question pool from HIMATT (highly integrated model assessment technology and tools) instrument was chosen. The eye tracking investigation consisted of three tasks the participants had to solve. Findings The findings showed that the students assessed the idea of the app, as well as the navigation positively. Only the color scheme of the prototype was not very attractive to a noticeable amount of the participants. So, it requires slight modifications concerning the app design. For the eye tracking study, it can be stated that the students viewed the relevant parts, and they basically had no difficulties to solve the tasks. Originality/value Due to the empirical testing of the app prototype, the project team was able to adjust the application and to add further features. Furthermore, the backend was programmed and an additional tool (MyLA dashboard) was developed for lecturers. A mutual understanding of the targets, privacy issue and relevant features are indispensable for further development of the project.",
    url = "https://doi.org/10.1108/itse-04-2018-0024"
}

@article{ref251_b1e75d,
    author = "Gruzd, Anatoliy and Conroy, Nadia",
    title = "Learning Analytics Dashboard for Twitter",
    year = "2021",
    doi = "10.32920/ryerson.14639625",
    abstract = "\&lt;p\&gt;Considering the increasing use of Twitter for both formal and informal learning, the primary goal of this project is to design a Learning Analytics (LA) dashboard to support instructors’ evaluation of Twitter-based teaching. To achieve this goal, we conducted an online survey involving 54 higher education instructors who have used Twitter in their past teaching. The main purpose was to identify why instructors use Twitter and what types of analytics they would consider valuable. The results of the survey evidence that instructors use Twitter to help students engage with class material, promote discussion, and build learning communities. Instructors expressed interest in analytical tools to help them quantitatively and qualitatively interpret Twitter data. Coupled with an in-depth literature review in this area, we relied on the survey data to prototype a Learning Analytics dashboard (https://dashboard.socialmediadata.org/educhat). Our online dashboard uses a simple, easy-to-read interface in accordance with previous successful dashboard implementations. Graphical visualizations allow instructors to monitor discussion patterns, such as the frequency and times of posting. Visual content breakdowns by number of retweets, original posts, and topics in the form of hashtags and named entities reveal the constituents of students’ posts. The dashboard provides additional analysis in the form of sentiment and subjectivity ranking as a way to contextually aid qualitative assessment. To support instructors’ awareness of class participation, we incorporated two visualizations that highlight the most active users and individuals who are most frequently mentioned in others’ tweets. Instructors can use the dashboard to gauge the participation at the individual- or classroom-level, and further discover what topics and links students discuss and share on Twitter. Three instructors piloted the LA dashboard over a 4-month semester in the Fall of 2017. Following their use, we conducted evaluation interviews with these instructors. Instructor evaluations confirmed that the proposed design is aligned with their pedagogical needs; they favored an intuitive interface that combined summative metrics for the entire class and personalized assessment of individual students. Based on instructors’ feedback, our future work will iteratively refine the design by integrating additional interactive features to adjust time scales of the output, investigate source data, collect data from lists of Twitter users (as opposed to a single hashtag), and further integrate the dashboard with other LMS (Learning Management System) data.\&lt;/p\&gt;",
    url = "https://doi.org/10.32920/ryerson.14639625"
}

@article{ref252_65ac93,
    author = "Verbert, Katrien",
    title = "LADA: Learning Analytics Dashboard Applications",
    year = "2013"
}

@article{ref253_f44a5d,
    author = "Dimov, J.",
    title = "Infrastructure of learning analytics dashboard",
    year = "2017"
}

@article{ref254_5d0640,
    author = "Şahi̇n, Muhittin and Yurdugül, Halil",
    title = "Educational Data Mining and Learning Analytics: Past, Present and Future",
    year = "2020",
    issn = "1308-7177",
    doi = "10.14686/buefad.606077",
    abstract = "Educational data mining and learning analytics have recently emerged as two important fields aimed at rendering e-learning environments more effective. Aim of this study seeks first to reveal the differences between these two fields and then to discuss the future of these concepts by evaluating how they changed throughout history. Educational data mining refers to uncovering the patterns hidden in the big data whilst learning analytics is the use of these patterns to optimize e-learning environments. One of the purposes of the study is to add to the literature on the future trends regarding these concepts. In the very near future, it seems that studies will be performed on EDM and the Industry 4.0 and one of its application areas, “(Internet of Things-IoT)” and EDM has the potential to substantially help researchers in discovering the patterns in the interaction data in the Learning Management Systems and in designing more effective learning environments. The studies on the future of learning analytics are categorized in five main headings: personalization of learning processes, learning design, learning experience design, dashboard design and the Industry 4.0 applications.",
    url = "https://doi.org/10.14686/buefad.606077"
}

@article{ref255_dd4ad6,
    author = "Orduña, Pablo and Almeida, Aitor and López–de–Ipiña, Diego and Garcia-Zúbia, Javier",
    title = "Learning Analytics on federated remote laboratories: Tips and techniques",
    year = "2014",
    doi = "10.1109/educon.2014.6826107",
    url = "https://doi.org/10.1109/educon.2014.6826107"
}

@article{ref256_5921a5,
    author = "Ruipérez‐Valiente, José and Gómez, Manuel and Andreo‐Martínez, Pedro and Kim, Yoon",
    title = "Ideating and Developing a Visualization Dashboard to Support Teachers Using Educational Games in the Classroom",
    year = "2021",
    issn = "2169-3536",
    doi = "10.1109/access.2021.3086703",
    abstract = "Technology has become an integral part of our everyday life, and its use in educational environments keeps growing. Additionally, video games are one of the most popular mediums across cultures and ages. There is ample evidence that supports the benefits of using games for learning and assessment, and educators are mainly supportive of using games in classrooms. However, we do not usually find educational games within the classroom activities. One of the main problems is that teachers report difficulties to actually know how their students are using the game so that they can analyze properly the effect of the activity and the interaction of students. To support teachers, educational games should incorporate learning analytics to transform data generated by students when playing useful information in a friendly and understandable way. For this work, we build upon Shadowspect, a 3D geometry puzzle game that has been used by teachers in a group of schools in the US. We use learning analytics techniques to generate a set of metrics implemented in a live dashboard that aims to facilitate that teachers can understand students' interaction with Shadowspect. We depict the multidisciplinary design process that we have followed to generate the metrics and the dashboard with great detail. Finally, we also provide uses cases that exemplify how teachers can use the dashboard to understand the global progress of their class and each of their students at an individual level, in order to intervene, adapt their classes and provide personalize feedback when appropriate.",
    url = "https://doi.org/10.1109/access.2021.3086703"
}

@article{ref257_ca8c16,
    author = "Cobos, Ruth and Gil, Silvia and Lareo, Ángel and Vargas, Francisco",
    title = "Open-DLAs",
    year = "2016",
    doi = "10.1145/2876034.2893430",
    url = "https://doi.org/10.1145/2876034.2893430"
}

@article{ref258_baf870,
    author = "West, Deborah and Luzeckyj, Ann and Toohey, Danny and Vanderlelie, Jessica and Searle, Bill",
    title = "Do academics and university administrators really know better? The ethics of positioning student perspectives in learning analytics",
    year = "2019",
    issn = "1449-3098",
    doi = "10.14742/ajet.4653",
    abstract = "Increasingly learning analytics (LA) has begun utilising staff- and student-facing dashboards capturing visualisations to present data to support student success and improve learning and teaching. The use of LA is complex, multifaceted and raises many issues for consideration, including ethical and legal challenges, competing stakeholder views and implementation decisions. It is widely acknowledged that LA development requires input from various stakeholders. This conceptual article explores the LA literature to determine how student perspectives are positioned as dashboards and visualisations are developed. While the sector acknowledges the central role of students, as demonstrated here, much of the literature reflects an academic, teacher-centric or institutional view. This view reflects some of the key ethical concerns related to informed consent and the role of power translating to a somewhat paternalistic approach to students. We suggest that as students are the primary stakeholders – they should be consulted in the development and application of LA. An ethical approach to LA requires that we engage with our students in their learning and the systems and information that support that process rather than assuming we know we know what students want, what their concerns are or how they would like data presented.",
    url = "https://doi.org/10.14742/ajet.4653"
}

@article{ref259_5d0b41,
    author = "Vázquez‐Ingelmo, Andrea and García‐Peñalvo, Francisco and Therón, Roberto",
    title = "Capturing high-level requirements of information dashboards' components through meta-modeling",
    year = "2019",
    doi = "10.1145/3362789.3362837",
    abstract = "Information dashboards are increasing their sophistication to match new necessities and adapt to the high quantities of generated data nowadays. These tools support visual analysis, knowledge generation, and thus, are crucial systems to assist decision-making processes. However, the design and development processes are complex, because several perspectives and components can be involved. Tailoring capabilities are focused on providing individualized dashboards without affecting the time-to-market through the decrease of the development processes' time. Among the methods used to configure these tools, the software product lines paradigm and model-driven development can be found. These paradigms benefit from the study of the target domain and the abstraction of features, obtaining high-level models that can be instantiated into concrete models. This paper presents a dashboard meta-model that aims to be applicable to any dashboard. Through domain engineering, different features of these tools are identified and arranged into abstract structures and relationships to gain a better understanding of the domain. The goal of the meta-model is to obtain a framework for instantiating any dashboard to adapt them to different contexts and user profiles. One of the contexts in which dashboards are gaining relevance is Learning Analytics, as learning dashboards are powerful tools for assisting teachers and students in their learning activities. To illustrate the instantiation process of the presented meta-model, a small example within this relevant context (Learning Analytics) is also provided.",
    url = "https://doi.org/10.1145/3362789.3362837"
}

@article{ref260_f66584,
    author = "Florián-Gaviria, Beatriz and Glahn, Christian and Gesa, Ramón",
    title = "A Software Suite for Efficient Use of the European Qualifications Framework in Online and Blended Courses",
    year = "2013",
    issn = "1939-1382",
    doi = "10.1109/tlt.2013.18",
    abstract = "Since introduction of the European qualifications framework (EQF) as one instrument to bridge from learning institutions to competence driven lifelong learning, it remains a challenge for instructors and teachers in higher education to make efficient use of this framework for designing, monitoring, and managing their lessons. This paper presents a software suite for enabling teachers to make better use of EQF in their teaching. The software suite extends course design based on well-defined learning outcomes, monitoring performance and competence acquisition according to the EQF levels, assessment using scoring rubrics of EQF levels and competences in a 360-degree feedback, as well as visualizations of learning analytics and open student models in dashboards for different social perspectives in social planes. This paper includes a case study with 20 teachers who used the software suite in all phases of the course lifecycle for three programming courses. The results show that integrated applications for adopting the EQF in teaching practice are strongly needed. These results also show that the suite can assist teachers in creating contextual awareness, kindling reflection, understanding students and course progress, and inferring patterns of success and failure in competences development.",
    url = "https://doi.org/10.1109/tlt.2013.18"
}

@article{ref261_57ead5,
    author = "Adinolfi, Paola and D’Avanzo, Ernesto and Lytras, Miltiadis and Novo‐Corti, Isabel and Picatoste, Xosé",
    title = "Sentiment Analysis to Evaluate Teaching Performance",
    year = "2016",
    issn = "1947-8429",
    doi = "10.4018/ijksr.2016100108",
    url = "https://doi.org/10.4018/ijksr.2016100108"
}

@article{ref262_ab7287,
    author = "Ong, Shuoh-Chwen and Chua, Fang-Fang",
    title = "ChemistLab: An Educational Game with Learning Analytics Dashboard",
    year = "2022",
    doi = "10.1109/icalt55010.2022.00038",
    url = "https://doi.org/10.1109/icalt55010.2022.00038"
}

@article{ref263_1da2ee,
    author = "Park, Yeonjeong and Jo, Il‐Hyun",
    title = "Learning Analytics Dashboard Evaluation Measure",
    year = "2019",
    doi = "10.1037/t80697-000",
    url = "https://doi.org/10.1037/t80697-000"
}

@article{ref264_6be16b,
    author = "Broos, Tom and Verbert, Katrien and Langie, Greet and Van Soom, Carolien and De Laet, Tinne",
    title = "Small data as a conversation starter for learning analytics",
    year = "2017",
    issn = "2397-7604",
    doi = "10.1108/jrit-05-2017-0010",
    abstract = "Purpose The purpose of this paper is to draw attention to the potential of “small data” to complement research in learning analytics (LA) and to share some of the insights learned from this approach. Design/methodology/approach This study demonstrates an approach inspired by design science research, making a dashboard available to n =1,905 students in 11 study programs (used by n =887) to learn how it is being used and to gather student feedback. Findings Students react positively to the LA dashboard, but usage and feedback differ depending on study success. Research limitations/implications More research is needed to explore the expectations of a high-performing student with regards to LA dashboards. Originality/value This publication demonstrates how a small data approach to LA contributes to building a better understanding.",
    url = "https://doi.org/10.1108/jrit-05-2017-0010"
}

@article{ref265_eb82fc,
    author = "Wiedbusch, Megan and Kite, Vance and Yang, Xi and Park, Soonhye and Chi, Min and Taub, Michelle and Azevedo, Roger",
    title = "A Theoretical and Evidence-Based Conceptual Design of MetaDash: An Intelligent Teacher Dashboard to Support Teachers' Decision Making and Students’ Self-Regulated Learning",
    year = "2021",
    issn = "2504-284X",
    doi = "10.3389/feduc.2021.570229",
    abstract = "Teachers’ ability to self-regulate their own learning is closely related to their competency to enhance self-regulated learning (SRL) in their students. Accordingly, there is emerging research for the design of teacher dashboards that empower instructors by providing access to quantifiable evidence of student performance and SRL processes. Typically, they capture evidence of student learning and performance to be visualized through activity traces (e.g., bar charts showing correct and incorrect response rates, etc.) and SRL data (e.g., eye-tracking on content, log files capturing feature selection, etc.) in order to provide teachers with monitoring and instructional tools. Critics of the current research on dashboards used in conjunction with advanced learning technologies (ALTs) such as simulations, intelligent tutoring systems, and serious games, argue that the state of the field is immature and has 1) focused only on exploratory or proof-of-concept projects, 2) investigated data visualizations of performance metrics or simplistic learning behaviors, and 3) neglected most theoretical aspects of SRL including teachers’ general lack of understanding their’s students’ SRL. Additionally, the work is mostly anecdotal, lacks methodological rigor, and does not collect critical process data (e.g. frequency, duration, timing, or fluctuations of cognitive, affective, metacognitive, and motivational (CAMM) SRL processes) during learning with ALTs used in the classroom. No known research in the areas of learning analytics, teacher dashboards, or teachers’ perceptions of students’ SRL and CAMM engagement has systematically and simultaneously examined the deployment, temporal unfolding, regulation, and impact of all these key processes during complex learning. In this manuscript, we 1) review the current state of ALTs designed using SRL theoretical frameworks and the current state of teacher dashboard design and research, 2) report the important design features and elements within intelligent dashboards that provide teachers with real-time data visualizations of their students’ SRL processes and engagement while using ALTs in classrooms, as revealed from the analysis of surveys and focus groups with teachers, and 3) propose a conceptual system design for integrating reinforcement learning into a teacher dashboard to help guide the utilization of multimodal data collected on students’ and teachers’ CAMM SRL processes during complex learning.",
    url = "https://doi.org/10.3389/feduc.2021.570229"
}

@article{ref266_6925d0,
    author = "Pozdniakov, Stanislav and Martínez‐Maldonado, Roberto and Tsai, Yi‐Shan and Echeverría, Vanessa and Srivastava, Namrata and Gašević, Dragan",
    title = "How Do Teachers Use Dashboards Enhanced with Data Storytelling Elements According to their Data Visualisation Literacy Skills?",
    year = "2023",
    doi = "10.1145/3576050.3576063",
    url = "https://doi.org/10.1145/3576050.3576063"
}

@article{ref267_f1dc04,
    author = "Riedel, Jana and Ruhland, Claudia and Schufmann, Sandra and Zawidzki, Julia",
    title = "LEARNING ANALYTICS DASHBOARDS - EXPECTATIONS OF TEACHERS AND LEARNERS",
    year = "2021",
    issn = "2340-1117",
    doi = "10.21125/edulearn.2021.1508",
    url = "https://doi.org/10.21125/edulearn.2021.1508"
}

@article{ref268_b32479,
    author = "Gruzd, Anatoliy and Conroy, Nadia",
    title = "Designing a learning analytics dashboard for Twitter-facilitated teaching",
    year = "2021",
    doi = "10.32920/14637885",
    url = "https://doi.org/10.32920/14637885"
}

@article{ref269_dcef9f,
    author = "Jivet, Ioana and Wong, Jacqueline and Scheffel, Maren and Torre, Manuel and Specht, Marcus and Drachsler, Hendrik",
    title = "Quantum of Choice: How learners’ feedback monitoring decisions, goals and self-regulated learning skills are related",
    year = "2021",
    doi = "10.1145/3448139.3448179",
    abstract = "Learning analytics dashboards (LADs) are designed as feedback tools for learners, but until recently, learners rarely have had a say in how LADs are designed and what information they receive through LADs. To overcome this shortcoming, we have developed a customisable LAD for Coursera MOOCs on which learners can set goals and choose indicators to monitor. Following a mixed-methods approach, we analyse 401 learners' indicator selection behaviour in order to understand the decisions they make on the LAD and whether learner goals and self-regulated learning skills influence these decisions. We found that learners overwhelmingly chose indicators about completed activities. Goals are not associated with indicator selection behaviour, while help-seeking skills predict learners' choice of monitoring their engagement in discussions and time management skills predict learners' interest in procrastination indicators. The findings have implications for our understanding of learners' use of LADs and their design.",
    url = "https://doi.org/10.1145/3448139.3448179"
}

@article{ref270_bf191e,
    author = "Redjem, Inas and Treuillier, Célina and Boyer, Anne",
    title = "Designing transparent learning analytics dashboards",
    year = "2023"
}

@article{ref271_234f9c,
    author = "Echeverría, Vanessa and Fernandez‐Nieto, Gloria and Zhao, Linxuan and Palominos, Evelyn and Srivastava, Namrata and Gašević, Dragan and Pammer‐Schindler, Viktoria and Martínez‐Maldonado, Roberto",
    title = "A learning analytics dashboard to support students' reflection on collaboration",
    year = "2024",
    issn = "0266-4909",
    doi = "10.1111/jcal.13088",
    abstract = "Abstract Background Dashboards play a prominent role in learning analytics (LA) research. In collaboration activities, dashboards can show traces of team participation. They are often evaluated based on students' perceived satisfaction and engagement with the dashboard. However, there is a notable methodological gap in understanding how these dashboards support the nuanced process of student reflection. Objective This paper presents empirical evidence on how students from high and low‐performing groups reflect individually on their performance while using a Learning Analytics Dashboard (LAD). Methods We address this in the context of education in healthcare, wherein we captured actions and positioning data from a simulation‐based collaborative activity and generated a collaborative LAD. A total of 41 nursing students were invited to participate in a post‐hoc semi‐structured individual interview to use a collaborative LAD while answering a set of prompts to reflect on their individual and group performance. Students' reflections were coded and analysed using Bain's 5R reflection framework. We used epistemic network analysis to capture the dynamic reflection process and to understand the connections between the reflection stages (from low to high). We compared how different these connections were for students in high and low‐performing groups. Results and Conclusions Our results revealed that most students were only able to achieve low and middle stages of reflection. Yet, students in low‐performing groups predominantly followed low‐to‐middle stages of reflection. In contrast, students from high‐performing groups demonstrated the ability to transition between low‐to‐middle and low‐to‐high stages of reflection. Based on these findings, we discuss implications for both research and practice, particularly emphasising the necessity to scaffold reflection when using LADs.",
    url = "https://doi.org/10.1111/jcal.13088"
}

@article{ref272_ae869a,
    author = "Odriozola, Santos and Luís, José",
    title = "Learning Analytics and Learning Dashboards: a Human-Computer Interaction Perspective",
    year = "2015"
}

@article{ref273_9a08a6,
    author = "Masako, Furukawa and Kojiro, Hata and Kazutsuna, Yamaji",
    title = "Development of the learning analytics platform and its dashboard",
    year = "2020"
}

@article{ref274_9ff219,
    author = "Israel‐Fishelson, Rotem and Kohen-Vacs, Dan",
    title = "Towards Optimization of Learning Analytics Dashboards That Are Customized for the Students' Requirements",
    year = "2023",
    issn = "1939-1382",
    doi = "10.1109/tlt.2023.3332500",
    url = "https://doi.org/10.1109/tlt.2023.3332500"
}

@article{ref275_83c027,
    author = "Alfredo, Riordan and Echeverría, Vanessa and Zhao, Linxuan and Lawrence, LuEttaMae and Fan, Jie and Yan, Lixiang and Li, Xinyu and Swiecki, Zachari and Gašević, Dragan and Martínez‐Maldonado, Roberto",
    title = "Designing a Human-centred Learning Analytics Dashboard In-use",
    year = "2024",
    issn = "1929-7750",
    doi = "10.18608/jla.2024.8487",
    abstract = "Despite growing interest in applying human-centred design methods to create learning analytics (LA) systems, most efforts have concentrated on initial design phases, with limited exploration of how LA tools and practices can co-evolve during the actual learning and teaching activities. This paper examines how a human-centred LA dashboard can be further refined and adapted by teachers while actively using it in a real-world scenario (i.e., design-in-use), beyond its intended design (i.e., design-for-use). We use instrumental genesis as a theoretical lens to analyze the temporary and permanent instrumentalization of design features and individual and collective instrumentation of the LA dashboard. The analysis of semi-structured individual interviews with five nursing teachers who used an LA dashboard to guide team reflections with 224 students (56 teams) revealed technical and pedagogical changes that occurred in both the system’s features (instrumentalization) and teaching practices (instrumentation). We found that teachers adopted the LA dashboard beyond initially intended ways by (i) providing emotional support with the analytics, (ii) reducing details in AI-powered data visualizations for easier comprehension, (iii) creating data narratives to address data limitations, and (iv) collectively developing new practices to use the LA dashboard for co-teaching. Therefore, teachers’ design-in-use of the LA dashboard highlights the ongoing need for design improvements to address challenges posed by dynamic data and complex algorithms underlying AI and analytics interfaces.",
    url = "https://doi.org/10.18608/jla.2024.8487"
}

@article{ref276_346e7d,
    author = "Kim, Mi and Lee, Jae",
    title = "A Delphi Study on K University’s Learning Analytics Dashboard Design",
    year = "2022",
    issn = "1598-2106",
    doi = "10.22251/jlcci.2022.22.20.781",
    url = "https://doi.org/10.22251/jlcci.2022.22.20.781"
}

@article{ref277_248bc4,
    author = "Centenaro, Belisa and Cechinel, Cristian and Ramos, Vinícius and Primo, Tiago and Muñoz, Roberto",
    title = "Systematic Mapping of Moodle Dashboards Focused on Learning Analytics Tasks",
    year = "2021",
    doi = "10.1109/laclo54177.2021.00013",
    url = "https://doi.org/10.1109/laclo54177.2021.00013"
}

@article{ref278_4adcff,
    author = "Hardebolle, Cécile and Jermann, Patrick and Pinto, Francisco and Tormey, Roland",
    title = "Impact of a learning analytics dashboard on the practice of students and teachers",
    year = "2019"
}

@article{ref279_dc7e15,
    author = "Hardebolle, Cécile and Tormey, Roland and Jermann, Patrick and Pinto, Francisco and Di Vincenzo, Maria",
    title = "Impact on the practice of students and teachers of a learning analytics dashboard",
    year = "2019"
}

@article{ref280_b47065,
    author = "Dabbebi, Ines",
    title = "Design and dynamic generation of contextual Learning Analytics dashboards",
    year = "2019"
}

@article{ref281_50590d,
    author = "Shabaninejad, Shiva",
    title = "Recommending insightful actions in learning analytics dashboards",
    year = "2024",
    doi = "10.14264/e168aa5",
    url = "https://doi.org/10.14264/e168aa5"
}

@article{ref282_b56cca,
    author = "Ahmad, Atezaz and Yordanov, Ivaylo and Yau, Jane and Schneider, Ján and Drachsler, Hendrik",
    title = "A Trusted Learning Analytics Dashboard for Displaying OER",
    year = "2023",
    doi = "10.1007/978-3-658-38703-7\_15",
    url = "https://doi.org/10.1007/978-3-658-38703-7\_15"
}

@article{ref283_b64d5f,
    author = "Barbé, Rémi and Encelle, Benoît and Sehaba, Karim",
    title = "Adaptation in Learning Analytics Dashboards: A Systematic Review",
    year = "2024",
    doi = "10.5220/0012628600003693",
    url = "https://doi.org/10.5220/0012628600003693"
}

@article{ref284_6320e9,
    author = "Demartini, Claudio and Sciascia, Luciano and Bosso, Andrea and Manuri, Federico",
    title = "Artificial Intelligence Bringing Improvements to Adaptive Learning in Education: A Case Study",
    year = "2024",
    issn = "2071-1050",
    doi = "10.3390/su16031347",
    abstract = "Despite promising outcomes in higher education, the widespread adoption of learning analytics remains elusive in various educational settings, with primary and secondary schools displaying considerable reluctance to embrace these tools. This hesitancy poses a significant obstacle, particularly given the prevalence of educational technology and the abundance of data generated in these environments. In contrast to higher education institutions that readily integrate learning analytics tools into their educational governance, high schools often harbor skepticism regarding the tools’ impact and returns. To overcome these challenges, this work aims to harness learning analytics to address critical areas, such as school dropout rates, the need to foster student collaboration, improving argumentation and writing skills, and the need to enhance computational thinking across all age groups. The goal is to empower teachers and decision makers with learning analytics tools that will equip them to identify learners in vulnerable or exceptional situations, enabling educational authorities to take suitable actions that are aligned with students’ needs; this could potentially involve adapting learning processes and organizational structures to meet the needs of students. This work also seeks to evaluate the impact of such analytics tools on education within a multi-dimensional and scalable domain, ranging from individual learners to teachers and principals, and extending to broader governing bodies. The primary objective is articulated through the development of a user-friendly AI-based dashboard for learning. This prototype aims to provide robust support for teachers and principals who are dedicated to enhancing the education they provide within the intricate and multifaceted social domain of the school.",
    url = "https://doi.org/10.3390/su16031347"
}

@article{ref285_eb838c,
    author = "Aguilar, Stephen",
    title = "Using Motivation Theory to Design Equity-Focused Learning Analytics Dashboards",
    year = "2023",
    issn = "2813-4346",
    doi = "10.3390/higheredu2020015",
    abstract = "Learning Analytics applications, and their associated dashboards, are frequently used in post-secondary settings; yet, there has been limited work exploring the motivational implications of their deployment, especially for under-served student populations that are more susceptible to (perceived) negative messages about their academic performance. In this paper, I argue that Situated Expectancy-Value Theory (EVT) is well-positioned to serve as a useful lens when developing and evaluating learning analytics dashboard designs and their future development. Used in this way, SEVT can help the learning analytics community to ensure that student experiences with learning analytics are adaptively motivating, both in general and for underserved student populations more specifically.",
    url = "https://doi.org/10.3390/higheredu2020015"
}

@article{ref286_022a8e,
    author = "Lahbi, Zakaria and Sabbane, Mohamed",
    title = "Learning Analytics Dashboard Model to Supervise a Distance Learning",
    year = "2020",
    issn = "2277-3878",
    doi = "10.35940/ijrte.e6675.018520",
    abstract = "Providing performance measures of training devices becomes a necessity for teachers. Without having them, teachers will have no clue in measuring the goals achieved. However, the analysis techniques available today allow us to go further and to consider new applications to serve the quality and effectiveness of training devices. This article presents the design and the implementation of a system for the supervision of an online course and the different roles involved in a computer environment for human learning. As a result, a description of the needs in terms of dashboards during data observation phases generated during a learning situation will be provided. Our objective is to define all the stakeholders that a dashboard can contain, the different user profiles, their activities, their viewing preferences and their objectives. Our main goal is the design and implementation of a dashboard model that meets all the requirements of different stakeholders of consultation or adaptation rating and customization. This model will give results in the form of different visualization style that will discuss in this article",
    url = "https://doi.org/10.35940/ijrte.e6675.018520"
}

@article{ref287_a4c421,
    author = "Chen, Yuxin and Saleh, Asmalina and Hmelo‐Silver, Cindy and Glazewski, Krista and Mott, Bradford and Lester, James",
    title = "Supporting Collaboration: From Learning Analytics to Teacher Dashboards",
    year = "2020",
    doi = "10.22318/icls2020.1689",
    url = "https://doi.org/10.22318/icls2020.1689"
}

@article{ref288_5f1c54,
    author = "Schumacher, Clara and Klasen, Daniel and Ifenthaler, Dirk",
    title = "Evidenzbasierte Implementation eines Learning Analytics Dashboards in ein bestehendes Lernmanagementsystem (Evidence-based Implementation of a Learning Analytics Dashboard into an Existing Learning Management System).",
    year = "2018"
}

@article{ref289_24031f,
    author = "Molenaar, Inge and van Campen, Carolien",
    title = "Learning analytics in practice",
    year = "2016",
    doi = "10.1145/2883851.2883892",
    abstract = "Even though the recent influx of tablets in primary education goes together with the vision that educational technology empowered with learning analytics will revolutionize education, empirical results supporting this claim are scares. Adaptive educational technology Snappet combines extracted and embedded learning analytics daily in classrooms. While students make exercises on the tablet this technology displays real-time data of learner performance in a teacher dashboard (extracted analytics). At the same time, learner performance is used to adaptively adjust exercises to students' progress (embedded analytics). This quasiexperimental study compares the development of students' arithmetic skills over one schoolyear (grade 2 and 4) in a traditional paper based setting to learning with the adaptive educational technology Snappet. The results indicate that students in the Snappet condition make significantly more progress on arithmetic skills in grade 4. Moreover, in this grade students with a high ability level, benefit the most from working with this adaptive educational technology. Overall the development pattern of students with different abilities was more divergent in the AET condition compared to the control condition. These results indicate that adaptive educational technologies combining extracted and embedded learning analytics are indeed creating new education scenarios that contribute to personalized learning in primary education.",
    url = "https://doi.org/10.1145/2883851.2883892"
}

@article{ref290_d6d60a,
    author = "Duval, Erik and Klerkx, Joris and Verbert, Katrien and Nagel, Till and Govaerts, Sten and Chico, Gonzalo and Odriozola, Jose and Vandeputte, Bram",
    title = "Learning Dashboards \& Learnscapes",
    year = "2012"
}

@article{ref291_8f0f88,
    author = "De Bruyne, Ellen and Herrewijn, Laura and Hoefkens, Amber and Heirman, Wannes and Vanthournout, Gert and Depessemier, Pieter",
    title = "door onderzoek geïnformeerde ontwikkeling van een Learning Analytics dashboard.",
    year = "2023",
    issn = "0168-1095",
    doi = "10.59532/tvho.v41i1.13948",
    abstract = "Learning Analytics (LA) zijn begaan met het gebruik van data om het leerproces van studenten te begrijpen, hun leeromgeving te optimaliseren of door data-geïnformeerde innovatie mogelijk te maken. Kwalitatieve dashboards zijn cruciaal om deze doelen te bereiken. Het praktijkgericht wetenschappelijk onderzoek LAP! tracht op een door onderzoek geïnformeerde manier dashboards voor verschillende gebruikersgroepen te ontwikkelen en gebruikt daarbij educational design research als raamwerk. Deze bijdrage illustreert de analyse- en exploratiefase in dit project. Hierin werd onderzoek opgezet naar de inhoud, structuur, lay-out en relevantie van deze dashboards aan de hand van drie acties: een literatuurverkenning; een gebruikersonderzoek bij studenten, lesgevers en opleidingshoofden; en een databankanalyse die met behulp van machine learning naging welke beschikbare variabelen een voorspellende kracht hebben op studiesucces. Verder werd ook de juridische en wettelijke basis van de verwerking van persoonsgegevens in de dashboards in kaart gebracht en geëvalueerd via een Data Protection Impact Assessment (DPIA). De resultaten van de drie onderzoeksacties inzake de selectie van variabelen zijn niet steeds eenduidig, maar bieden wel voldoende houvast voor de ontwikkeling van een prototype voor een generiek dashboard. Via cycli van ontwikkeling, implementatie en testen zal dit prototype worden verfijnd en worden aangepast aan de noden van verschillende gebruikersgroepen.",
    url = "https://doi.org/10.59532/tvho.v41i1.13948"
}

@article{ref292_85f451,
    author = "Campos, Fabio and Ahn, June and DiGiacomo, Daniela and Nguyen, Ha and Hays, Maria",
    title = "Making Sense of Sensemaking: Understanding How K–12 Teachers and Coaches React to Visual Analytics",
    year = "2021",
    issn = "1929-7750",
    doi = "10.18608/jla.2021.7113",
    abstract = "With the spread of learning analytics (LA) dashboards in K--12 schools, educators are increasingly expected to make sense of data to inform instruction. However, numerous features of school settings, such as specialized vantage points of educators, may lead to different ways of looking at data. This observation motivates the need to carefully observe and account for the ways data sensemaking occurs, and how it may differ across K--12 professional roles. Our mixed-methods study reports on interviews and think-aloud sessions with middle-school mathematics teachers and instructional coaches from four districts in the United States. By exposing educators to an LA dashboard, we map their varied reactions to visual data and reveal prevalent sensemaking patterns. We find that emotional, analytical, and intentional responses inform educators’ sensemaking and that different roles at the school afford unique vantage points toward data. Based on these findings, we offer a typology for representing sensemaking in a K--12 school context and reflect on how to expand visual LA process models.",
    url = "https://doi.org/10.18608/jla.2021.7113"
}

@article{ref293_e1d559,
    author = "Costas-Jauregui, Vladimir and Oyelere, Solomon and Caussin-Torrez, Bernardo and Barros-Gavilanes, Gabriel and Agbo, Friday and Toivonen, Tapani and Motz, Regina and Tenesaca, Juan",
    title = "Descriptive Analytics Dashboard for an Inclusive Learning Environment",
    year = "2021",
    doi = "10.1109/fie49875.2021.9637388",
    url = "https://doi.org/10.1109/fie49875.2021.9637388"
}

@article{ref294_a473f9,
    author = "Groher, Iris and Vierhauser, Michael and Hartl, Erik",
    title = "A Learning Analytics Dashboard for Improved Learning Outcomes and Diversity in Programming Classes",
    year = "2024",
    doi = "10.5220/0012735000003693",
    abstract = "The increased emphasis on competency management and learning objectives in higher education has led to a rise in Learning Analytics (LA) applications.These tools play a vital role in measuring and optimizing learning outcomes by analyzing and interpreting student-related data.LA tools furthermore provide course instructors with insights on how to refine teaching methods and material and address diversity in student performance to tailor instruction to individual needs.This tool demonstration paper introduces our Learning Analytics Dashboard, designed for an introductory Python programming course.With a focus on gender diversity, the dashboard analyzes graded Jupyter Notebooks, to provide insights into student performance across assignments and exams.An initial assessment of the dashboard, applying it to our Python programming course in the previous year, has provided us with interesting insights and information on how to further improve our class and teaching materials.We present the dashboard's design, features, and outcomes while outlining our plans for its future development and enhancement.",
    url = "https://doi.org/10.5220/0012735000003693"
}

@article{ref295_3fdefe,
    author = "Rodda, Alena",
    title = "Student-Centered Design and Evaluation of a Learning Analytics Dashboard",
    year = "2023",
    issn = "1865-1348",
    doi = "10.1007/978-3-031-42788-6\_5",
    url = "https://doi.org/10.1007/978-3-031-42788-6\_5"
}

@article{ref296_65615e,
    author = "Tsoni, Rozita and Garani, Georgia and Verykios, Vassilios",
    title = "Incorporating Data Warehouses into Data Pipelines for Deploying Learning Analytics Dashboards",
    year = "2023",
    doi = "10.1109/iisa59645.2023.10345957",
    url = "https://doi.org/10.1109/iisa59645.2023.10345957"
}

@article{ref297_f0ba06,
    author = "Tan, Jennifer and Yang, Simon and Koh, Elizabeth and Jonathan, Christin",
    title = "Fostering 21st century literacies through a collaborative critical reading and learning analytics environment",
    year = "2016",
    doi = "10.1145/2883851.2883965",
    url = "https://doi.org/10.1145/2883851.2883965"
}

@article{ref298_54eb83,
    author = "Sung-Hee, Jin and Yoo, Mina",
    title = "An Analytic Review of the studies on Learning Analytics based Dashboard in e-Learning Environments",
    year = "2015",
    issn = "1229-7291",
    doi = "10.15833/kafeiam.21.2.185",
    url = "https://doi.org/10.15833/kafeiam.21.2.185"
}

@article{ref299_383c44,
    author = "Pérez‐Berenguer, Daniel and Kessler, Mathieu and Molina, Jesús",
    title = "A Customizable and Incremental Processing Approach for Learning Analytics",
    year = "2020",
    issn = "2169-3536",
    doi = "10.1109/access.2020.2975384",
    abstract = "The ability of learning analytics to improve the learning/teaching processes is widely recognized. In this paper, the learning analytics architecture developed at the Digital Content Production Center of the Technical University of Cartagena (Spain) is presented. This architecture contributes to the field of learning analytics in two aspects: it allows for dashboard customization and improves the efficiency of the analysis of learners' interaction data. Events resulting from learners' interaction are captured and stored in Caliper standard format, to be further processed incrementally to allow dashboards to be shown without delay to teachers. Customization is considered a mandatory requirement for learning analytics tools, however, although some proposals have recently been made, a greater research effort in this topic is necessary. In the present work, this requirement is addressed by defining a domain-specific language (DSL) that allows teachers to customize dashboards. This language allows to express indicators (logical expressions) that classify students into different groups depending on their performance level. The paper also shows how our learning analytics approach was evaluated with a course that applies a flipped classroom method, and how it compares to the most relevant related works that have been published.",
    url = "https://doi.org/10.1109/access.2020.2975384"
}

@article{ref300_61c2f1,
    author = "Zheng, Juan",
    title = "Exploring Instructors' Emotions and Metacognition: A Case Study of Instructor Interactions With a Learning Analytics Dashboard",
    year = "2019",
    doi = "10.3102/1433991",
    abstract = "Learning analytics (LA) is providing new methodologies that are being applied to the design and application of dashboards to support teaching and learning.However, few studies attempt to understand instructors' emotions and metacognition while they are interacting with an LA dashboard (LAD).The current study investigates instructors' emotional states and metacognitive activities while they interact with an LAD designed to support online asynchronous collaboration of multiple groups.Instructors' think-aloud data was collected while they used the dashboard and coded for epistemic emotions and metacognitive activities.Differences in epistemic emotions and metacognitive activities were found between instructors who were more or less proficient at using the dashboard.Practical implications are discussed for the design of LA dashboards for teaching and learning.",
    url = "https://doi.org/10.3102/1433991"
}

@article{ref301_6f5579,
    author = "Nguyen, Viet and Nguyen, Quang and Nguyen, Vuong",
    title = "A Model to Forecast Learning Outcomes for Students in Blended Learning Courses Based On Learning Analytics",
    year = "2018",
    doi = "10.1145/3268808.3268827",
    url = "https://doi.org/10.1145/3268808.3268827"
}

@article{ref302_9f2f01,
    author = "de Leng, Bas and Pawelka, Friedrich",
    title = "The use of learning dashboards to support complex in-class pedagogical scenarios in medical training: how do they influence students’ cognitive engagement?",
    year = "2020",
    issn = "1793-2068",
    doi = "10.1186/s41039-020-00135-7",
    abstract = "Abstract This study aims to contribute to empirical and interdisciplinary knowledge on how visual learning analytics tools can support students’ cognitive engagement in complex in-class scenarios. Taking a holistic approach, instructional design, learning analytics, and students’ perceptions were examined together. The teaching of systematic viewing and image interpretation in radiology education was used to exemplify a complex in-class scenario, and a specific learning dashboard was designed as a support tool. The design was based on both educational and visualization theories and aligned with a pedagogical scenario integrating individual and class-wide activities. The quantity and quality of the cognitive engagement of a group of 26 students were explored. A mixed method approach was used, including computer log file analyses of individual work, analysis of video recordings of in-class small group discussions, and a focus group discussion with the students involved. The in-class scenario with the learning dashboard resulted in a good balance between individual tasks and group discussions and a high degree of active cognitive engagement. Constructive and interactive forms of cognitive engagement were, however, less evident. In addition, the products of these constructive (description of findings) and interactive (type of dialogue) cognitive engagements were of mediocre quality and therefore not optimal for knowledge transfer. The study also showed that the way the students and teacher understood their respective tasks and used the available interaction techniques of the learning dashboard highly influenced the students’ cognitive engagement. Finally, several ideas emerged that could help to overcome the deficits found in the training of participants and to improve the tasks set and the learning dashboard itself.",
    url = "https://doi.org/10.1186/s41039-020-00135-7"
}

@article{ref303_09c2ef,
    author = "Shin, Jongho and Choi, Jaewon and Park, Suyeong and Oh, Sanghee",
    title = "Development of the dashboard based on learning analytics for teaching support in higher education",
    year = "2018",
    issn = "1229-7291",
    doi = "10.15833/kafeiam.24.3.489",
    url = "https://doi.org/10.15833/kafeiam.24.3.489"
}

@article{ref304_fa5f12,
    author = "Garcia-Zúbia, Javier and Cuadros, Jordi and Serrano, Vanessa and Hernández‐Jayo, Unai and Angulo-Martinez, Ignacio and Villar-Martínez, Aitor and Orduña, Pablo and Alves, Gustavo",
    title = "Dashboard for the VISIR remote lab",
    year = "2019",
    doi = "10.1109/expat.2019.8876527",
    abstract = "The VISIR dashboard (VISIR-DB) is a learning analytics tool connected with the VISIR remote lab. In VISIR, every action performed by a student from the interface over the remote laboratory and back is logged and recorded. VISIR-DB helps visualizing, in a fast and deep way, the recorded logs from this communication. Using this tool, a teacher can analyze and understand better how the students are using the remote lab during their learning process on analog electronics. With this information, the VISIR platform can be improved and the use of remote labs can be better understood.",
    url = "https://doi.org/10.1109/expat.2019.8876527"
}

@article{ref305_96e615,
    author = "Aguilar, Stephen and Baek, Clare",
    title = "Motivated Information Seeking and Graph Comprehension Among College Students",
    year = "2019",
    doi = "10.1145/3303772.3303805",
    url = "https://doi.org/10.1145/3303772.3303805"
}

@article{ref306_9ab304,
    author = "Mohammed, Mohammed",
    title = "Review of Literature on the Use of Learning Analytics and Learning Analytical Dashboard (LAD) in Improving Student Performance in Higher Education Institutions in Kenya",
    year = "2024",
    issn = "2454-6194",
    doi = "10.51584/ijrias.2024.90214",
    url = "https://doi.org/10.51584/ijrias.2024.90214"
}

@article{ref307_1c0a6d,
    author = "Van Rijn, Lars and Karolyi, Heike and Hanses, Michael and de Witt, Claudia",
    title = "Feedback mit Learning Analytics – Interdisziplinäres Design eines Dashboards für Studierende",
    year = "2024",
    issn = "2219-6994",
    doi = "10.21240/zfhe/19-4/05",
    abstract = "Formatives Feedback wirkt sich positiv auf akademische Leistungen, Zufriedenheit und das selbstregulierte Lernen von Studierenden aus. Feedback mit hohem Informationsgehalt hat einen positiven Effekt auf Lernergebnisse und akademische Leistung. Dieser Beitrag beschreibt die Entwicklung eines Assistenzsystems zu formativem Feedback durch Learning Analytics im Fernstudium, das Studierende in ihrem Lernprozess bis zur Klausur begleitet. Es wird gezeigt, wie Online-Aktivitäten Studierender als Repräsentation studentischen Engagements für hochinformatives Feedback genutzt werden. Aus Clickstreams des Lernmanagementsystems Moodle werden dafür Indikatoren abgeleitet, die sich auf Daten zur Unterstützung metakognitiver Lernstrategien fokussieren. In Kollaboration mit Dozierenden wurde in einem interdisziplinären Ansatz ein Learning Analytics Dashboard entwickelt, das Studierende bei der Reflexion ihrer Lern- und Prüfungsvorbereitung unterstützt. Dieses sog. Feedbackzentrum umfasst Datenvisualisierungen kombiniert mit regelbasierten Feedbacktexten und Informationen zur Unterstützung des selbstregulierten Lernens. Abschließend werden die bisherigen Entwicklungs- und Forschungsarbeiten diskutiert und zukünftige Weiterentwicklungen skizziert.",
    url = "https://doi.org/10.21240/zfhe/19-4/05"
}

@article{ref308_ec3439,
    author = "Mokhtar, Salimah and Alshboul, Jawad and Shahin, Ghassan",
    title = "Towards Data-driven Education with Learning Analytics for Educator 4.0",
    year = "2019",
    issn = "1742-6588",
    doi = "10.1088/1742-6596/1339/1/012079",
    abstract = "Abstract Learning analytics has not been extensively used yet as necessary tools in the management and operation of public universities in Malaysia. Massive amount of data been created and collected on students at the faculty but mostly remain dark and unexplored. Generating many reports, having lots of alerts or dashboards does not make a faculty data-driven. To be smart, a faculty must utilize technology to enable and support better planning and decision-making, and to be data-driven, a faculty must have analytics to drive actions for value. This paper intends to explore the impact of IR 4.0 in the field of education and research into the possibility on how a university or a faculty can adapt to IR 4.0 and function in the big data environment. It will present the concept of Education 4.0, data-driven education and learning analytics. To transform, universities must rethink the current teaching practices and then redesign learning to suit future demand. This is discuss next. Finally, it summarizes the roles that educators 4.0 should play in Education 4.0.",
    url = "https://doi.org/10.1088/1742-6596/1339/1/012079"
}

@article{ref309_3bf4ae,
    author = "Taufik, Rahman and Meliana, Selly and Handono, Aryo",
    title = "Implementing of clustering in learning analytics dashboard to support teacher in evaluation",
    year = "2024",
    issn = "0094-243X",
    doi = "10.1063/5.0208235",
    url = "https://doi.org/10.1063/5.0208235"
}

@article{ref310_698603,
    author = "Gallagher, Timothy and Slof, Bert and van der Schaaf, Marieke and Toyoda, Ryo and Tehreem, Yusra and Fracaro, Sofia and Kester, Liesbeth",
    title = "Comparison with Self vs Comparison with Others: The Influence of Learning Analytics Dashboard Design on Learner Dashboard Use",
    year = "2022",
    issn = "0302-9743",
    doi = "10.1007/978-3-031-22124-8\_2",
    url = "https://doi.org/10.1007/978-3-031-22124-8\_2"
}

@article{ref311_94d36d,
    author = "Olalde, Iñigo and Larrañaga, Nagore",
    title = "Conceptual framework for process-oriented feedback through Learning Analytics Dashboards",
    year = "2020"
}

@article{ref312_56fbf1,
    author = "Peraić, Ivan and Grubišić, Ani and Pintarić, Neven",
    title = "Machine Learning in Learning Analytics Dashboards: A Systematic Literature Review",
    year = "2025",
    doi = "10.1109/sami63904.2025.10883302",
    url = "https://doi.org/10.1109/sami63904.2025.10883302"
}

@article{ref313_2117d9,
    author = "Choi, Heeryung and Borrella, Inma and Ponce-Cueto, Eva",
    title = "Meta-LAD: Developing a Learning Analytics Dashboard with a Theoretically Grounded and Context-Specific Approach",
    year = "2023",
    doi = "10.1109/lwmoocs58322.2023.10306139",
    url = "https://doi.org/10.1109/lwmoocs58322.2023.10306139"
}

@article{ref314_06192c,
    author = "Oliva-Córdova, Luis and Amado-Salvatierra, Héctor and Monterroso, L. and , Bojórquez-Roque and Villalba‐Condori, Klinge",
    title = "A learning analytics experience using interaction visualization dashboards to support virtual tutoring",
    year = "2019"
}

@article{ref315_8dc022,
    author = "Joseph‐Richard, Paul and Uhomoibhi, James and Jaffrey, Andrew",
    title = "Predictive learning analytics and the creation of emotionally adaptive learning environments in higher education institutions: a study of students' affect responses",
    year = "2021",
    issn = "2056-4880",
    doi = "10.1108/ijilt-05-2020-0077",
    abstract = "Purpose The aims of this study are to examine affective responses of university students when viewing their own predictive learning analytics (PLA) dashboards, and to analyse how those responses are perceived to affect their self-regulated learning behaviour. Design/methodology/approach A total of 42 Northern Irish students were shown their own predicted status of academic achievement on a dashboard. A list of emotions along with definitions was provided and the respondents were instructed to verbalise them during the experience. Post-hoc walk-through conversations with participants further clarified their responses. Content analysis methods were used to categorise response patterns. Findings There is a significant variation in ways students respond to the predictions: they were curious and motivated, comforted and sceptical, confused and fearful and not interested and doubting the accuracy of predictions. The authors show that not all PLA-triggered affective states motivate students to act in desirable and productive ways. Research limitations/implications This small-scale exploratory study was conducted in one higher education institution with a relatively small sample of students in one discipline. In addition to the many different categories of students included in the study, specific efforts were made to include “at-risk” students. However, none responded. A larger sample from a multi-disciplinary background that includes those who are categorised as “at-risk” could further enhance the understanding. Practical implications The authors provide mixed evidence for students' openness to learn from predictive learning analytics scores. The implications of our study are not straightforward, except to proceed with caution, valuing benefits while ensuring that students' emotional well-being is protected through a mindful implementation of PLA systems. Social implications Understanding students' affect responses contributes to the quality of student support in higher education institutions. In the current era on online learning and increasing adaptation to living and learning online, the findings allow for the development of appropriate strategies for implementing affect-aware predictive learning analytics (PLA) systems. Originality/value The current study is unique in its research context, and in its examination of immediate affective states experienced by students who viewed their predicted scores, based on their own dynamic learning data, in their home institution. It brings out the complexities involved in implementing student-facing PLA dashboards in higher education institutions.",
    url = "https://doi.org/10.1108/ijilt-05-2020-0077"
}

@article{ref316_bfe7f3,
    author = "Azcona, David and Hsiao, I‐Han and Smeaton, Alan",
    title = "Personalizing Computer Science Education by Leveraging Multimodal Learning Analytics",
    year = "2018",
    doi = "10.1109/fie.2018.8658596",
    abstract = {This Research Full Paper implements a framework that harness sources of programming learning analytics on three computer programming courses a Higher Education Institution. The platform, called PredictCS, automatically detects lower-performing or "at-risk" students in programming courses and automatically and adaptively sends them feedback. This system has been progressively adopted at the classroom level to improve personalized learning. A visual analytics dashboard is developed and accessible to Faculty. This contains information about the models deployed and insights extracted from student's data. By leveraging historical student data we built predictive models using student characteristics, prior academic history, logged interactions between students and online resources, and students' progress in programming laboratory work. Predictions were generated every week during the semester's classes. In addition, during the second half of the semester, students who opted-in received pseudo real-time personalised feedback. Notifications were personalised based on students' predicted performance on the course and included a programming suggestion from a top-student in the class if any programs submitted had failed to meet the specified criteria. As a result, this helped students who corrected their programs to learn more and reduced the gap between lower and higher-performing students.},
    url = "https://doi.org/10.1109/fie.2018.8658596"
}

@article{ref317_009eac,
    author = "Olalde, Iñigo and Larrañaga, Nagore",
    title = "Diseño de un Learning analytics dashboard para ofrecer feedback orientado al proceso",
    year = "2020"
}

@article{ref318_bd13dd,
    author = "Pozdniakov, Stanislav and Martínez‐Maldonado, Roberto and Tsai, Yi‐Shan and Cukurova, Mutlu and Bartindale, Tom and Chen, Peter and Marshall, Harrison and Richardson, Dan and Gašević, Dragan",
    title = "The Question-driven Dashboard: How Can We Design Analytics Interfaces Aligned to Teachers’ Inquiry?",
    year = "2022",
    doi = "10.1145/3506860.3506885",
    abstract = "One of the ultimate goals of several learning analytics (LA) initiatives is to close the loop and support students' and teachers' reflective practices. Although there has been a proliferation of end-user interfaces (often in the form of dashboards), various limitations have already been identified in the literature such as key stakeholders not being involved in their design, little or no account for sense-making needs, and unclear effects on teaching and learning. There has been a recent call for human-centred design practices to create LA interfaces in close collaboration with educational stakeholders to consider the learning design, and their authentic needs and pedagogical intentions. This paper addresses the call by proposing a question-driven LA design approach to ensure that end-user LA interfaces explicitly address teachers' questions. We illustrate the approach in the context of synchronous online activities, orchestrated by pairs of teachers using audio-visual and text-based tools (namely Zoom and Google Docs). This study led to the design and deployment of an open-source monitoring tool to be used in real-time by teachers when students work collaboratively in breakout rooms, and across learning spaces.",
    url = "https://doi.org/10.1145/3506860.3506885"
}

@article{ref319_f2d6e8,
    author = "Ndukwe, Ifeanyi and Daniel, Ben and Butson, Russell",
    title = "Data Science Approach for Simulating Educational Data: Towards the Development of Teaching Outcome Model (TOM)",
    year = "2018",
    issn = "2504-2289",
    doi = "10.3390/bdcc2030024",
    abstract = "The increasing availability of educational data provides the educational researcher with numerous opportunities to use analytics to extract useful knowledge to enhance teaching and learning. While learning analytics focuses on the collection and analysis of data about students and their learning contexts, teaching analytics focuses on the analysis of the design of the teaching environment and the quality of learning activities provided to students. In this article, we propose a data science approach that incorporates the analysis and delivery of data-driven solution to explore the role of teaching analytics, without compromising issues of privacy, by creating pseudocode that simulates data to help develop test cases of teaching activities. The outcome of this approach is intended to inform the development of a teaching outcome model (TOM), that can be used to inspire and inspect quality of teaching. The simulated approach reported in the research was accomplished through Splunk. Splunk is a Big Data platform designed to collect and analyse high volumes of machine-generated data and render results on a dashboard in real-time. We present the results as a series of visual dashboards illustrating patterns, trends and results in teaching performance. Our research aims to contribute to the development of an educational data science approach to support the culture of data-informed decision making in higher education.",
    url = "https://doi.org/10.3390/bdcc2030024"
}

@article{ref320_fa5958,
    author = "Rohloff, Tobias and Sauer, Dominic and Meinel, Christoph",
    title = "Student Perception of a Learner Dashboard in MOOCs to Encourage Self-Regulated Learning",
    year = "2019",
    doi = "10.1109/tale48000.2019.9225939",
    url = "https://doi.org/10.1109/tale48000.2019.9225939"
}

@article{ref321_02f1a6,
    author = "Ritsos, Panagiotis and Roberts, Jonathan",
    title = "Towards more Visual Analytics in Learning Analytics",
    year = "2014",
    doi = "10.2312/eurova.20141147",
    url = "https://doi.org/10.2312/eurova.20141147"
}

@article{ref322_505d29,
    author = "Kasepalu, Reet and Chejara, Pankaj and Prieto, Luis and Ley, Tobias",
    title = "Do Teachers Find Dashboards Trustworthy, Actionable and Useful? A Vignette Study Using a Logs and Audio Dashboard",
    year = "2021",
    issn = "2211-1662",
    doi = "10.1007/s10758-021-09522-5",
    abstract = "Abstract Monitoring and guiding multiple groups of students in face-to-face collaborative work is a demanding task which could possibly be alleviated with the use of a technological assistant in the form of learning analytics. However, it is still unclear whether teachers would indeed trust, understand, and use such analytics in their classroom practice and how they would interact with such an assistant. The present research aimed to find out what the perception of in-service secondary school teachers is when provided with a dashboard based on audio and digital trace data when monitoring a collaborative learning activity. In a vignette study, we presented twenty-one in-service teachers with videos from an authentic collaborative activity, together with visualizations of simple collaboration analytics of those activities. The teachers perceived the dashboards as providers of useful information for their everyday work. In addition to assisting in monitoring collaboration, the involved teachers imagined using it for picking out students in need, getting information about the individual contribution of each collaborator, or even as a basis for assessment. Our results highlight the need for guiding dashboards as only providing new information to teachers did not compel them to intervene and additionally, a guiding dashboard could possibly help less experienced teachers with data-informed assessment.",
    url = "https://doi.org/10.1007/s10758-021-09522-5"
}

@article{ref323_1f7650,
    author = "Pechenizkiy, Mykola and Gašević, Dragan",
    title = "Introduction into Sparks of the Learning Analytics Future",
    year = "2015",
    issn = "1929-7750",
    doi = "10.18608/jla.2014.13.8",
    abstract = "This section offers a compilation of 16 extended abstracts summarizing research of the doctoral students who participated in the Second Learning Analytics Summer Institute (LASI 2014) held at Harvard University in July 2014. The abstracts highlight the motivation, main goals and expected contributions to the field from the ongoing learning analytics doctoral research around the globe. These works cover several major topics in learning analytics including novel methods for automated annotations, longitudinal analytic studies, networking analytics, multi-modal analytics, dashboards, and data-driven feedback and personalization. The assumed settings include the traditional classroom, online and mobile learning, blended learning, and massive open online course education models.",
    url = "https://doi.org/10.18608/jla.2014.13.8"
}

@article{ref324_215c5f,
    author = "Shao, Peixia",
    title = "Exploring the Effects of Learning Analytics Dashboards on Learning Outcomes: A Meta-Analysis",
    year = "2023",
    doi = "10.3102/2013289",
    url = "https://doi.org/10.3102/2013289"
}

@article{ref325_804869,
    author = "Gallagher, Timothy and Slof, Bert and van der Schaaf, Marieke and Toyoda, Ryo and Tehreem, Yusra and Fracaro, Sofia and Kester, Liesbeth",
    title = "Correction to: Comparison with Self vs Comparison with Others: The Influence of Learning Analytics Dashboard Design on Learner Dashboard Use",
    year = "2023",
    issn = "0302-9743",
    doi = "10.1007/978-3-031-22124-8\_37",
    url = "https://doi.org/10.1007/978-3-031-22124-8\_37"
}

@article{ref326_943fcc,
    author = "Gerard, Libby",
    title = "Science Teachers' Use of a Learning Analytics Dashboard to Design Responsive Instruction",
    year = "2022",
    doi = "10.3102/1894922",
    url = "https://doi.org/10.3102/1894922"
}

@article{ref327_46df0f,
    author = "Cheng, Nuo and Zhao, Wei and Xu, Xiaoqing and Liu, Hongxia and Tao, Jinhong",
    title = "The influence of learning analytics dashboard information design on cognitive load and performance",
    year = "2024",
    issn = "1360-2357",
    doi = "10.1007/s10639-024-12606-1",
    url = "https://doi.org/10.1007/s10639-024-12606-1"
}

@article{ref328_d5ce4a,
    author = "Dazo, Suzanne and Stepanek, Nicholas and Chauhan, Aarjav and Dorn, Brian",
    title = "Examining Instructor Use of Learning Analytics",
    year = "2017",
    doi = "10.1145/3027063.3053256",
    abstract = "This study takes an instructor-centric approach to Learning Analytic (LA) research by analyzing instructor use of the LA within an educational streaming video platform called TrACE. The goal of this study is to understand how instructors naturally interact with analytic dashboards through an empirical analysis. To accomplish this, data of 14 instructors from three institutions that used TrACE from Spring 2015 to Spring 2016 was collected. Data was analyzed to identify frequency of analytic visits, duration of analytic use, differences in analytic use, and differences in use between semesters. Instructors demonstrated preferences for some analytics over others, but the majority of teachers generate short sessions that may not allow for in-depth exploration in analytics. Finally, instructor activity is not always consistent between semesters. Focus groups were conducted to explore motivations behind these findings and future work includes developing LA that address discovered issues.",
    url = "https://doi.org/10.1145/3027063.3053256"
}

@article{ref329_6005ed,
    author = "Park, Eunsung",
    title = "Self-Regulated Learning and Adaptive Learning Analytics Dashboards: A Reflexive Thematic Analysis",
    year = "2020",
    doi = "10.3102/1586123",
    url = "https://doi.org/10.3102/1586123"
}

@article{ref330_c888eb,
    author = "Kim, Minsun and Kim, SeonGyeom and Lee, Suyoun and Yoon, Yoosang and Myung, Junho and Yoo, Haneul and Lim, Hyunseung and Han, Jieun and Kim, Yoonsu and Ahn, Soyeon and Kim, Juho and Oh, Alice and Hong, Hwajung and Lee, Tak",
    title = "LLM-Driven Learning Analytics Dashboard for Teachers in EFL Writing Education",
    year = "2024",
    doi = "10.48550/arxiv.2410.15025",
    abstract = "This paper presents the development of a dashboard designed specifically for teachers in English as a Foreign Language (EFL) writing education. Leveraging LLMs, the dashboard facilitates the analysis of student interactions with an essay writing system, which integrates ChatGPT for real-time feedback. The dashboard aids teachers in monitoring student behavior, identifying noneducational interaction with ChatGPT, and aligning instructional strategies with learning objectives. By combining insights from NLP and Human-Computer Interaction (HCI), this study demonstrates how a human-centered approach can enhance the effectiveness of teacher dashboards, particularly in ChatGPT-integrated learning.",
    url = "https://doi.org/10.48550/arxiv.2410.15025"
}

@article{ref331_700257,
    author = "Krabbel, Helene and Munim, Ziaul and Bustgaard, Morten and Lindroos, Emilia",
    title = "Factors Affecting the System Usability of a Maritime Learning Analytics Dashboard prototype",
    year = "2025",
    issn = "2367-3389",
    doi = "10.1007/978-3-031-84170-5\_2",
    url = "https://doi.org/10.1007/978-3-031-84170-5\_2"
}

@article{ref332_500094,
    author = "Williams, Kasha",
    title = "Self-Regulated Learning: Utilizing Learning Analytics Dashboards to Prepare Students for the Workforce",
    year = "2021"
}

@article{ref333_fc0c0b,
    author = "Serrano, Vanessa and Cuadros, Jordi and Fernández‐Ruano, Laura and Garcia-Zúbia, Javier and Hernández‐Jayo, Unai and Lluch, Francesc",
    title = "Learning Analytics Dashboards for Assessing Remote Labs Users' Work: A Case Study with VISIR-DB",
    year = "2024",
    issn = "2211-1662",
    doi = "10.1007/s10758-024-09752-3",
    abstract = "Abstract In science and engineering education, remote laboratories are designed to bring ubiquity to experimental scenarios, by having real laboratories operated through the Internet. Despite that remote laboratories enable the collection of students' work data, the educational use of these data is still underdeveloped. Learning analytics dashboards are common tools to present and analyze educational data to provide indicators to understand learning processes. This paper presents how data from remote labs, such as Virtual Instruments Systems In Reality (VISIR), can be analyzed through a learning analytics dashboard to help instructors provide better feedback to their pupils. Visualizations to study the use of the VISIR, to assess students’ performance in a particular activity and to facilitate the assisted assessment of students are introduced to the VISIR dashboard (VISIR-DB). These visualizations include a new recodification of circuits that keeps the fragment being measured, in order to better identify student’s intention. VISIR-DB also incorporates functions to check a priori steps in the resolution process and/or potential errors (observation items), and logical combinations of them to grade students' performance according to the expected outcomes (assessment milestones). Both work indicators, observation items and assessment milestones, can be defined in activity-specific text files and allow for checking the circuit as coded by the interface, the conceptual circuit it represents, its components, parameters, and measurement result. Main results in the use of VISIR for learning DC circuits course show that students mainly use VISIR when indicated by instructors and a great variability regarding to time of use and number of experiments performed. For the particular assessment activity, VISIR-DB helps to easily detect that there is a significant number of students that did not achieved any of the expected tasks. Additionally, it helps to identify students that still make a huge number of errors at the end of the course. Appropriate interventions can be taken from here.",
    url = "https://doi.org/10.1007/s10758-024-09752-3"
}

@article{ref334_25aded,
    author = "Rienties, Bart and Cross, Simon and Marsh, Vicky and Ullmann, Thomas",
    title = "Making sense of learner and learning Big Data: reviewing five years of Data Wrangling at the Open University UK",
    year = "2017",
    issn = "0268-0513",
    doi = "10.1080/02680513.2017.1348291",
    url = "https://doi.org/10.1080/02680513.2017.1348291"
}

@article{ref335_3c1860,
    author = "Alcock, Sarah and Rienties, Bart and Aristeidou, Maria and Mostéfaoui, Soraya",
    title = "How do visualizations and automated personalized feedback engage professional learners in a Learning Analytics Dashboard?",
    year = "2024",
    doi = "10.1145/3636555.3636886",
    abstract = "Learning Analytics Dashboards (LAD) are the subject of research in a multitude of schools and higher education institutions, but a lack of research into learner-facing dashboards in professional learning has been identified. This study took place in an authentic professional learning context and aims to contribute insights into LAD design by using an academic approach in a practice-based environment. An existing storytelling LAD created to support 81 accountants was evaluated using Technology Acceptance Model, finding a learner expectation for clarity, conciseness, understanding and guidance on next steps. High usage levels and a 'take what you need' approach was identified, with all visualizations and automated personalized feedback being considered useful although to varying degrees. Professional learners in this study focus on understanding and acting upon weaknesses rather than celebrating strengths. The lessons for LAD design are to offer choice and create elements which support learners to take action to improve performance at a multitude of time points and levels of success.",
    url = "https://doi.org/10.1145/3636555.3636886"
}

@article{ref336_46c25b,
    author = "Vázquez‐Ingelmo, Andrea and García‐Peñalvo, Francisco and Therón, Roberto",
    title = "Towards a Technological Ecosystem to Provide Information Dashboards as a Service: A Dynamic Proposal for Supplying Dashboards Adapted to Specific Scenarios",
    year = "2021",
    issn = "2076-3417",
    doi = "10.3390/app11073249",
    abstract = "Data are crucial to improve decision-making and obtain greater benefits in any type of activity. However, the large amount of information generated by new technologies has made data analysis and knowledge generation a complex task. Numerous tools have emerged to facilitate this generation of knowledge, such as dashboards. Although dashboards are useful tools, their effectiveness can be affected by poor design or by not taking into account the context in which they are placed. Therefore, it is necessary to design and create custom dashboards according to the audience and data domain. This paper presents an application of the software product line paradigm and the integration of this approach into a web service to allow users to request source code for customized information dashboards. The main goal is to introduce the idea of creating a holistic ecosystem of different services to craft and integrate information visualizations in a variety of contexts. One of the contexts that can be especially favored by this approach is the educational context, where learning analytics, data analysis of student performance, and didactic tools are becoming very relevant. Three different use cases of this approach are presented to illustrate the benefits of the developed generative service.",
    url = "https://doi.org/10.3390/app11073249"
}

@article{ref337_6c130a,
    author = "Tytenko, Sergiy and Halushko, Alina and Demchyshyn, Andrii",
    title = "OVERVIEW OF LEARNING ANALYTICS DASHBOARDS AND ELEMENTS OF GAMIFICATION IN INTELLIGENT SYSTEMS FOR SELF-REGULATED LEARNING",
    year = "2019",
    issn = "2567-5273",
    doi = "10.30890/2567-5273.2022-21-01-011",
    abstract = "This article presents a systematic review of the research literature of the information panels of learning analytics, self-regulated learning and gamification, which report empirical results to assess the impact on learning and teaching. Also in this pape",
    url = "https://doi.org/10.30890/2567-5273.2022-21-01-011"
}

@article{ref338_c22b09,
    author = "Asatryan, Samvel and Safaryan, Naira",
    title = "DATA-DRIVEN EDUCATION IN UNIVERSITY PHYSICS: A COMPREHENSIVE ANALYSIS OF LEARNING ANALYTICS DASHBOARDS AND AI TUTORING",
    year = "2025",
    issn = "1829-1295",
    doi = "10.24234/miopap.v12i1.85",
    url = "https://doi.org/10.24234/miopap.v12i1.85"
}

@article{ref339_7e90b7,
    author = "Marie-Luce, Bourguet and Yingting, Hao",
    title = "Co-creation and Learner Personas Applied to the Inclusive Design of a Learning Analytics Dashboard",
    year = "2023",
    doi = "10.5281/zenodo.8202728",
    url = "https://doi.org/10.5281/zenodo.8202728"
}

@article{ref340_6f6e77,
    author = "Urrutia, Manuel and Cobos, Ruth and Dickens, Kate and White, Su and Davis, Hugh",
    title = "Visualising the MOOC experience: a dynamic MOOC dashboard built through institutional collaboration",
    year = "2016"
}

@article{ref341_0e1c7c,
    author = "Olney, Tom and Walker, Steve and Wood, Carlton and Clarke, Anactoria",
    title = "Are We Living In LA (P)LA Land? Reporting on the Practice of 30 STEM Tutors in their Use of a Learning Analytics Implementation at the Open University",
    year = "2021",
    issn = "1929-7750",
    doi = "10.18608/jla.2021.7261",
    abstract = "Most higher education institutions view their increasing use of learning analytics as having significant potential to improve student academic achievement, retention outcomes, and learning and teaching practice but the realization of this potential remains stubbornly elusive. While there is an abundance of published research on the creation of visualizations, dashboards, and predictive models, there has been little work done to explore the impact of learning analytics on the actual practice of teachers. Through the lens of social informatics (an approach that views the users of technologies as active social actors whose technological practices constitute a wider socio-technical system) this qualitative study reports on an investigation into the practice of 30 tutors in the STEM faculty at Europe’s largest distance learning organization, The Open University UK (OU). When asked to incorporate learning analytics (including predictive learning analytics) contained in the Early Alert Indicator (EAI) dashboard during the 2017–2018 academic year into their practice, we found that tutors interacted with this dashboard in certain unanticipated ways and developed three identifiable “shadow practices”.",
    url = "https://doi.org/10.18608/jla.2021.7261"
}

@article{ref342_fe544a,
    author = "Vozniuk, Andrii and Rodríguez‐Triana, María and Gillet, Denis",
    title = "CLEO - Workshop on Contextual Learning Analytics Enforcing data Ownership. Building Interactive Learning Dashboards with Logstash, Elasticsearch and Kibana (ELK)",
    year = "2016"
}

@article{ref343_485a2b,
    author = "De Laet, Tinne and Millecamp, Martijn and Ortiz‐Rojas, Margarita and Jiménez, Alberto and Verbert, Katrien",
    title = "Adoption and impact of a learning analytics dashboard supporting the advisor—Student dialogue in a higher education institute in Latin America",
    year = "2020",
    doi = "10.6084/m9.figshare.12362267",
    abstract = {The dataset contains the anonymized data set on which the accompanying paper "Adoption and impact of a learning analytics dashboard supporting the advisor—Student dialogue in a higher education institute in Latin America", published in the British Journal of Educational Technology in 2020, is based.},
    url = "https://doi.org/10.6084/m9.figshare.12362267"
}

@article{ref344_466096,
    author = "Kew, Si and Koh, Elizabeth and Choo, Zi and Jonathan, Christin",
    title = "A Systematic Review on Student-Facing Learning Analytics Dashboards: Reference Frames and Indicators",
    year = "2024",
    doi = "10.1109/cste62025.2024.00015",
    url = "https://doi.org/10.1109/cste62025.2024.00015"
}

@article{ref345_7ba6bb,
    author = "Wang, Chao and Ng, Jeremy and López, Nora and Hu, Xiao",
    title = "Preliminary Evaluation of Learning Analytics Dashboard for College Teachers’ Online Professional Learning",
    year = "2024",
    doi = "10.1109/icalt61570.2024.00030",
    url = "https://doi.org/10.1109/icalt61570.2024.00030"
}

@article{ref346_a8dcca,
    author = "Prinsloo, Paul",
    title = "A social cartography of analytics in education as performative politics",
    year = "2019",
    issn = "0007-1013",
    doi = "10.1111/bjet.12872",
    url = "https://doi.org/10.1111/bjet.12872"
}

@article{ref347_635b33,
    author = "Tlili, Ahmed and Hattab, Sarra and Essalmi, Fathi and Chen, Nian‐Shing and Huang, Ronghuai and Kinshuk, R. and Chang, Maiga and Burgos, Daniel",
    title = "A Smart Collaborative Educational Game with Learning Analytics to Support English Vocabulary Teaching",
    year = "2021",
    issn = "1989-1660",
    doi = "10.9781/ijimai.2021.03.002",
    abstract = "Learning Analytics (LA) approaches have proved to be able to enhance learning process and learning performance.However, little is known about applying these approaches for second language acquisition using educational games.Therefore, this study applied LA approaches to design a smart collaborative educational game, to enhance primary school children learning English vocabularies.Specifically, the game provided dashboards to the teachers about their students in a real-time manner.A pilot experiment was conducted in a public primary school where the students' data from experimental and control groups, namely learning and motivation test scores, interview and observation, were collected and analyzed.The obtained results showed that the experimental group (who used the smart game with LA) had significantly higher motivation and performance for learning English vocabularies than the control group (who used the smart game without LA).The findings of this study can help researchers and practitioners incorporate LA in their educational games to help students enhance language acquisition.",
    url = "https://doi.org/10.9781/ijimai.2021.03.002"
}

@article{ref348_391dd2,
    author = "Akçapınar, Gökhan and López‐Pernas, Sonsoles and Er, Erkan and Saqr, Mohammed",
    title = "How a Learning Analytics Dashboard Intervention Influences the Dynamics of Students’ Learning Behavior",
    year = "2024",
    issn = "2196-4971",
    doi = "10.1007/978-981-97-1814-6\_79",
    url = "https://doi.org/10.1007/978-981-97-1814-6\_79"
}

@article{ref349_41f735,
    author = "Wang, Zuo and Lin, Weiyue and Hu, Xiao",
    title = "Self-service Teacher-facing Learning Analytics Dashboard with Large Language Models",
    year = "2025",
    doi = "10.1145/3706468.3706491",
    url = "https://doi.org/10.1145/3706468.3706491"
}

@article{ref350_0e8582,
    author = "Gallagher, Timothy",
    title = "Designing Learning Analytics Dashboards for Digital Learning Environments: Investigating Learner Preferences, Usage, and Self-Efficacy",
    year = "2024",
    doi = "10.33540/2227",
    abstract = "This dissertation, a product of the European Union's CHARMING project, investigates the intersection of technology and learning, focusing on the design of learning analytics for lifelong learning. It emphasizes the importance of effective learning design and the innovative use of technology in digital learning environments. Chapter 1 presents the problem statement, highlighting the knowledge gap related to learning analytics design and the overarching research question: How does learning analytics dashboard (LAD) design influence learner preferences, interaction, and self-efficacy in training and education? Chapter 2 investigates workplace learner preferences for LADs designed for different phases of the self-regulated learning (SRL) cycle. The study reveals a preference for progress reference frames before and after task performance, while social reference frames are least preferred. Chapter 3 examines the impact of LADs with progress and social reference frames on occupational self-efficacy in virtual reality simulation-based training environments. The findings suggest that both reference frames could elicit equal change in self-efficacy, with social reference frames potentially inducing more significant change. Chapter 4 analyzes log-file data to understand chemical plant employees' engagement with LADs. The results indicate that progress reference frames might foster mastery goal orientation behaviors, while social reference frames seem to promote performance goal orientation behaviors. Chapter 5 investigates the impact of LAD reference frame type and direction of comparison on academic self-efficacy among university students. The findings highlight the influence of both comparison type and direction on changes in academic self-efficacy. Chapter 6 discusses the main research findings, theoretical and practical implications, limitations, and future research opportunities. The dissertation contributes to the understanding of LAD design and its influence on learning-related variables, providing valuable insights for educational stakeholders and researchers. This dissertation advances the understanding of learning analytics dashboard design and its impact on learner preferences, interaction, and self-efficacy in various educational contexts. The findings provide a foundation for future research and the development of more effective digital learning environments.",
    url = "https://doi.org/10.33540/2227"
}

@article{ref351_c95978,
    author = "Şahi̇n, Muhittin and Yurdugül, Halil",
    title = "An intervention engine design and development based on learning analytics: the intelligent intervention system (In2S)",
    year = "2019",
    issn = "2196-7091",
    doi = "10.1186/s40561-019-0100-7",
    abstract = "Abstract In this study, an intervention engine based on learning analytics was designed and developed. The intervention engine is named the Intelligent Intervention System (In 2 S). Within the scope of this research; In 2 S system and its components have been introduced, and the system is evaluated based on learners’ views. In 2 S includes three types of intervention that are instructional, supportive, and motivational intervention. The instructional intervention was structured based on assessment tasks. The supportive and motivational interventions were structured based on the learning experiences of the learners. Signal lights (red, yellow, and green) are presented to the learners for each assessment task as an instructional intervention. Supportive intervention is presented to the learners via the dashboard. In the context of motivational intervention, elements of gamification as a leader board, badges, and notifications have been used. In order to obtain the learner’s views about the In 2 S, semi-structured interviews were conducted with the learners who had a previous learning experience with the system. The learning environment was evaluated based on their views. Learners had a nine-week learning experience in the e-learning environment. Then, eight students who used the system most actively and eight students who used the system most passively were selected for focus group interviews.. According to the findings, it was seen that the learners who use the intervention engine indicated that the system is useful and want to use it in the context of other courses.",
    url = "https://doi.org/10.1186/s40561-019-0100-7"
}

@article{ref352_d6895a,
    author = "Fernandez‐Nieto, Gloria and Echeverría, Vanessa and Shum, Simon and Mangaroska, Katerina and Kitto, Kirsty and Palominos, Evelyn and Axisa, Carmen and Martínez‐Maldonado, Roberto",
    title = "Storytelling With Learner Data: Guiding Student Reflection on Multimodal Team Data",
    year = "2021",
    issn = "1939-1382",
    doi = "10.1109/tlt.2021.3131842",
    url = "https://doi.org/10.1109/tlt.2021.3131842"
}

@article{ref353_a6fb23,
    author = "Martínez, Víctor and Ramírez, Lorena and Montané-Jiménez, Luis",
    title = "Learning analytics dashboard para apoyar la enseñanza de la programación en educación a distancia",
    year = "2024",
    issn = "2387-0893",
    doi = "10.36825/riti.12.26.013",
    abstract = "When teaching programming in a blended or distance learning mode, the face to face interactions that help teachers identify students' aptitudes or problems in the subject decrease or, in some cases, may be non-existent. As a result, students' learning may be deficient. The objective of this study is to support teachers in identifying students who are struggling with the subject. Therefore, through a Learning Analytics process, mechanisms were defined and created to collect student performance metrics. These metrics are displayed on a dashboard using Data Visualization techniques to bridge this interaction gap. For the research, a quasi-experimental design was implemented, applying the interview technique to obtain qualitative data. The results show the effectiveness of the tool in identifying students who are having difficulties with the subject, as well as the topics that need to be reinforced generally.",
    url = "https://doi.org/10.36825/riti.12.26.013"
}

@article{ref354_27cb8f,
    author = "An, Pengcheng and Holstein, Kenneth and d’Anjou, Bernice and Eggen, Berry and Bakker, Saskia",
    title = "The TA Framework: Designing Real-time Teaching Augmentation for K-12 Classrooms",
    year = "2020",
    doi = "10.1145/3313831.3376277",
    abstract = "Recently, the HCI community has seen increased interest in the design of teaching augmentation (TA): tools that extend and complement teachers' pedagogical abilities during ongoing classroom activities. Examples of TA systems are emerging across multiple disciplines, taking various forms: e.g., ambient displays, wearables, or learning analytics dashboards. However, these diverse examples have not been analyzed together to derive more fundamental insights into the design of teaching augmentation. Addressing this opportunity, we broadly synthesize existing cases to propose the TA framework. Our framework specifies a rich design space in five dimensions, to support the design and analysis of teaching augmentation. We contextualize the framework using existing designs cases, to surface underlying design trade-offs: for example, balancing actionability of presented information with teachers' needs for professional autonomy, or balancing unobtrusiveness with informativeness in the design of TA systems. Applying the TA framework, we identify opportunities for future research and design.",
    url = "https://doi.org/10.1145/3313831.3376277"
}

@article{ref355_478b10,
    author = "Hu, Xiao and Hou, Xiang‐Yu and Lei, Chi‐Un and Yang, Chengrui and Ng, Jeremy",
    title = "An outcome-based dashboard for moodle and Open edX",
    year = "2017",
    doi = "10.1145/3027385.3029483",
    url = "https://doi.org/10.1145/3027385.3029483"
}

@article{ref356_1a196f,
    author = "de Vreugd, Lars and van Leeuwen, Anouschka and van der Schaaf, Marieke",
    title = "Students' Use of a Learning Analytics Dashboard and Influence of Reference Frames: Goal Setting, Motivation, and Performance",
    year = "2025",
    issn = "0266-4909",
    doi = "10.1111/jcal.70015",
    abstract = "ABSTRACT Background University students need to self‐regulate but are sometimes incapable of doing so. Learning Analytics Dashboards (LADs) can support students' appraisal of study behaviour, from which goals can be set and performed. However, it is unclear how goal‐setting and self‐motivation within self‐regulated learning elicits behaviour when using an LAD. Objectives This study's purpose is exploring reference frames’ influence on goal setting, LAD elements’ influence on student motivation, and the predictive value of goal setting and motivation on behaviour, adding to our understanding of the factors predicting task attainment and the role of reference frames. Methods In an experimental survey design, university students ( n = 88) used an LAD with a peer reference frame (Condition 1) or without one (Condition 2), set a goal, determined goal difficulty, self‐assessed motivation and LAD elements' influence on motivation. Researchers coded goal specificity. Four weeks later, students self‐assessed task attainment, task satisfaction, time on task, and task frequency. T ‐tests and MANOVA explored effects of the reference frame. Regression analyses determined predictive potential of goal difficulty, goal specificity, and motivation on goal attainment. Results and Conclusions Results showed no difference between conditions on goal specificity, difficulty, or motivation. The peer reference frame's perceived influence on motivation was small. LAD elements’ influence on motivation varied but were mainly positive. Regression models were not predictive, except the task satisfaction exploratory model. Most participants (77\%) attained their goals. Reference frame integration should be carefully considered, given potential negative effects. Students may require educators’ support when setting goals, but the support should balance students’ autonomy.",
    url = "https://doi.org/10.1111/jcal.70015"
}

@article{ref357_44a8a9,
    author = "Martínez‐Maldonado, Roberto",
    title = "Correction to: A handheld classroom dashboard: teachers’ perspectives on the use of real-time collaborative learning analytics",
    year = "2019",
    issn = "1556-1607",
    doi = "10.1007/s11412-019-09314-1",
    url = "https://doi.org/10.1007/s11412-019-09314-1"
}

@article{ref358_0e046f,
    author = "Broos, Tom and Verbert, Katrien and Langie, Greet and Van Soom, Carolien and De Laet, Tinne",
    title = "Multi-institutional positioning test feedback dashboard for aspiring students",
    year = "2018",
    doi = "10.1145/3170358.3170419",
    abstract = "Our work focuses on a multi-institutional implementation and evaluation of a Learning Analytics Dashboards (LAD) at scale, providing feedback to N=337 aspiring STEM (science, technology, engineering and mathematics) students participating in a region-wide positioning test before entering the study program. Study advisors were closely involved in the design and evaluation of the dashboard. The multi-institutional context of our case study requires careful consideration of external stakeholders and data ownership and portability issues, which gives shape to the technical design of the LAD. Our approach confirms students as active agents with data ownership, using an anonymous feedback code to access the LAD and to enable students to share their data with institutions at their discretion. Other distinguishing features of the LAD are the support for active content contribution by study advisors and LATEX type-setting of question item feedback to enhance visual recognizability. We present our lessons learnt from a first iteration in production.",
    url = "https://doi.org/10.1145/3170358.3170419"
}

@article{ref359_d8706c,
    author = "Gibson, Andrew and Martínez‐Maldonado, Roberto",
    title = "That dashboard looks nice, but what does it mean?",
    year = "2017",
    doi = "10.1145/3152771.3156171",
    abstract = "As learning analytics (LA) systems become more common, teachers and students are often required to not only make sense of the user interface (UI) elements of a system, but also to make meaning that is pedagogically appropriate to the learning context. However, we suggest that the dominant way of thinking about the relationship between representation and meaning results in an overemphasis on the UI, and that re-thinking this relationship is necessary to create systems that can facilitate deeper meaning making. We propose a conceptual view as a basis for discussion among the LA and HCI communities around a different way of thinking about meaning making, specifically that it should be explicit in the design process, provoking greater consideration of system level elements such as algorithms, data structures and information flow. We illustrate the application of the conceptualisation with two cases of LA design in the areas of Writing Analytics and Multi-modal Dashboards.",
    url = "https://doi.org/10.1145/3152771.3156171"
}

@article{ref360_71addb,
    author = "Sclater, Niall and Berg, Alan and Webb, Michael",
    title = "Developing an open architecture for learning analytics",
    year = "2015"
}

@article{ref361_7f7815,
    author = "Tretow-Fish, Tobias and Andersen, Jesper and Khalid, Saifuddin",
    title = "Prototyping an Adaptive Learning Platform's Learning Analytic Dashboards on Behavioral Data to Support Teachers' Pedagogical Actions",
    year = "2024",
    doi = "10.1109/icaccess61735.2024.10499493",
    url = "https://doi.org/10.1109/icaccess61735.2024.10499493"
}

@article{ref362_8624e0,
    author = "West, Deborah and Luzeckyj, Ann and Searle, Bill and Toohey, Danny and Vanderlelie, Jessica and Bell, Kevin",
    title = "Perspectives from the stakeholder: Students’ views regarding learning analytics and data collection",
    year = "2020",
    issn = "1449-3098",
    doi = "10.14742/ajet.5957",
    abstract = "This article reports on a study exploring student perspectives on the collection and use of student data for learning analytics. With data collected via a mixed methods approach from 2,051 students across six Australian universities, it provides critical insights from students as a key stakeholder group. Findings indicate that while students are generally comfortable with the use of data to support their learning, they do have concerns particularly in relation to the use of demographic data, location data and data collected from wireless networks, social media and mobile applications. Two key themes emerged related to the need for transparency to support informed consent and personal-professional boundary being critical. This supports findings from other research, which reflects the need for a nuanced approach when providing information to students about the data we collect, including what we are collecting, why and how this is being used.\&\#x0D; Implications for practice or policy:\&\#x0D; \&\#x0D; When implementing the use of dashboards, institutions should ideally include opportunities for students to opt in and out, rather than being set so that students have agency over their data and learning.\&\#x0D; When undertaking work in relation to learning analytics, staff need to ensure the focus of their work relates to student learning rather than academic research.\&\#x0D; When institutions and academic staff collect and use student data (regardless of the purpose for doing so), all aspects of these processes need to be transparent to students.\&\#x0D;",
    url = "https://doi.org/10.14742/ajet.5957"
}

@article{ref363_a9a186,
    author = "Ley, Tobias and Tammets, Kairit and Pishtari, Gerti and Chejara, Pankaj and Kasepalu, Reet and Khalil, Mohammad and Saar, Merike and Tuvi, Iiris and Väljataga, Terje and Wasson, Barbara",
    title = "Towards a partnership of teachers and intelligent learning technology: A systematic literature review of model‐based learning analytics",
    year = "2023",
    issn = "0266-4909",
    doi = "10.1111/jcal.12844",
    abstract = "Abstract Background With increased use of artificial intelligence in the classroom, there is now a need to better understand the complementarity of intelligent learning technology and teachers to produce effective instruction. Objective The paper reviews the current research on intelligent learning technology designed to make models of student learning and instruction transparent to teachers, an area we call model‐based learning analytics. We intended to gain an insight into the coupling between the knowledge models that underpin the intelligent system and the knowledge used by teachers in their classroom decision making. Methods Using a systematic literature review methodology, we first identified 42 papers, mainly from the domain of intelligent tutoring systems and learning analytics dashboards that conformed to our selection criteria. We then qualitatively analysed the context in which the systems were applied, models they used and benefits reported for teachers and learners. Results and Conclusions A majority of papers used either domain or learner models, suggesting that instructional decisions are mostly left to teachers. Compared to previous reviews, our set of papers appeared to have a stronger focus on providing teachers with theory‐driven insights and instructional decisions. This suggests that model‐based learning analytics can address some of the shortcomings of the field, like meaningfulness and actionability of learning analytics tools. However, impact in the classroom still needs further research, as in half of the cases the reported benefits were not backed with evidence. Future research should focus on the dynamic interaction between teachers and technology and how learning analytics has an impact on learning and decision making by teachers and students. We offer a taxonomy of knowledge models that can serve as a starting point for designing such interaction.",
    url = "https://doi.org/10.1111/jcal.12844"
}

@article{ref364_0e9441,
    author = "Oliver-Quelennec, Katia and Bouchet, François and Carron, Thibault and Pinçon, Claire",
    title = "Evaluating the Transposition of a Learning Analytics Dashboard Co-design Tangible Tool to a Digital Tool",
    year = "2022",
    issn = "2662-5628",
    doi = "10.1007/978-3-031-18512-0\_8",
    url = "https://doi.org/10.1007/978-3-031-18512-0\_8"
}

@article{ref365_abfe6e,
    author = "Brun, Armelle and Bonnin, Geoffray and Castagnos, Sylvain and Roussanaly, Azim and Boyer, Anne",
    title = "Learning analytics made in France: the METAL project",
    year = "2019",
    issn = "2056-4880",
    doi = "10.1108/ijilt-02-2019-0022",
    abstract = "Purpose The purpose of this paper is to present the METAL project, a French open learning analytics (LA) project for secondary school, that aims at improving the quality of teaching. The originality of METAL is that it relies on research through exploratory activities and focuses on all the aspects of a learning analytics environment. Design/methodology/approach This work introduces the different concerns of the project: collection and storage of multi-source data owned by a variety of stakeholders, selection and promotion of standards, design of an open-source LRS, conception of dashboards with their final users, trust, usability, design of explainable multi-source data-mining algorithms. Findings All the dimensions of METAL are presented, as well as the way they are approached: data sources, data storage, through the implementation of an LRS, design of dashboards for secondary school, based on co-design sessions data mining algorithms and experiments, in line with privacy and ethics concerns. Originality/value The issue of a global dissemination of LA at an institution level or at a broader level such as a territory or a study level is still a hot topic in the literature, and is one of the focus and originality of this paper, associated with the large spectrum of different concerns.",
    url = "https://doi.org/10.1108/ijilt-02-2019-0022"
}

@article{ref366_c0dcaf,
    author = "Li, Siyuan",
    title = "Investigating the Role of Student Ownership in the Design ofStudent-facing Learning Analytics Dashboards (SFLADs) in Relationto Student Perceptions of SFLADs",
    year = "2019"
}

@article{ref367_ab02e5,
    author = "Charleer, Sven and Odriozola, Jose and Klerkx, Joris and Duval, Erik",
    title = "LARAe: learning analytics reflection \& awareness environment",
    year = "2014"
}

@article{ref368_2cebc5,
    author = "Oliver-Quelennec, Katia",
    title = "Student learning analytics dashboards : adapted, adaptable, and adaptive over timeFrom design to modelling",
    year = "2024"
}

@article{ref369_c2d94a,
    author = "Hatala, Marek and Nazeri, Sina",
    title = "Associations between Students’ Standing Seen in Learning Analytics Dashboards and Their Following Learning Behaviours:",
    year = "2024",
    issn = "1929-7750",
    doi = "10.18608/jla.2024.8547",
    abstract = "An essential part of making dashboards more effective in motivating students and leading to desirable behavioural change is knowing what information to communicate to the student and how to frame and present it. Most of the research studying dashboards' impact on learning analyzes learning indicators of students as a group. Understanding how a student's learning unfolds after viewing the dashboard is necessary for personalized dashboard selection and its content. In the context of the discussion activity, we analyzed 28,290 actions of 896 students after they saw their learning status on the dashboards, which were integrated into 21 discussions in 11 courses. We provide a comparative perspective on three dashboard types: the class average, the leaderboard, and message-quality dashboards. Our results indicate that students' behaviours after viewing three dashboards were associated with their displayed standing in the discussion: views showing the student's status below the frame of reference were associated with a higher likelihood of posting, and views of the student outperforming the norm with diminished further posting, although demonstrating higher discussion engagement. We reiterate a need to understand the impact of dashboard states on students' behaviour, creating a foundation for a personalized selection of dashboard views based on individual students' standing.",
    url = "https://doi.org/10.18608/jla.2024.8547"
}

@article{ref370_e6ce92,
    author = "Tackett, Sean and Green, David and Dyal, Michael and O'Keefe, Erin and Thomas, Tanya and Nguyen, Tiffany and Vo, Duyen and Patel, Mausam and Murdock, Christopher and Wolfe, Erin and Shehadeh, Lina",
    title = "Use of Commercially Produced Medical Education Videos in a Cardiovascular Curriculum: Multiple Cohort Study",
    year = "2021",
    issn = "2369-3762",
    doi = "10.2196/27441",
    abstract = "Background Short instructional videos can make learning more efficient through the application of multimedia principles, and video animations can illustrate the complex concepts and dynamic processes that are common in health sciences education. Commercially produced videos are commonly used by medical students but are rarely integrated into curricula. Objective Our goal was to examine student engagement with medical education videos incorporated into a preclinical Cardiovascular Systems course. Methods Students who took the first-year 8-week Cardiovascular Systems course in 2019 and 2020 were included in the study. Videos from Osmosis were recommended to be watched before live sessions throughout the course. Video use was monitored through dashboards, and course credit was given for watching videos. All students were emailed electronic surveys after the final exam asking about the course’s blended learning experience and use of videos. Osmosis usage data for number of video views, multiple choice questions, and flashcards were extracted from Osmosis dashboards. Results Overall, 232/359 (64.6\%) students completed surveys, with rates by class of 81/154 (52.6\%) for MD Class of 2022, 39/50 (78\%) for MD/MPH Class of 2022, and 112/155 (72.3\%) for MD Class of 2023. Osmosis dashboard data were available for all 359 students. All students received the full credit offered for Osmosis engagement, and learning analytics demonstrated regular usage of videos and other digital platform features. Survey responses indicated that most students found Osmosis videos to be helpful for learning (204/232, 87.9\%; P=.001) and preferred Osmosis videos to the traditional lecture format (134/232, 57.8\%; P\&lt;.001). Conclusions Commercial medical education videos may enhance curriculum with low faculty effort and improve students’ learning experiences. Findings from our experience at one medical school can guide the effective use of supplemental digital resources for learning, and related evaluation and research.",
    url = "https://doi.org/10.2196/27441"
}

@article{ref371_a09a9c,
    author = "Shaikh, Mohammad and Ali, Syed",
    title = "An Integrated Analysis of the Effects of Learning Analytics Dashboards on Learner Perspective, Inspiration, Engagement, and Accomplishment",
    year = "2025",
    doi = "10.4018/979-8-3693-8593-7.ch002",
    url = "https://doi.org/10.4018/979-8-3693-8593-7.ch002"
}

@article{ref372_e431a6,
    author = "Volarić, Tomislav and Ljubić, Hrvoje",
    title = "Learner and course dashboards for intelligent learning management systems",
    year = "2017",
    doi = "10.23919/softcom.2017.8115555",
    url = "https://doi.org/10.23919/softcom.2017.8115555"
}

@article{ref373_48daca,
    author = "Beile, Penny and Choudhury, Kanak and Mulvihill, Rachel and Wang, Morgan",
    title = "Aligning Library Assessment with Institutional Priorities: A Study of Student Academic Performance and Use of Five Library Services",
    year = "2020",
    issn = "0010-0870",
    doi = "10.5860/crl.81.3.435",
    abstract = "This large-scale study was conducted for the purposes of determining how representative library users are compared to the whole student population, to explore how library services contribute to student success, and to position the library to be included in the institution's learning analytics landscape. To that end, data were collected as students at University of Central Florida (n = 25,336) interacted with five library service points over four semesters. Analysis revealed a positive association between students who used one or more library services and higher end-of-semester GPAs. The article emphasizes how results were disseminated and ongoing work to build an interactive learning analytics library dashboard that complements existing institutional dashboards.",
    url = "https://doi.org/10.5860/crl.81.3.435"
}

@article{ref374_23f778,
    author = "Aguilar, Stephen",
    title = "Experimental Evidence of Performance Feedback vs. Mastery Feedback on Students’ Academic Motivation",
    year = "2022",
    doi = "10.1145/3506860.3506916",
    abstract = "Work throughout the learning analytics community has examined associations between Learning Analytics Dashboard (LAD) features and a number of important student outcomes, including academic motivation and self-regulated learning strategies. While there are many potential implications of visualized academic information within a LAD on student outcomes, there remains an unanswered question: are there causal differences between showing performance information (e.g., comparing students' progress to the class average) vs. mastery information (e.g., their individual score) on students' motivation?",
    url = "https://doi.org/10.1145/3506860.3506916"
}

@article{ref375_b5e7f9,
    author = "Urick, Angela",
    title = "Student Engagement as a Learning Analytic: Dashboard Development for the K20 Inventory of Student, School, and Career Engagement",
    year = "2023",
    doi = "10.3102/ip.23.2015290",
    url = "https://doi.org/10.3102/ip.23.2015290"
}

@article{ref376_6a592c,
    author = "Kaliisa, Rogers and Misiejuk, Kamila and López‐Pernas, Sonsoles and Khalil, Mohammad and Saqr, Mohammed",
    title = "Have Learning Analytics Dashboards Lived Up to the Hype? A Systematic Review of Impact on Students' Achievement, Motivation, Participation and Attitude",
    year = "2023",
    doi = "10.48550/arxiv.2312.15042",
    abstract = "While learning analytics dashboards (LADs) are the most common form of LA intervention, there is limited evidence regarding their impact on students learning outcomes. This systematic review synthesizes the findings of 38 research studies to investigate the impact of LADs on students' learning outcomes, encompassing achievement, participation, motivation, and attitudes. As we currently stand, there is no evidence to support the conclusion that LADs have lived up to the promise of improving academic achievement. Most studies reported negligible or small effects, with limited evidence from well-powered controlled experiments. Many studies merely compared users and non-users of LADs, confounding the dashboard effect with student engagement levels. Similarly, the impact of LADs on motivation and attitudes appeared modest, with only a few exceptions demonstrating significant effects. Small sample sizes in these studies highlight the need for larger-scale investigations to validate these findings. Notably, LADs showed a relatively substantial impact on student participation. Several studies reported medium to large effect sizes, suggesting that LADs can promote engagement and interaction in online learning environments. However, methodological shortcomings, such as reliance on traditional evaluation methods, self-selection bias, the assumption that access equates to usage, and a lack of standardized assessment tools, emerged as recurring issues. To advance the research line for LADs, researchers should use rigorous assessment methods and establish clear standards for evaluating learning constructs. Such efforts will advance our understanding of the potential of LADs to enhance learning outcomes and provide valuable insights for educators and researchers alike.",
    url = "https://doi.org/10.48550/arxiv.2312.15042"
}

@article{ref377_ccf91b,
    author = "Franzoni, Valentina and Milani, Alfredo and Mengoni, Paolo and Piccinato, Fabrizio",
    title = "Artificial Intelligence Visual Metaphors in E-Learning Interfaces for Learning Analytics",
    year = "2020",
    issn = "2076-3417",
    doi = "10.3390/app10207195",
    abstract = "This work proposes an innovative visual tool for real-time continuous learners analytics. The purpose of the work is to improve the design, functionality, and usability of learning management systems to monitor user activity to allow educators to make informed decisions on e-learning design, usually limited to dashboards graphs, tables, and low-usability user logs. The standard visualisation is currently scarce, and often inadequate to inform educators about the design quality and students engagement on their learning objects. The same low usability can be found in learning analytics tools, which mostly focus on post-course analysis, demanding specific skills to be effectively used, e.g., for statistical analysis and database queries. We propose a tool for student analytics embedded in a Learning Management System, based on the innovative visual metaphor of interface morphing. Artificial intelligence provides in remote learning immediate feedback, crucial in a face-to-face setting, highlighting the students’ engagement in each single learning object. A visual metaphor is the representation of a person, group, learning object, or concept through a visual image that suggests a particular association or point of similarity. The basic idea is that elements of the application interface, e.g., learning objects’ icons and student avatars, can be modified in colour and dimension to reflect key performance indicators of learner’s activities. The goal is to provide high-affordance information on the student engagement and usage of learning objects, where aggregation functions on subsets of users allow a dynamic evaluation of cohorts with different granularity. The proposed visual metaphors (i.e., thermometer bar, dimensional morphing, and tag cloud morphing) have been implemented and experimented within academic-level courses. Experimental results have been evaluated with a comparative analysis of user logs and a subjective usability survey, which show that the tool obtains quantitative, measurable effectiveness and the qualitative appreciation of educators. Among metaphors, the highest success is obtained by Dimensional morphing and Tag cloud transformation.",
    url = "https://doi.org/10.3390/app10207195"
}

@article{ref378_15f0de,
    author = "Kloos, Carlos and Alario‐Hoyos, Carlos and Fernández-Panadero, Carmen and Ayres, Iria and Muñoz‐Merino, Pedro and Cobos, Ruth and Moreno, Jaime and Tovar, Edmundo and Cabedo, Rosa and Piedra, Nelson and Chicaiza, Janneth and López, Jorge",
    title = "eMadrid project: MOOCs and learning analytics",
    year = "2016",
    doi = "10.1109/siie.2016.7751870",
    url = "https://doi.org/10.1109/siie.2016.7751870"
}

@article{ref379_25c25c,
    author = "Ifenthaler, Dirk",
    title = "Learning analytics for school and system management",
    year = "2021",
    issn = "2788-8568",
    doi = "10.1787/d535b828-en",
    url = "https://doi.org/10.1787/d535b828-en"
}

@article{ref380_ea2678,
    author = "Khalifa, Mohamed and Riyami, Bouchaïb and Ouatiq, Amina",
    title = "The Preferences and Needs of Higher Education Students from Learning Analytics Dashboards in a Blended Learning Environment",
    year = "2025",
    issn = "1740-2832",
    doi = "10.1504/ijtip.2025.10070077",
    url = "https://doi.org/10.1504/ijtip.2025.10070077"
}

@article{ref381_4c9d76,
    title = {Review for "Applying learning analytics dashboards based on process‐oriented feedback to improve students' learning effectiveness"},
    year = "2020",
    doi = "10.1111/jcal.12502/v1/review1",
    url = "https://doi.org/10.1111/jcal.12502/v1/review1"
}

@article{ref382_4c9d76,
    author = "Rienties, Bart",
    title = {Review for "Applying learning analytics dashboards based on process‐oriented feedback to improve students' learning effectiveness"},
    year = "2020",
    doi = "10.1111/jcal.12502/v1/review2",
    url = "https://doi.org/10.1111/jcal.12502/v1/review2"
}

@article{ref383_4c9d76,
    title = {Review for "Applying learning analytics dashboards based on process‐oriented feedback to improve students' learning effectiveness"},
    year = "2020",
    doi = "10.1111/jcal.12502/v2/review1",
    url = "https://doi.org/10.1111/jcal.12502/v2/review1"
}

@article{ref384_9c3de9,
    author = "Dyckhoff, Anna and Schroeder, Ulrik",
    title = "Action Research and Learning Analytics in Higher Education",
    year = "2014"
}

@article{ref385_862af3,
    author = "Manske, Sven and Hecking, Tobias and Bollen, Lars and Göhnert, Tilman and Ramos, Alfredo and Hoppe, H.",
    title = "A Flexible Framework for the Authoring of Reusable and Portable Learning Analytics Gadgets",
    year = "2014",
    doi = "10.1109/icalt.2014.80",
    url = "https://doi.org/10.1109/icalt.2014.80"
}

@article{ref386_afd743,
    author = "Pelánek, Radek",
    title = "Analyzing and Visualizing Learning Data: A System Designer's Perspective",
    year = "2021",
    issn = "1929-7750",
    doi = "10.18608/jla.2021.7345",
    abstract = "In this work, we consider learning analytics for primary and secondary schools from the perspective of the designer of a learning system. We provide an overview of practically useful analytics techniques with descriptions of their applications and specific illustrations. We highlight data biases and caveats that complicate the analysis and its interpretation. Although we intentionally focus on techniques for internal use by designers, many of these techniques may inspire the development of dashboards for teachers or students. We also identify the consequences and challenges for research.",
    url = "https://doi.org/10.18608/jla.2021.7345"
}

@article{ref387_4e8912,
    author = "Wang, Dongqing and Han, Hou",
    title = {Author response for "Applying learning analytics dashboards based on process‐oriented feedback to improve students' learning effectiveness"},
    year = "2020",
    doi = "10.1111/jcal.12502/v3/response1",
    url = "https://doi.org/10.1111/jcal.12502/v3/response1"
}

@article{ref388_966807,
    title = {Decision letter for "Applying learning analytics dashboards based on process‐oriented feedback to improve students' learning effectiveness"},
    year = "2020",
    doi = "10.1111/jcal.12502/v3/decision1",
    url = "https://doi.org/10.1111/jcal.12502/v3/decision1"
}

@article{ref389_01897e,
    author = "Tobarra, Llanos and Ros, Salvador and Hernández, Roberto and Robles-Gómez, Antonio and Caminero, Agustín and Vargas, Rafael",
    title = "Integrated Analytic dashboard for virtual evaluation laboratories and collaborative forums",
    year = "2014",
    doi = "10.1109/taee.2014.6900177",
    url = "https://doi.org/10.1109/taee.2014.6900177"
}

@article{ref390_966807,
    title = {Decision letter for "Applying learning analytics dashboards based on process‐oriented feedback to improve students' learning effectiveness"},
    year = "2020",
    doi = "10.1111/jcal.12502/v1/decision1",
    url = "https://doi.org/10.1111/jcal.12502/v1/decision1"
}

@article{ref391_966807,
    title = {Decision letter for "Applying learning analytics dashboards based on process‐oriented feedback to improve students' learning effectiveness"},
    year = "2020",
    doi = "10.1111/jcal.12502/v2/decision1",
    url = "https://doi.org/10.1111/jcal.12502/v2/decision1"
}

@article{ref392_63c48b,
    author = "Balaban, Igor and Zlatović, Miran and Matus, Marko",
    title = "Using learning analytics dashboards to monitor student progress: the case of a blended computer science university course",
    year = "2024",
    doi = "10.1109/icalt61570.2024.00027",
    url = "https://doi.org/10.1109/icalt61570.2024.00027"
}

@article{ref393_4e8912,
    author = "Wang, Dongqing and Han, Hou",
    title = {Author response for "Applying learning analytics dashboards based on process‐oriented feedback to improve students' learning effectiveness"},
    year = "2020",
    doi = "10.1111/jcal.12502/v2/response1",
    url = "https://doi.org/10.1111/jcal.12502/v2/response1"
}

@article{ref394_eb538b,
    author = "Wang, Han and Huang, Tao and Zhao, Yuan and Hu, Shengze",
    title = "The Impact of Dashboard Feedback Type on Learning Effectiveness, Focusing on Learner Differences",
    year = "2023",
    issn = "2071-1050",
    doi = "10.3390/su15054474",
    abstract = "With the exponential growth of educational data, increasing attention has been given to student learning supported by learning analytics dashboards. Related research has indicated that dashboards relying on descriptive analytics are deficient compared to more advanced analytics. However, there is a lack of empirical data to demonstrate the performance and differences between different types of analytics in dashboards. To investigate these, the study used a controlled, between-groups experimental method to compare the effects of descriptive and prescriptive dashboards on learning outcomes. Based on the learning analytics results, the descriptive dashboard describes the learning state and the prescriptive dashboard provides suggestions for learning paths. The results show that both descriptive and prescriptive dashboards can effectively promote students’ cognitive development. The advantage of prescriptive dashboard over descriptive dashboard is its promotion in learners’ learning strategies. In addition, learners’ prior knowledge and learning strategies determine the extent of the impact of dashboard feedback on learning outcomes.",
    url = "https://doi.org/10.3390/su15054474"
}

@article{ref395_9f129c,
    author = "Bañeres, David and Guerrero‐Roldán, Ana‐Elena and Rodríguez, M. and Karadeniz, Abdülkadir",
    title = "A Predictive Analytics Infrastructure to Support a Trustworthy Early Warning System",
    year = "2021",
    issn = "2076-3417",
    doi = "10.3390/app11135781",
    abstract = "Learning analytics is quickly evolving. Old fashioned dashboards with descriptive information and trends about what happened in the past are slightly substituted by new dashboards with forecasting information and predicting relevant outcomes about learning. Artificial intelligence is aiding this revolution. The accessibility to computational resources has increased, and specific tools and packages for integrating artificial intelligence techniques leverage such new analytical tools. However, it is crucial to develop trustworthy systems, especially in education where skepticism about their application is due to the risk of teachers’ replacement. However, artificial intelligence systems should be seen as companions to empower teachers during the teaching and learning process. During the past years, the Universitat Oberta de Catalunya has advanced developing a data mart where all data about learners and campus utilization are stored for research purposes. The extensive collection of these educational data has been used to build a trustworthy early warning system whose infrastructure is introduced in this paper. The infrastructure supports such a trustworthy system built with artificial intelligence procedures to detect at-risk learners early on in order to help them to pass the course. To assess the system’s trustworthiness, we carried out an evaluation on the basis of the seven requirements of the European Assessment List for trustworthy artificial intelligence (ALTAI) guidelines that recognize an artificial intelligence system as a trustworthy one. Results show that it is feasible to build a trustworthy system wherein all seven ALTAI requirements are considered at once from the very beginning during the design phase.",
    url = "https://doi.org/10.3390/app11135781"
}

@article{ref396_ed4319,
    author = "Wang, Zuo and Ng, Jeremy and Liu, Ruilun and Hu, Xiao",
    title = "Learning Analytics Enabled Virtual Reality Content Creation Platform: System Design and Preliminary Evaluation",
    year = "2022",
    doi = "10.1109/icalt55010.2022.00055",
    url = "https://doi.org/10.1109/icalt55010.2022.00055"
}

@article{ref397_3c5d72,
    author = "Jerez, Alex and Guenaga, Mariluz and Núñez, Asier",
    title = "A web platform for the assessment of competences in Mobile Learning Contexts",
    year = "2014",
    doi = "10.1109/educon.2014.6826111",
    url = "https://doi.org/10.1109/educon.2014.6826111"
}

@article{ref398_e3b088,
    author = "Urick, Angela",
    title = "Student Engagement as a Learning Analytic: Dashboard Development for the K20 Inventory of Student, School, and Career Engagement (Poster 11)",
    year = "2023",
    doi = "10.3102/2015290",
    url = "https://doi.org/10.3102/2015290"
}

@article{ref399_9bc158,
    author = "Thomas, Michael and Reinders, Hayo and Gelan, Anouk",
    title = "Learning Analytics in Online Language Learning",
    year = "2017",
    doi = "10.4324/9781315205618-13",
    abstract = "This chapter addresses the challenges and future potential of learning analytics. It examines some of the key questions raised by the research literature that will influence language education over the next decade, and investigates what kind of data can be used to inform effective decision-making in online language-learning contexts and how it can be visualized. The chapter turns to consider preliminary data arising from the needs analysis phase of the VITAL Project (Visualization Tools and Analytics to Monitor Online Language Learning and Teaching), a two-year EU-funded project that specifically addresses the gap in the research literature on analytics in language learning and teaching. Turning to the first large-scale project on learning analytics and online language learning, Link \& Li's theoretical framework provides a useful starting point to consider the role of dashboards for language learners and instructors.",
    url = "https://doi.org/10.4324/9781315205618-13"
}

@article{ref400_f0f35e,
    author = "Echeverría, Vanessa and Yan, Lixiang and Zhao, Linxuan and Abel, Sophie and Alfredo, Riordan and Dix, Samantha and Jaggard, Hollie and Wotherspoon, Rosie and Osborne, Abra and Shum, Simon and Gašević, Dragan and Martínez‐Maldonado, Roberto",
    title = "TeamSlides: a Multimodal Teamwork Analytics Dashboard for Teacher-guided Reflection in a Physical Learning Space",
    year = "2024",
    doi = "10.1145/3636555.3636857",
    abstract = "Advancements in Multimodal Learning Analytics (MMLA) have the potential to enhance the development of effective teamwork skills and foster reflection on collaboration dynamics in physical learning environments. Yet, only a few MMLA studies have closed the learning analytics loop by making MMLA solutions immediately accessible to educators to support reflective practices, especially in authentic settings. Moreover, deploying MMLA solutions in authentic settings can bring new challenges beyond logistic and privacy issues. This paper reports the design and use of TeamSlides, a multimodal teamwork analytics dashboard to support teacher-guided reflection. We conducted an in-the-wild classroom study involving 11 teachers and 138 students. Multimodal data were collected from students working in team healthcare simulations. We examined how teachers used the dashboard in 22 debrief sessions to aid their reflective practices. We also interviewed teachers to discuss their perceptions of the dashboard's value and the challenges faced during its use. Our results suggest that the dashboard effectively reinforced discussions and augmented teacher-guided reflection practices. However, teachers encountered interpretation conflicts, sometimes leading to mistrust or misrepresenting the information. We discuss the considerations needed to overcome these challenges in MMLA research.",
    url = "https://doi.org/10.1145/3636555.3636857"
}

@article{ref401_7e38b2,
    author = "Clauss, Alexander and Lenk, Florian and Schoop, Eric",
    title = "Enhancing International Virtual Collaborative Learning with Social Learning Analytics",
    year = "2019",
    doi = "10.1109/ictcs.2019.8923106",
    url = "https://doi.org/10.1109/ictcs.2019.8923106"
}

@article{ref402_a68ecb,
    author = "Giorgashvili, Tornike and Jivet, Ioana and Artelt, Cordula and Biedermann, Daniel and Bengs, Daniel and Goldhammer, Frank and Hahnel, Carolin and Mendzheritskaya, Julia and Mordel, Julia and Onofrei, Monica and Winter, Marc and Wolter, Ilka and Horz, Holger and Drachsler, Hendrik",
    title = "Exploring Learners’ Self-reflection and Intended Actions After Consulting Learning Analytics Dashboards in an Authentic Learning Setting",
    year = "2024",
    issn = "0302-9743",
    doi = "10.1007/978-3-031-72315-5\_10",
    url = "https://doi.org/10.1007/978-3-031-72315-5\_10"
}

@article{ref403_52f5d1,
    author = "Cobos, Ruth",
    title = "Self-Regulated Learning and Active Feedback of MOOC Learners Supported by the Intervention Strategy of a Learning Analytics System",
    year = "2023",
    issn = "2079-9292",
    doi = "10.3390/electronics12153368",
    abstract = "MOOCs offer great learning opportunities, but they also present several challenges for learners that hinder them from successfully completing MOOCs. To address these challenges, edX-LIMS (System for Learning Intervention and its Monitoring for edX MOOCs) was developed. It is a learning analytics system that supports an intervention strategy (based on learners’ interactions with the MOOC) to provide feedback to learners through web-based Learner Dashboards. Additionally, edX-LIMS provides a web-based Instructor Dashboard for instructors to monitor their learners. In this article, an enhanced version of the aforementioned system called edX-LIMS+ is presented. This upgrade introduces new services that enhance both the learners’ and instructors’ dashboards with a particular focus on self-regulated learning. Moreover, the system detects learners’ problems to guide them and assist instructors in better monitoring learners and providing necessary support. The results obtained from the use of this new version (through learners’ interactions and opinions about their dashboards) demonstrate that the feedback provided has been significantly improved, offering more valuable information to learners and enhancing their perception of both the dashboard and the intervention strategy supported by the system. Additionally, the majority of learners agreed with their detected problems, thereby enabling instructors to enhance interventions and support learners’ learning processes.",
    url = "https://doi.org/10.3390/electronics12153368"
}

@article{ref404_db147e,
    author = "Aghaei, Kimia and Hatala, Marek and Mogharrab, Alireza",
    title = "How Students’ Emotion and Motivation Changes After Viewing Dashboards with Varied Social Comparison Group: A Qualitative Study",
    year = "2023",
    doi = "10.1145/3576050.3576107",
    url = "https://doi.org/10.1145/3576050.3576107"
}

@article{ref405_09b151,
    author = "Molenaar, Inge and Campen, C.A.N. and Hasselman, Fred",
    title = "The effects of a learning analytics empowered technology on students' arithmetic skill development",
    year = "2017",
    doi = "10.1145/3027385.3029488",
    abstract = "Learning analytics empowered educational technologies (LA-ET) in primary classrooms allow for blended learning scenarios with teacher-lead instructions, class-paced and individually-paced practice. This quasi-experimental study investigates the effects of a LA-ET on the development of students' arithmetic skills over one schoolyear. Children learning in a traditional paper \& pencil condition were compared to learners using a LA-ET on tablet computers in grade 4. The educational technology combined teacher dashboards (extracted analytics) and class and individually paced assignments (embedded analytics). The results indicated that children in the LA-ET condition made significantly more progress on arithmetic skills in one schoolyear compared to children in the paper \& pencil condition.",
    url = "https://doi.org/10.1145/3027385.3029488"
}

@article{ref406_706719,
    author = "Hwang, Yoonja and Kim, Insook and Seo, Youn-Kyung",
    title = "A Study on the Development of Faculty Dashboard Prototype Based on Learning Analytics for University Students' Competency Enhancement: A University of Engineering Students",
    year = "2021",
    issn = "2586-3886",
    doi = "10.31137/ecc.2021.5.3.55",
    url = "https://doi.org/10.31137/ecc.2021.5.3.55"
}

@article{ref407_f0c3a4,
    author = "He, Lingjun and Levine, Richard and Bohonak, Andrew and Fan, Juanjuan and Stronach, Jeanne",
    title = "Predictive Analytics Machinery for STEM Student Success Studies",
    year = "2018",
    issn = "0883-9514",
    doi = "10.1080/08839514.2018.1483121",
    url = "https://doi.org/10.1080/08839514.2018.1483121"
}

@article{ref408_e42092,
    author = "van der Stappen, Esther",
    title = "Workplace learning analytics in higher engineering education",
    year = "2018",
    doi = "10.1109/educon.2018.8363102",
    url = "https://doi.org/10.1109/educon.2018.8363102"
}

@article{ref409_9a8642,
    author = "Yeşilyurt, Yusuf",
    title = "AI-Enabled Assessment and Feedback Mechanisms for Language Learning",
    year = "2023",
    issn = "2326-8905",
    doi = "10.4018/978-1-6684-9893-4.ch002",
    url = "https://doi.org/10.4018/978-1-6684-9893-4.ch002"
}

@article{ref410_e07da8,
    author = "Winer, Amir and Geri, Nitza",
    title = "Learning analytics performance improvement design (LAPID) in higher education: Framework and concerns",
    year = "2019",
    issn = "2325-4688",
    doi = "10.36965/ojakm.2019.7(2)41-55",
    url = "https://doi.org/10.36965/ojakm.2019.7(2)41-55"
}

@article{ref411_279a9f,
    author = "Ng, Jeremy and Wang, Zuo and Hu, Xiao",
    title = "Needs Analysis and Prototype Evaluation of Student-facing LA Dashboard for Virtual Reality Content Creation",
    year = "2022",
    doi = "10.1145/3506860.3506880",
    url = "https://doi.org/10.1145/3506860.3506880"
}

@article{ref412_5abc3b,
    author = "Waldmann, Maximilian and Walgenbach, Katharina",
    title = "Digitalisierung der Hochschulbildung",
    year = "2020",
    issn = "0044-3247",
    doi = "10.3262/zp2003357",
    url = "https://doi.org/10.3262/zp2003357"
}

@article{ref413_01513b,
    author = "Lee, Jeongwon and Kim, Dongho",
    title = "From awareness to empowerment: self-determination theory-informed learning analytics dashboards to enhance student engagement in asynchronous online courses",
    year = "2024",
    issn = "1042-1726",
    doi = "10.1007/s12528-024-09416-2",
    url = "https://doi.org/10.1007/s12528-024-09416-2"
}

@article{ref414_d205b5,
    author = "Sabuncuoğlu, Alpay and Sezgin, Tevfik",
    title = "Developing a Multimodal Classroom Engagement Analysis Dashboard for Higher-Education",
    year = "2023",
    issn = "2573-0142",
    doi = "10.1145/3593240",
    abstract = "Developing learning analytics dashboards (LADs) is a growing research interest as online learning tools have become more accessible in K-12 and higher education settings. This paper reports our multimodal classroom engagement data analysis and dashboard design process and the resulting engagement dashboard. Our work stems from the importance of monitoring classroom engagement, which refers to students' active physical and cognitive involvement in learning that influences their motivation and success in a given course. To monitor this vital facade of learning, we developed an engagement dashboard using an iterative and user-centered process. We first created a multimodal machine learning model that utilizes face and pose features obtained from recent deep learning models. Then, we created a dashboard where users can view their engagement over time and discover their learning/teaching patterns. Finally, we conducted user studies with undergraduate and graduate-level participants to obtain feedback on our dashboard design. Our paper makes three contributions by (1) presenting a student-centric, open-source dashboard, (2) demonstrating a baseline architecture for engagement analysis using our open-access data, and (3) presenting user insights and design takeaways to inspire future LADs. We expect our research to guide the development of tools for novice teacher education, student self-evaluation, and engagement evaluation in crowded classrooms.",
    url = "https://doi.org/10.1145/3593240"
}

@article{ref415_e025b5,
    author = "Smit, Michelle and Bond-Barnard, Taryn",
    title = "TOWARDS A LEARNING ANALYTICS DASHBOARD TO IDENTIFY ‘AT RISK’ LEARNER PERSONAS FOR AN ONLINE, HYBRID STRUCTURED MASTERS PROGRAMME: A HUMAN-CENTRED DESIGN APPROACH",
    year = "2024",
    issn = "2340-1095",
    doi = "10.21125/iceri.2024.1591",
    url = "https://doi.org/10.21125/iceri.2024.1591"
}

@article{ref416_e617a2,
    author = "Bulut, Okan and Wongvorachan, Tarid",
    title = "Feedback Generation through Artificial Intelligence",
    year = "2022",
    issn = "2816-2021",
    doi = "10.18357/otessac.2022.2.1.125",
    abstract = "Feedback is an essential part of the educational assessment that improves student learning. As education changes with the advancement of technology, educational assessment has also adapted to the advent of Artificial Intelligence (AI). Despite the increasing use of online assessments during the last decade, a limited number of studies have discussed the feedback generation process as implemented through AI. To address this gap, we propose a conceptual paper to organize and discuss the application of AI in the feedback generation and delivery processes. Among different branches of AI, Natural Language Processing (NLP), Educational Data Mining (EDM), and Learning Analytics (LA) play the most critical roles in the feedback generation process. The process begins with analyzing students’ data from educational assessments to build a predictive machine learning model with additional features such as students’ interaction with course material using EDM methods to predict students’ learning outcomes. Written feedback can be generated from a model with NLP-based algorithms before being delivered, along with non-verbal feedback via a LA dashboard or a digital score report. Also, ethical recommendations for using AI for feedback generation are discussed. This paper contributes to understanding the feedback generation process to serve as a venue for the future development of digital feedback.",
    url = "https://doi.org/10.18357/otessac.2022.2.1.125"
}

@article{ref417_6d5b77,
    author = "Seaton, Jennifer and Graf, Sabine and Chang, Maiga and Farhmand, Arta",
    title = "Incorporating Learning Analytics in an Educational Game to Provide Players with Information about how to Improve Their Performance",
    year = "2018",
    doi = "10.1109/icalt.2018.00121",
    url = "https://doi.org/10.1109/icalt.2018.00121"
}

@article{ref418_c023d5,
    author = "Alhadad, Sakinah",
    title = "Attentional and cognitive processing of analytics visualisations:",
    year = "2016",
    issn = "2653-665X",
    doi = "10.14742/apubs.2016.826",
    abstract = "There has been an increasing demand for course-level learning analytics to inform design improvements and interventions. While there has been an increasing research and development focus on dashboards to facilitate this, less has been done to investigate the impact of design features on optimising the interpretation process when translating learning analytics into actionable interventions and design changes. In this paper, I assess the effect of two prominent design features on the attentional and cognitive processes when using learning analytics at the course level. Emergent thematic analysis revealed response patterns suggesting systematic effects of three design features (course-only data, course- versus school-level data, course-only data with learning events marked) on the interpretive patterns, proposed actions, and consequential thinking of participants in the study. Implications for future designs of course-level learning analytics dashboards, as well as academic development are discussed.",
    url = "https://doi.org/10.14742/apubs.2016.826"
}

@article{ref419_c10903,
    author = "Ouatiq, Amina and Riyami, Bouchaïb and Mansouri, Khalifa and Qbadou, Mohammed and Aoula, Es-Saâdia",
    title = "Towards the Co-Design of a Teachers' Dashboards in a Hybrid Learning Environment",
    year = "2022",
    doi = "10.1109/iraset52964.2022.9738149",
    url = "https://doi.org/10.1109/iraset52964.2022.9738149"
}

@article{ref420_00350e,
    author = "Perez, Ryan",
    title = "User-Centered Dashboard Design: Iterative Design to Support Teacher Informational Needs in Online Learning Contexts",
    year = "2019",
    doi = "10.3102/1432967",
    abstract = "Instructors need better tools to monitor student engagement within online courses.One class of tools that could provide this much needed information is learning analytics dashboards.However, current implementations do not provide instructors with the information they need or want in a format that is easily usable.To address this gap, we are conducting a design-based research project to create a dashboard that provides rich and timely information about student interactions within online courses, so instructors can make sense of what is happening and adjust their courses accordingly.We report on three iterations of this design process, which sheds light on how learning analytics dashboards can be created in a way that supports instructor needs and preferences.",
    url = "https://doi.org/10.3102/1432967"
}

@article{ref421_6547d2,
    author = "Lee‐Cultura, Serena and Sharma, Kshitij and Giannakos, Michail",
    title = "Multimodal Teacher Dashboards: Challenges and Opportunities of Enhancing Teacher Insights Through a Case Study",
    year = "2023",
    issn = "1939-1382",
    doi = "10.1109/tlt.2023.3276848",
    abstract = "Teacher dashboards provide insights on students' progress through visualisations and scores derived from data generated during teaching and learning activities (e.g., response times, task correctness) to improve teaching. Despite the potential usefulness of enhancing teacher dashboards, and the respective teaching practices, with rich information regarding students' cognitive and affective states (e.g., cognitive load), few studies on teacher dashboards have considered such information. In this study, we drew on contemporary developments of MultiModal Learning Analytics and designed a MultiModal (MM) teacher dashboard with notification system. The proposed system 1) receives data from various sensors, 2) computes relevant cognitive and affective measurements, 3) visualises the resulting measure- ments in a clean customisable interface, and 4) notifies instructors during moments of interest so they may determine an appropriate method to support struggling students. To evaluate our MM teacher dashboard, we first collected MultiModal Data (MMD), performance data, and video recordings of students' interactions during an in-situ study where 26 students engaged with a motion-based learning task. Then, we used our MM teacher dashboard to present the collected MMD and video recordings to 20 experienced teachers and educational researchers, and collected qualitative data regarding respondents' insights on the advantages and challenges of visualising students' MMD. Results showed that teachers found a MM teacher dashboard enhanced with notification system, useful to complement their pedagogical practices. We offer empirically founded guidelines for design and integration of a MM teacher dashboard with notification systems, aimed to enhance teachers' understanding of students' learning states (e.g., real-time awareness of students' stress).",
    url = "https://doi.org/10.1109/tlt.2023.3276848"
}

@article{ref422_2829ce,
    author = "حسنين, منى and محمد, وليد and الحميد, وائل and قرني, حنان",
    title = "معايير تصميم لوحة معلومات تحليلات التعلم في بيئة تعلم ذكية Learning Analytics Dashboard Design Criteria in a Smart Learning Environment",
    year = "2024",
    issn = "2682-3616",
    doi = "10.21608/jsu.2024.404063",
    url = "https://doi.org/10.21608/jsu.2024.404063"
}

@article{ref423_45098b,
    author = "Alfredo, Riordan and Nie, Lanbing and Kennedy, Paul and Power, Tamara and Hayes, Carolyn and Chen, Hui and McGregor, Carolyn and Swiecki, Zachari and Gašević, Dragan and Martínez‐Maldonado, Roberto",
    title = {"That Student Should be a Lion Tamer!" StressViz: Designing a Stress Analytics Dashboard for Teachers},
    year = "2023",
    doi = "10.1145/3576050.3576058",
    url = "https://doi.org/10.1145/3576050.3576058"
}

@article{ref424_8c31ee,
    author = "Rayón, Alex and Guenaga, Mariluz and Núñez, Asier",
    title = "Supporting competency-assessment through a learning analytics approach using enriched rubrics",
    year = "2014",
    doi = "10.1145/2669711.2669913",
    url = "https://doi.org/10.1145/2669711.2669913"
}

@article{ref425_95a333,
    author = "Di Bitonto, Pierpaolo and Pesare, Enrica and Roselli, Teresa and Rossano, Veronica",
    title = "Digitally Enhanced Assessment in Virtual Learning Environments",
    year = "2015",
    issn = "2326-3261",
    doi = "10.18293/dms2015-034",
    abstract = "One of the main challenges in teaching and learning activities is the assessment: it allows teachers and learners to improve the future activities on the basis of the previous ones.It allows a deep analysis and understanding of the whole learning process.This is particularly difficult in virtual learning environments where a general overview is not always available.In the latest years, Learning Analytics are becoming the most popular methods to analyze the data collected in the learning environments in order to support teachers and learners in the complex process of learning.If they are properly integrated in learning activities, indeed, they can supply useful information to adapt the activities on the basis of student's needs.In this context, the paper presents a solution for the digitally enhanced assessment.Two different Learning Dashboards have been designed in order to represent the most interesting Learning Analytics aiming at providing teachers and learners with easy understandable view of learning data in virtual learning environments.",
    url = "https://doi.org/10.18293/dms2015-034"
}

@article{ref426_fbbf37,
    author = "Sung, Eunmo and Jin, Sung-Hee and Kim, Younyoung",
    title = "Learning Activities and Learning Behaviors for Learning Analytics in e-Learning Environments",
    year = "2016"
}

@article{ref427_784195,
    author = "Kasepalu, Reet",
    title = "Overcoming the Difficulties for Teachers in Collaborative Learning Using Multimodal Learning Analytics",
    year = "2020",
    doi = "10.1109/icalt49669.2020.00124",
    url = "https://doi.org/10.1109/icalt49669.2020.00124"
}

@article{ref428_4a9e9a,
    author = "Rohloff, Tobias and Oldag, Soren and Renz, Jan and Meinel, Christoph",
    title = "Utilizing Web Analytics in the Context of Learning Analytics for Large-Scale Online Learning",
    year = "2019",
    doi = "10.1109/educon.2019.8725118",
    url = "https://doi.org/10.1109/educon.2019.8725118"
}

@article{ref429_bfc6ec,
    author = "Hutchins, Nicole and Biswas, Gautam",
    title = "Co‐designing teacher support technology for problem‐based learning in middle school science",
    year = "2023",
    issn = "0007-1013",
    doi = "10.1111/bjet.13363",
    abstract = "Abstract This paper provides an experience report on a co‐design approach with teachers to co‐create learning analytics‐based technology to support problem‐based learning in middle school science classrooms. We have mapped out a workflow for such applications and developed design narratives to investigate the implementation, modifications and temporal roles of the participants in the design process. Our results provide precedent knowledge on co‐designing with experienced and novice teachers and co‐constructing actionable insight that can help teachers engage more effectively with their students' learning and problem‐solving processes during classroom PBL implementations. Practitioner notes What is already known about this topic Success of educational technology depends in large part on the technology's alignment with teachers' goals for their students, teaching strategies and classroom context. Teacher and researcher co‐design of educational technology and supporting curricula has proven to be an effective way for integrating teacher insight and supporting their implementation needs. Co‐designing learning analytics and support technologies with teachers is difficult due to differences in design and development goals, workplace norms, and AI‐literacy and learning analytics background of teachers. What this paper adds We provide a co‐design workflow for middle school teachers that centres on co‐designing and developing actionable insights to support problem‐based learning (PBL) by systematic development of responsive teaching practices using AI‐generated learning analytics. We adapt established human‐computer interaction (HCI) methods to tackle the complex task of classroom PBL implementation, working with experienced and novice teachers to create a learning analytics dashboard for a PBL curriculum. We demonstrate researcher and teacher roles and needs in ensuring co‐design collaboration and the co‐construction of actionable insight to support middle school PBL. Implications for practice and/or policy Learning analytics researchers will be able to use the workflow as a tool to support their PBL co‐design processes. Learning analytics researchers will be able to apply adapted HCI methods for effective co‐design processes. Co‐design teams will be able to pre‐emptively prepare for the difficulties and needs of teachers when integrating middle school teacher feedback during the co‐design process in support of PBL technologies.",
    url = "https://doi.org/10.1111/bjet.13363"
}

@article{ref430_912304,
    author = "Farahmand, Arta and Dewan, M. and Lin, Fuhua",
    title = "Student-Facing Educational Dashboard Design for Online Learners",
    year = "2020",
    doi = "10.1109/dasc-picom-cbdcom-cyberscitech49142.2020.00067",
    url = "https://doi.org/10.1109/dasc-picom-cbdcom-cyberscitech49142.2020.00067"
}

@article{ref431_ac12ec,
    author = "Rets, Irina and Herodotou, Christothea and Gillespie, Anna",
    title = "Six Practical Recommendations Enabling Ethical Use of Predictive Learning Analytics in Distance Education",
    year = "2023",
    issn = "1929-7750",
    doi = "10.18608/jla.2023.7743",
    abstract = "The progressive move of higher education institutions (HEIs) towards blended and online environments, accelerated by COVID-19, and their access to a greater variety of student data has heightened the need for ethical learning analytics (LA). This need is particularly salient in light of a lack of comprehensive, evidence-based guidelines on ethics that address gaps voiced in LA ethics research. Studies on the topic are predominantly conceptual, representing mainly institutional rather than stakeholder views, with some areas of ethics remaining underexplored. In this paper, we address this need by using a case of four years of interdisciplinary research in developing the award-winning Early Alerts Indicators (EAI) dashboard at a distance learning university. Through a lens focused on ethical considerations and informed by the practical approach to ethics, we conducted a case study review, using 10 relevant publications that report on the development and implementation of the tool. Our six practical recommendations on how to ethically engage with LA can inform an ethical development of LA that not only protects student privacy, but also ensures that LA tools are used in ways that effectively support student learning and development.",
    url = "https://doi.org/10.18608/jla.2023.7743"
}

@article{ref432_a93c35,
    author = "Hooshyar, Danial and Tammets, Kairit and Ley, Tobias and Aus, Kati and Kollom, Kaire",
    title = "Learning Analytics in Supporting Student Agency: A Systematic Review",
    year = "2023",
    issn = "2071-1050",
    doi = "10.3390/su151813662",
    abstract = "Student agency, or agency for learning, refers to an individual’s ability to act and cause changes during the learning process. Recently, learning analytics (LA) has demonstrated its potential in promoting agency, as it enables students to take an active role in their learning process and supports the development of their self-regulatory skills. Despite the growing interest and potential for supporting student agency, there have yet to be any studies reviewing the extant works dealing with the use of LA in supporting student agency. We systematically reviewed the existing related works in eight major international databases and identified 15 articles. Analysis of these articles revealed that most of the studies aimed to investigate student or educators’ agency experiences, propose design principles for LA, and to a lesser extent, develop LA methods/dashboards to support agency. Of those studies developing LA, none initially explored student agency experiences and then utilized their findings to develop evidence-based LA methods and dashboards for supporting student agency. Moreover, we found that the included articles largely rely on descriptive and diagnostic analytics, paying less attention to predictive analytics and completely overlooking the potential of prescriptive learning analytics in supporting agency. Our findings also shed light on nine key design elements for effective LA support of student agency, including customization, decision-making support, consideration of transparency and privacy, and facilitation of co-design. Surprisingly, we found that no studies have considered the use of LA to support student agency in K–12 education, while higher education has been the focal point of the LA community. Finally, we highlighted the fields of study and data visualization types that the studies mostly targeted and, more importantly, identified eight crucial challenges facing LA in its support of student agency.",
    url = "https://doi.org/10.3390/su151813662"
}

@article{ref433_29beed,
    author = "Pastushenko, Olena",
    title = "Gamification in Assignments",
    year = "2019",
    doi = "10.1145/3341215.3356335",
    url = "https://doi.org/10.1145/3341215.3356335"
}

@article{ref434_2bbed1,
    author = "De Laet, Tinne and Broos, Tom and van Staalduinen, Jan-Paul and Ebner, Martin and Leitner, Philipp",
    title = "Transferring learning dashboards to new contexts: experiences from three case studies",
    year = "2018"
}

@article{ref435_6ebf02,
    author = "Chalvatza, Filothei and Karkalas, Sokratis and Mavrikis, Manolis",
    title = "Communicating Learning Analytics: Stakeholder Participation and Early Stage Requirement Analysis",
    year = "2019",
    doi = "10.5220/0007716503390346",
    abstract = "This paper reflects on a user-centered design methodology for requirements elicitation at early stages of a design process for Learning Analytics tools. This methodology may be used as a domain specific instrument to elicit user perspectives about the communicational aspects of learning analytics dashboards. The focus of this work is identifying ways to communicate the data analysis findings in a way that is easily perceptible and facilitates actionable decision making. We present the structure as well as the logic behind the design of this instrument. As a case study, the paper describes an implementation of this methodology in the context of school-wide analytics communicating to stakeholders quality indicators through summarising and visualising data collected through student and parent surveys. We provide high-level and transferable recommendations derived from the analysis of the workshop with key stakeholders and identify future improvements in our methodology.",
    url = "https://doi.org/10.5220/0007716503390346"
}

@article{ref436_37bc76,
    author = "Aldosemani, Tahani and Al Khateeb, Ahmed",
    title = "Learning Loss Recovery Dashboard: A Proposed Design to Mitigate Learning Loss Post Schools Closure",
    year = "2022",
    issn = "2071-1050",
    doi = "10.3390/su14105944",
    abstract = "Research has shown the effectiveness of designing a Learning Analytics Dashboard (LAD) for learners and instructors, including everyone’s levels of progress and performance. An intertwined relationship exists between learning analytics (LA) and the learning process. Understanding information or data about learners and their learning journey can contribute to a deeper understanding of learners and the learning process. The design of an effective learning dashboard relies heavily on LA, including assessment of the learning process, i.e., gains and losses. A Learning Loss Recovery Dashboard (LLRD) can be designed as an instructional tool, to support the learning process as well as learners’ performance and their academic achievement. The current project proposes a LLRD prototype model to deal with potential learning loss; increase the achievement of learning outcomes; and provide a single, comprehensive learning process, where schools can evaluate and remedy any potential learning loss resulting from the distance-learning period that was caused by the COVID-19 pandemic. This systematic dashboard prototype functions to determine learning gains by K–12 learners. It is expected that the implementation of the proposed dashboard would provide students, teachers, and educational administrators with an integrated portal, for a holistic and unified remedial experience for addressing learning loss.",
    url = "https://doi.org/10.3390/su14105944"
}

@article{ref437_ac5055,
    author = "Thomas, Danielle and Lin, Jionghao and Gatz, Erin and Gurung, Ashish and Gupta, Shivang and Norberg, K. and Fancsali, Stephen and Aleven, Vincent and Branstetter, Lee and Brunskill, Emma and Koedinger, Kenneth",
    title = "Improving Student Learning with Hybrid Human-AI Tutoring: A Three-Study Quasi-Experimental Investigation",
    year = "2024",
    doi = "10.1145/3636555.3636896",
    abstract = "Artificial intelligence (AI) applications to support human tutoring have potential to significantly improve learning outcomes, but engagement issues persist, especially among students from low-income backgrounds. We introduce an AI-assisted tutoring model that combines human and AI tutoring and hypothesize that this synergy will have positive impacts on learning processes. To investigate this hypothesis, we conduct a three-study quasi-experiment across three urban and low-income middle schools: 1) 125 students in a Pennsylvania school; 2) 385 students (50\% Latinx) in a California school; and 3) 75 students (100\% Black) in a Pennsylvania charter school, all implementing analogous tutoring models. We compare learning analytics of students engaged in human-AI tutoring compared to students using math software only. We find human-AI tutoring has positive effects, particularly in student's proficiency and usage, with evidence suggesting lower achieving students may benefit more compared to higher achieving students. We illustrate the use of quasi-experimental methods adapted to the particulars of different schools and data-availability contexts so as to achieve the rapid data-driven iteration needed to guide an inspired creation into effective innovation. Future work focuses on improving the tutor dashboard and optimizing tutor-student ratios, while maintaining annual costs per students of approximately $700 annually.",
    url = "https://doi.org/10.1145/3636555.3636896"
}

@article{ref438_db584a,
    author = "Wright, Mary and Howard, J.",
    title = "Assessment for Improvement: Two Models for Assessing a Large Quantitative Reasoning Requirement",
    year = "2015",
    issn = "1936-4660",
    doi = "10.5038/1936-4660.8.1.6",
    abstract = "We present two models for assessment of a large and diverse quantitative reasoning (QR) requirement at the University of Michigan. These approaches address two key challenges in assessment: (1) dissemination of findings for curricular improvement and (2) resource constraints associated with measurement of large programs. Approaches we present for data collection include convergent validation of self-report surveys, as well as use of mixed methods and learning analytics. Strategies we present for dissemination of findings include meetings with instructors to share data and best practices, sharing of results through social media, and use of easily accessible dashboards. These assessment approaches may be of particular interest to universities with large numbers of students engaging in a QR experience, projects that involve multiple courses with diverse instructional goals, or those who wish to promote evidence-based curricular improvement.",
    url = "https://doi.org/10.5038/1936-4660.8.1.6"
}

@article{ref439_b72239,
    author = "Dawson, Phillip and Apperley, Thomas",
    title = "Workshop 1: Open-Source Learning Analytics and “what the student does”",
    year = "2012"
}

@article{ref440_35b4a5,
    author = "Poquet, Oleksandra",
    title = "A shared lens around sensemaking in learning analytics: What activity theory, definition of a situation and affordances can offer",
    year = "2024",
    issn = "0007-1013",
    doi = "10.1111/bjet.13435",
    abstract = "Abstract The paper argues that learning analytics as a research field can benefit from a theory‐informed shared language to describe sensemaking of learning and teaching data. To make the case for such shared language, first, I critically review prominent sensemaking theories to then demonstrate how studies in learning analytics do not use coherent descriptions of sensemaking, eclectically combining the paradigms that have underlying differences. I then propose a conceptualization of sensemaking that overcomes the differences between these theories and explains how the concepts of activity system , the definition of the situation and affordances can be used to capture individual differences in sensemaking. The paper concludes with a preliminary framework and examples demonstrating its utility in raising new theoretical questions, informing design principles and providing shared language for researchers in learning analytics. Practitioner notes What is already known about this topic Sensemaking happens when individuals try to explain unknown situations. Learning analytics uses sensemaking as a lens to understand dashboard use. Systematic analysis of sensemaking is essential for learning analytics. What this paper adds The paper notes that noticing and perceiving are commonly examined in learning analytics on dashboard use. The paper suggests a revision of fundamental assumptions in sensemaking. A paper proposes a toy model of sensemaking that includes operationalization of the definition of the situation, activity where sensemaking happens and processes of noticing and perceiving affordances. Implications for practice and/or policy Learning analytics must examine sensemaking of data about teaching and learning in a systematic manner. Internal perceptions of the social environment and activity that are informed by the data need to be considered in evaluating dashboard use.",
    url = "https://doi.org/10.1111/bjet.13435"
}

@article{ref441_47d2a0,
    author = "Romero, Yadira and Tame, Hannah and Holzhausen, Ylva and Petzold, Mandy and Wyszynski, Jan-Vincent and Peters, Harm and Alhassan-Altoaama, Mohammed and Domańska, Monika and Dittmar, Martin",
    title = "Design and usability testing of an in-house developed performance feedback tool for medical students",
    year = "2021",
    issn = "1472-6920",
    doi = "10.1186/s12909-021-02788-4",
    abstract = "Abstract Background Feedback is essential in a self-regulated learning environment such as medical education. When feedback channels are widely spread, the need arises for a system of integrating this information in a single platform. This article reports on the design and initial testing of a feedback tool for medical students at Charité-Universitätsmedizin, Berlin, a large teaching hospital. Following a needs analysis, we designed and programmed a feedback tool in a user-centered approach. The resulting interface was evaluated prior to release with usability testing and again post release using quantitative/qualitative questionnaires. Results The tool we created is a browser application for use on desktop or mobile devices. Students log in to see a dashboard of “cards” featuring summaries of assessment results, a portal for the documentation of acquired practical skills, and an overview of their progress along their course. Users see their cohort’s average for each format. Learning analytics rank students’ strengths by subject. The interface is characterized by colourful and simple graphics. In its initial form, the tool has been rated positively overall by students. During testing, the high task completion rate (78\%) and low overall number of non-critical errors indicated good usability, while the quantitative data (system usability scoring) also indicates high ease of use. The source code for the tool is open-source and can be adapted by other medical faculties. Conclusions The results suggest that the implemented tool LevelUp is well-accepted by students. It therefore holds promise for improved, digitalized integrated feedback about students’ learning progress. Our aim is that LevelUp will help medical students to keep track of their study progress and reflect on their skills. Further development will integrate users’ recommendations for additional features as well as optimizing data flow.",
    url = "https://doi.org/10.1186/s12909-021-02788-4"
}

@article{ref442_334bf9,
    author = "Bayrak, Fatma and Yurdugül, Halil",
    title = "WEB-TABANLI ÖZ-DEĞERLENDİRME SİSTEMİNDE ÖĞRENCİ UYARI İNDEKSİNİ TEMEL ALAN ÖĞRENME ANALİTİĞİ MODÜLÜNÜN TASARLANMASI",
    year = "2016",
    issn = "2147-1908",
    doi = "10.17943/etku.59549",
    abstract = "The aim of this study was to design at the micro level a learning analytics module for the prepared web-based self-assessment system that allows learners to test themselves.The most important component of this student-centered web-based self-assessment system is a learning analytics module that is provided by information about the learners' existing situation (feedback) gathered after the learners have tested themselves.In contrast to the item response theory, the SATO student caution index, suggested in order to define learners' status, was taken as a base in the design of this model for classroom assessments with a low number of observations.The student caution index was calculated based on learners' in-class performance and the item difficulty index.Student classification can be made based on the student caution index and correct response rate.The classification includes 6 categories that can determine the effectiveness of the learners' learning, whether or not there were incorrect answers that were the result of carelessness, and what, if any, subjects need further attention.In addition, the way in which these categories were shown on the dashboard will be designed as part of this research.",
    url = "https://doi.org/10.17943/etku.59549"
}

@article{ref443_599de5,
    author = "Adesina, Ayodeji",
    title = "Virtual learning process environment (VLPE): a BPM-based learning process management architecture",
    year = "2013"
}

@article{ref444_b61d5a,
    author = "Pecori, Riccardo and Suraci, Vincenzo and Ducange, Pietro",
    title = "Efficient computation of key performance indicators in a distance learning university",
    year = "2019",
    issn = "2398-6247",
    doi = "10.1108/idd-09-2018-0050",
    url = "https://doi.org/10.1108/idd-09-2018-0050"
}

@article{ref445_3a720f,
    author = "Freitas, Elyda and Batista, Hyan and Barbosa, Gabriel and da Silva Filho, Moésio and Portela, Filipe and Isotani, Seiji and Cordeiro, Thiago and Bittencourt, Ig and Yasojima, Edson and Sobrinho, Álvaro and Pereira, Rodrigo and Mello, Rafael",
    title = "Learning Analytics Desconectada: Um Estudo de Caso em Análise de Produções Textuais",
    year = "2022",
    doi = "10.5753/wapla.2022.226823",
    abstract = "A utilização de Learning Analytics (LA) traz consigo diferentes benefícios às instituições de ensino. Porém, exige recursos computacionais e de internet inacessíveis às populações de baixa renda, tornando esta uma tecnologia que pode gerar desigualdade. Nesse contexto, este artigo tem dois objetivos: (i) apresentar o conceito de LA Desconectada, que permite a aplicação dessa tecnologia em ambientes com recursos limitados; e (ii) apresentar uma aplicação real para correção de produção textual de alunos de escolas públicas brasileiras, compatível com este conceito. O aplicativo permite a correção offline de redações escritas no papel e apresenta um dashboard impresso e com informações sumarizadas aos professores.",
    url = "https://doi.org/10.5753/wapla.2022.226823"
}

@article{ref446_471338,
    author = "Weng, Jian-Xuan and Huang, Anna and Lu, Owen and Chen, Irene and Yang, Stephen",
    title = "The Implementation of Precision Education for Learning Analytics",
    year = "2020",
    doi = "10.1109/tale48869.2020.9368432",
    url = "https://doi.org/10.1109/tale48869.2020.9368432"
}

@article{ref447_5ef940,
    author = "Herodotou, Christothea and Maguire, Claire and Hlosta, Martin and Mulholland, Paul",
    title = "Predictive Learning Analytics and University Teachers: Usage and perceptions three years post implementation",
    year = "2023",
    doi = "10.1145/3576050.3576061",
    abstract = "Predictive learning analytics (PLA) dashboards have been used by teachers to identify students at risk of failing their studies and provide proactive support. Yet, very few of them have been deployed at a large scale or had their use studied at a mature level of implementation. In this study, we surveyed 366 distance learning university teachers across four faculties three years after PLA has been made available across university as business as usual. Informed by the Unified Theory of Acceptance and Use of Technology (UTAUT), we present a context-specific version of UTAUT that reflects teachers' perceptions of PLA in distance learning higher education. The adoption and use of PLA was shown to be positively influenced by less experience in teaching, performance expectancy, self-efficacy, positive attitudes, and low anxiety, while negatively influenced by a lack of facilitating conditions and low effort expectancy, indicating that the type of technology and context within which it is used are significant factors determining our understanding of technology usage and adoption. This study provides significant insights as to how to design, apply and implement PLA with teachers in higher education.",
    url = "https://doi.org/10.1145/3576050.3576061"
}

@article{ref448_71576c,
    author = "De Barba, Paula and Oliveira, Eduardo and Hu, Xinyue",
    title = "Same graph, different data: A usability study of a student-facing dashboard based on self-regulated learning theory",
    year = "2022",
    issn = "2653-665X",
    doi = "10.14742/apubs.2022.168",
    abstract = "Student-facing learning analytics dashboards have the potential to reconnect students with their purpose for learning, reminding them of their goals and promoting reflection about their learning journey. However, far less is known about the specifics of the relationship between different types of visualisations and data presented in dashboards and their impact on students’ motivation. In this study, we used a Human-Centred Design method across three iterations to (1) understand how students prioritise similar visualisations when presenting different data (2) examine how they interact with these, and (3) propose a dashboard design that would accommodate students’ different motivational needs. In the first iteration, 26 participants ranked their preferred visualisations using paper prototypes; in the second iteration, a digital wireframe was created based on the results from the first iteration to conduct user tests with two participants; and in the third iteration, a high-fidelity prototype was created to reflect findings from the previous iterations. Overall, findings showed that students mostly valued setting goals and monitoring their progress from a multiple goals approach, and were reluctant about comparing their performance with peers due to concerns related to promoting unproductive competition amongst peers and data privacy. Implications for educators and learning designers are discussed.",
    url = "https://doi.org/10.14742/apubs.2022.168"
}

@article{ref449_fa3c22,
    author = "Kalir, Jeremiah",
    title = "Designing a Social Learning Analytics Tool for Open Annotation and Collaborative Learning",
    year = "2020",
    doi = "10.35542/osf.io/63ru8",
    abstract = "This book chapter recounts one approach to ethically co-designing a public dashboard that reports social learning analytics and encourages learners’ collaborative annotation across open texts and contexts. As a design narrative in the learning sciences, this chapter is a reflective, first-hand account organized around three related objectives: 1) Naming the theoretical stances toward open and social learning that informed design and research; 2) Describing key decisions and trade-offs pertinent to four iterations of a social learning analytics dashboard; and 3) Considering epistemological, technological, and infrastructural implications for the development and use of social learning analytics in open, flexible, and distance learning.",
    url = "https://doi.org/10.35542/osf.io/63ru8"
}

@article{ref450_21acc9,
    author = "Joy, Van and Peter, Van and Dirk, B and Volha, Petukhova and Jan, Alexandersson",
    title = "Observing, Coaching and Reflecting: A Multi-modal Natural Language-based Dialogue System in a Learning Context",
    year = "2015",
    issn = "1875-4163",
    doi = "10.3233/978-1-61499-530-2-220",
    url = "https://doi.org/10.3233/978-1-61499-530-2-220"
}

@article{ref451_44e944,
    author = "Taibi, Davide and Sándor, Attila and Simsek, Duygu and Shum, Simon and De Liddo, Anna and Ferguson, Rebecca",
    title = "Visualizing the LAK/EDM Literature Using Combined Concept and Rhetorical Sentence Extraction.",
    year = "2013"
}

@article{ref452_ff1732,
    author = "Bull, Susan and Ginon, Blandine and Kay, Judy and Kickmeier-Rust, Michael and Johnson, Matthew",
    title = "LAL workshop",
    year = "2016",
    doi = "10.1145/2883851.2883852",
    url = "https://doi.org/10.1145/2883851.2883852"
}

@article{ref453_0479e4,
    author = "Korir, Maina and Slade, Sharon and Holmes, W. and Rienties, Bart",
    title = "Eliciting students' preferences for the use of their data for learning analytics",
    year = "2022",
    doi = "10.4324/9781003177098-13",
    abstract = "Research on student perspectives of learning analytics suggests that students are generally unaware of the collection and use of their data by their learning institutions, and they are often not involved in decisions about whether and how their data are used. To determine the influence of risks and benefits awareness on students’ data use preferences for learning analytics, we designed two interventions: one describing the possible privacy risks of data use for learning analytics and the second describing the possible benefits. These interventions were distributed amongst 447 participants recruited using a crowdsourcing platform. Participants were randomly assigned to one of three experimental groups – risks, benefits, and risks and benefits – and received the corresponding intervention(s). Participants in the control group received a learning analytics dashboard (as did participants in the experimental conditions). Participants’ indicated the motivation for their data use preferences. Chapter 11 will discuss the implications of our findings in relation to how to better support learning institutions in being more transparent with students about the practice of learning analytics.",
    url = "https://doi.org/10.4324/9781003177098-13"
}

@article{ref454_212807,
    author = "Paredes, Yancy and Siegle, Robert and Hsiao, I‐Han and Craig, Scotty",
    title = "Educational Data Mining and Learning Analytics for Improving Online Learning Environments",
    year = "2020",
    issn = "1071-1813",
    doi = "10.1177/1071181320641113",
    url = "https://doi.org/10.1177/1071181320641113"
}

@article{ref455_00de70,
    author = "Lu, Owen and Huang, Anna and Huang, Jeff and Huang, Chester and Yang, Stephen",
    title = "Early-Stage Engagement: Applying Big Data Analytics on Collaborative Learning Environment for Measuring Learners' Engagement Rate",
    year = "2016",
    doi = "10.1109/eitt.2016.28",
    url = "https://doi.org/10.1109/eitt.2016.28"
}

@article{ref456_17fda2,
    author = "Jaakonmäki, Roope and Drachsler, Hendrik and Kickmeier-Rust, Michael and Dietze, Stefan and Fortenbacher, Albrecht and Marenzi, Ivana",
    title = "Cooking with learning analytics recipes",
    year = "2017",
    doi = "10.1145/3027385.3029465",
    url = "https://doi.org/10.1145/3027385.3029465"
}

@article{ref457_cb70e3,
    author = "Chaudy, Yaëlle and Connolly, Thomas",
    title = "Integrating Assessment, Feedback, and Learning Analytics in Educational Games",
    year = "2018",
    issn = "2327-6983",
    doi = "10.4018/978-1-5225-5936-8.ch006",
    url = "https://doi.org/10.4018/978-1-5225-5936-8.ch006"
}

@article{ref458_27e9a6,
    author = "Park, Sanghoon",
    title = "Analyzing and Comparing Online Learning Experiences through Micro-Level Analytics",
    year = "2015",
    issn = "1941-8027",
    doi = "10.18785/jetde.0802.04",
    abstract = "Learning analytics collects and uses observations of interactions, which allow course instructors to search for the underlying patterns of a student’ learning progress and to accordingly optimize the student’ learning progress at a micro-level. Understanding the online learning experience through the learning analytics approach is essential to inform future pedagogical decisions in online learning design. This paper attempts to define the concept of an online learning experience in three dimensions. In addition, the Experience Sampling Method (ESM) is suggested as a supplement to Web log analysis (WLA) to collect data on cognitive involvement and learning emotion as well as to collect behavioral interaction data. Then, using Clow’s learning analytics cycle as a framework, this paper demonstrates how the identified cognitive, emotional, and behavioral aspects of the online learning experience can be captured and reported in the online learning experience dashboard for each individual student. In addition, the online learning experience data between two courses were compared to find evidence of different learning experiences when courses are designed with different learning tasks. The main finding from this paper is that ESM enables us to capture online learners’ psychological dimensions of learning experiences and provides rich information on each learner’s progress in an online course.",
    url = "https://doi.org/10.18785/jetde.0802.04"
}

@article{ref459_7721ef,
    author = "Alalawi, Khalid and Athauda, Rukshan and Chiong, Raymond",
    title = "An Innovative Framework to Improve Course and Student Outcomes",
    year = "2021",
    doi = "10.1109/citisia53721.2021.9719985",
    url = "https://doi.org/10.1109/citisia53721.2021.9719985"
}

@article{ref460_b63eab,
    author = "Centenaro, Belisa",
    title = "Comparação entre a utilização de diferentes conjuntos de dados para a modelagem e identificação de acadêmicos em risco: Desenvolvimento de uma Dashboard de Learning Analytics e estudo bibliográfico de ferramentas análogas.",
    year = "2020"
}

@article{ref461_f1dc0c,
    author = "Tran, Tich and Jan, Tony and Kew, Si",
    title = "Learning Analytics for Improved Course Delivery: Applications and Techniques",
    year = "2022",
    doi = "10.1145/3568739.3568758",
    url = "https://doi.org/10.1145/3568739.3568758"
}

@article{ref462_ffa76f,
    author = "Sun, Bo and Lai, Song and Xu, Congcong and Xiao, Rong and Wei, Yungang and Xiao, Yongkang",
    title = "Differences of online learning behaviors and eye-movement between students having different personality traits",
    year = "2017",
    doi = "10.1145/3139513.3139527",
    url = "https://doi.org/10.1145/3139513.3139527"
}

@article{ref463_4cd9d7,
    author = "Cuadros, Jordi and Serrano, Vanessa and Lluch, Francesc and Garcia-Zúbia, Javier and Hernández‐Jayo, Unai",
    title = "Mapping VISIR Circuits for Computer-assisted Assessment",
    year = "2021",
    doi = "10.1109/weef/gedc53299.2021.9657349",
    url = "https://doi.org/10.1109/weef/gedc53299.2021.9657349"
}

@article{ref464_d9037a,
    author = "Campos, Fabio and Nguyen, Ha and Ahn, June and Jackson, Kara",
    title = "Leveraging cultural forms in human‐centred learning analytics design",
    year = "2023",
    issn = "0007-1013",
    doi = "10.1111/bjet.13384",
    abstract = "Abstract In this article, we offer theory‐grounded narratives of a 4‐year participatory design process of a Learning Analytics tool with K‐12 educators. We describe how we design‐in‐partnership by leveraging educators' routines, values and cultural representations into the designs of digital dashboards. We make our long‐term reasoning visible by reflecting upon how design decisions were made, discussing key tensions and analysing to what extent the developed tools were taken up in practice. Through thick design narratives, we reflect upon how cultural forms—recognizable cultural constructs that might cue and facilitate specific activities—were identified among educators and informed the design of a dashboard. We then examined the extent to which the designed tool supported coaches and teachers to engage in Generative Uncertainty, an interpretive stance in which educators manifest productive inquiries towards data. Our analysis highlights that attuning to cultural forms is a valuable first step but not enough towards designing LA tools for systems in ways that fit institutionalized practices, challenge instrumental uses and spur productive inquiry. We conclude by offering two key criteria for making culturally‐grounded design decisions in the context of long‐term partnerships. Practitioner notes What is already known about this topic Participatory design can invite stakeholders to directly inform the creation of LA artefacts that fit their needs, context and cultural markers. What this paper adds Cultural forms can be identified and leveraged in the design of LA tools. HCLA scholars ought to design for systems —the complex body of organizational routines, cultural practices and interactions among multiple stakeholders—and not just for users . Implications for practice and/or policy Leveraging cultural forms in LA needs to be accompanied by a critical view of which practices, behaviours, values and structures are suggested by such forms. Designing features that are easy to use, are associated with concrete tasks, and fit into existing cultural practices are three criteria for embedding cultural forms into LA design.",
    url = "https://doi.org/10.1111/bjet.13384"
}

@article{ref465_f1d452,
    author = "Liang, Changhao and Botički, Ivica and Ogata, Hiroaki",
    title = "Supporting Teachers in Group Work Formation and Analytics for In-class Group Activities",
    year = "2019",
    doi = "10.58459/icce.2019.654",
    url = "https://doi.org/10.58459/icce.2019.654"
}

@article{ref466_6d7110,
    author = "Zapparolli, Luciana and Stiubiener, Itana",
    title = "FAG – a management support tool with BI techniques to assist teachers in the virtual learning environment Moodle",
    year = "2017",
    issn = "2415-6698",
    doi = "10.25046/aj020375",
    abstract = "One of the great challenges in distance-learning is to follow the actions of the teachers/tutors and also the actions of students during the process of teaching and learning.This article presents the FAG Tool integrated with the LMS Moodle was developed to help managers of the Distance Education environment to monitor the actions of teachers/tutors and also teachers/tutors in the follow-up of student actions.Through the techniques of Business Intelligence (BI) and Learning Analytics (LA), the tool generates analytical reports and dashboards, presenting a holistic and transversal view, being this vision the differential of this tool.The use of FAG allows teachers/tutors to monitor the participation of all their students in all virtual rooms under their responsibility and thus take corrective measures in the teaching and learning process, such as reducing the risk of avoidance.For the managers, it can be considered as a support tool for decision making regarding the faculty, maintaining or not the teacher/tutor in the process of teaching and learning or even be a base to enlarge or reduce their classes depending on their performance in the virtual environment.Through the use of the FAG, this decision-making can happen during the teaching and learning process and not only after the end, as is usual, because the reports are easy to understand and present accurate information in time to ensure the success of the teaching and learning process.",
    url = "https://doi.org/10.25046/aj020375"
}

@article{ref467_f91e1e,
    author = "Youngs, Bonnie and Prakash, Akhil and Nugent, Rebecca",
    title = "Statistically-driven visualizations of student interactions with a French online course video",
    year = "2017",
    issn = "0958-8221",
    doi = "10.1080/09588221.2017.1367311",
    url = "https://doi.org/10.1080/09588221.2017.1367311"
}

@article{ref468_40ed25,
    author = "Daniel, Ben",
    title = "Improving the Pedagogy of Research Methodology through Learning Analytics",
    year = "2019"
}

@article{ref469_5b65b8,
    author = "Jayashanka, Rangana and Hewagamage, K. and Hettiarachchi, Enosha",
    title = "An Intelligent Interactive Visualizer to Improve Blended Learning in Higher Education",
    year = "2019",
    doi = "10.1109/ubi-media.2019.00022",
    url = "https://doi.org/10.1109/ubi-media.2019.00022"
}

@article{ref470_8769b3,
    author = "Rodríguez, Manuel",
    title = "Making Teaching and Learning Visible",
    year = "2019",
    doi = "10.1145/3362789.3362839",
    url = "https://doi.org/10.1145/3362789.3362839"
}

@article{ref471_183854,
    author = "Zapparolli, Luciana and Stiubiener, Itana",
    title = "Management support tool in virtual learning environments using moodle as a case study",
    year = "2016",
    doi = "10.1109/laclo.2016.7751756",
    url = "https://doi.org/10.1109/laclo.2016.7751756"
}

@article{ref472_dba98c,
    author = "Abed, Mustafa and Singh, Dalbir",
    title = "Clickstream Data Schema for Learning Analytics to Understand Learner Behaviour",
    year = "2020",
    issn = "2278-3091",
    doi = "10.30534/ijatcse/2020/49952020",
    abstract = "Learning analytics has caught numerous researchers' attention in the last decade due to its ability to analyze, report, and process information produced by learners.Several research studies have examined learning analytics in terms of learner behavior to provide quality and efficient reporting for educators and higher learning institutions.In particular, the click pattern of the learner has been examined in terms of its significant impact on determining learner performance.Yet, the clickstream data schema that has been addressed in the literature was limited to click patterns focusing on event-based click patterns and time in the logging history.However, learner interaction logging history available in clickstream data schema for major online learning management systems holds great potential to reveal diverse and complex learner behavior patterns based on various significant learner behavior elements that are waiting to be discovered.The literature has shown that great interest has been given in understanding specific behavior produced by the learner through an online learning management system.Thus, this study aims to review previous related studies that could reveal possible learner click patterns, which could be considered as an indicator to predict the learner's performance and behavior.Potential learner interaction data that could be applied to produce a clickstream data stream has been outlined.As an implication, the contribution of the study could open up new frontiers that trigger the development of a learning analytics dashboard for various online learning platforms.",
    url = "https://doi.org/10.30534/ijatcse/2020/49952020"
}

@article{ref473_bbc4c0,
    author = "Divjak, Blaženka and Vondra, Petra and Aničić, Katarina",
    title = "Strategic Development of a National Pre-tertiary Learning Analytics System",
    year = "2022",
    issn = "1846-3312",
    doi = "10.31341/jios.46.1.10",
    abstract = "This paper elaborates the design of a National Learning Analytics (LA) and Educational Data Mining System for pre-tertiary education in Croatia. The described approach consists of the following five phases: 1) objectives setting, 2) user needs analysis, 3) data availability analysis, 4) dashboards pre-design and 5) validation of functionalities. There is an evident research gap, as well as the lack of practical examples about the development of a Learning Analytics System (LAS) for pre-tertiary education on the national and/or regional level. Therefore, this research aims to make a scientific contribution by filling the recognized gap, but also contribute to solving the practical issues of national schools’ LAS development. To increase the usability of an LAS, the involvement of all end users is essential. In the described case, six main user groups were identified: 10-18 years old students, teachers, school management and support staff, regional and national authorities responsible for education, strategic bodies and researchers and, finally, project partners that work on system development. For each of them, separate dashboard functionalities have been designed through several rounds of consultations. Consultations with users started with focus groups and panels, to brainstorm the most important issues they would like to answer in order to enhance learning and teaching. Between the two rounds of consultation, there was an evaluation phase of the relevance of the gathered questions. Finally, targeted users provided feedback on the pre-production functionalities of each dashboard and validated them. In this process, several challenges were detected, including data gathering and protection, ethical issues and interpretation of results for students who are underage, and continuous adjustments to the users’ needs.",
    url = "https://doi.org/10.31341/jios.46.1.10"
}

@article{ref474_3e8618,
    author = "Pal, Neeti and Dahiya, Omdev",
    title = "Role of Learning Management System for Evaluating Students’ progress in Learning Environment",
    year = "2022",
    doi = "10.1109/ic3i56241.2022.10072794",
    url = "https://doi.org/10.1109/ic3i56241.2022.10072794"
}

@article{ref475_68937f,
    author = "Poellhuber, Louis-Vincent and Poëllhuber, Bruno and Desmarais, Michel and Léger, Christian and Roy, Normand and Vu, Manh",
    title = "Cluster-Based Performance of Student Dropout Prediction as a Solution for Large Scale Models in a Moodle LMS",
    year = "2023",
    doi = "10.1145/3576050.3576146",
    url = "https://doi.org/10.1145/3576050.3576146"
}

@article{ref476_626a33,
    author = "Nuninger, Walter and Picardi, Claudia and Goy, A. and Petrone, Giovanna",
    title = "Multi-Perspective Concept Mapping in a Digital Integrated Learning Environment",
    year = "2018",
    issn = "2326-8905",
    doi = "10.4018/978-1-5225-6361-7.ch007",
    url = "https://doi.org/10.4018/978-1-5225-6361-7.ch007"
}

@article{ref477_c44b91,
    author = "Herodotou, Christothea and Boroowa, Avinash and Hlosta, Martin and Rienties, Bart",
    title = "What Do Distance Learning Students Need From Student Analytics",
    year = "2020",
    doi = "10.22318/icls2020.1737",
    url = "https://doi.org/10.22318/icls2020.1737"
}

@article{ref478_ca2edf,
    author = "Simpson, Deborah and Marcdante, Karen and Souza, Kevin and Anderson, Andy and Holmboe, Eric",
    title = "Job Roles of the 2025 Medical Educator",
    year = "2018",
    issn = "1949-8357",
    doi = "10.4300/jgme-d-18-00253.1",
    abstract = {Medical educators recognize that physicians' roles are rapidly changing. The Accreditation Council for Graduate Medical Education's (ACGME's) Sponsoring Institution 2025 (SI2025) initiative identified 3 major driving forces in health care and graduate medical education: democratization, commoditization, and corporatization.1 Wartman and Combs argued that, as the practice of medicine transforms from the information age to the age of artificial intelligence, the medical community must accept that "devices will, on an increasing scale, outperform humans, cognitively and physically."2 As educators, we seek to understand these changes and design education to be consistent with the roles of physicians in this future system consistent with a true competency-based approach to education.3 Job analyses reveal that physicians in 2020 must be competent health care clinicians for patients and populations, superb communicators, fluent with digital data and technology, agile and innovation-driven, and capable as leaders and members of interprofessional teams.2,4Education, like health care, is aggressively changing to include anytime and anywhere adaptive strategies driven by learning analytics, virtual and augmented reality, gamification, and mobile/wearable technologies. Yet, despite the multitude of calls for medical education reform, including the recommendations arising from SI2025, limited attention is focused on what this means for the medical educators who will design, deliver, and assess learners and evaluate our educational programs in 2025. The SI2025 Task Force's competence No. 27 (Accountability for Faculty with Clinical and Educational Responsibilities) focuses on who will be responsible for the development of physicians in these areas, with shifts from medical schools to health care organizations, yet the specific skills and roles of educators in 2025 were not addressed.1 In addition, while medical educator colleagues have defined competencies for teaching5 and for clinician educators,6 a new lens must be added to account for the transformations occurring in education.To identify the future roles of medical educators, we led an "Educators of the Future—2025 Job Roles" session at the Association of American Medical Colleges (AAMC) 2017 Learn, Serve, Lead meeting. The 90-minute interactive session used the futurist concept of hard trends7 (measurable, predictable facts about transformations in education) and began with a rapidly playing set of screen shots and images of current education trends. Then, the session organizers provided provocative, hard trend–based perspectives on the future of medical education and the roles of the medical educator.The AAMC Annual Meeting brings together a diverse group of medical education stakeholders—clinical and educational leaders, teachers, learners—of various ages, geographic locations, and expertise in education. To take advantage of this diversity of perspectives, the session used small group discussions using experienced medical educators as facilitators. Each facilitator received a preparatory packet in advance. The groups were asked to consider the identified hard trends and to generate key job elements or features of the 2025 Medical Educator. Facilitators reported their group's results with panelists identifying cross-cutting themes to represent the input from 95 participants. Reports and discussions were audiotaped and transcribed for analysis by the authors. Job roles were then sent to facilitators who made clarifying revisions and affirmed the results.There was general agreement that these are new job roles in response to the changing landscape of health care and medical education. As we transition to these new roles, specialized training will be required, while jobs that exclusively emphasize subject matter expertise will decline. Groups of participants working independently converged on 6 common job roles. While it is unlikely that everyone will have the same degree of competence in each role, every 2025 medical educator will be expected to have basic competence in all 6 roles, which are shown in the box and described below.The use of big data in education will continue to grow. This requires educators to identify performance gaps for individuals and groups in order to personalize educational experiences (including competency-based and time-variable training), tailor performance assessments, and evaluate curricula.16 As diagnostic assessors, educators must be skilled at translating learning and predictive analytic results5 to actions that optimize learning and performance for individuals, cohorts, groups, and populations.High-quality content, developed by national experts, is increasingly available across a number of professional organizations and societies, textbook companies, and vendors who contract with experts in the field. Accessing, selecting, sequencing, delivering, and sharing these materials with learners to meet local needs, in real time at the point-of-care, already occurs in several specialties. As curators of content, educators must be skilled in selecting content materials from existing educational materials, and building alliances across stakeholders, including faculty, specialties, professions, accrediting bodies, and professional and interprofessional societies.Since release of the first smart phone in 1992, its effects and that of other technologies have transformed our personal and professional lives. This technological progress will continue at an exponential rate and presents an opportunity, not a threat, to enhance our work.17 Medical educators in 2025 must be early adopters, fluent in selecting, using, and assessing the appropriate technology tools. These range from an app to a virtual reality or augmented reality immersion activity. In addition, educators will need to recognize when technology use is misguided or fails.Moving from the diagnostic assessment of individual learner performance dashboards to advancing learners' growth and development will require a skilled educational navigator. Medical educators, as learner-centered navigators, will guide the use of resources, materials, and practice opportunities to achieve identified performance targets. As professional coaches, educators must be skilled in face-to-face and virtual facilitation, to provide personalized and group coaching sessions that support learners' accurate construction of meaning.Role modeling is teaching by example.18 Medical educators in 2025 must be the exemplars for competence in the various 2025 physician roles. These include individual care provider and leader/member of interprofessional teams, with superb communication skills and professionalism.4 Additional role-modeling elements will focus on demonstrating humanism,19 attention to personal well-being,20 and integrated systems thinking with cross-cutting foundations (eg, quality, patient safety, and lean approaches) as part of clinical and educator competence.2In 2025, medical educators will be designers of the learning environment. Think of this role as an architect or engineer who designs the "space" to optimize learning. Educators will need to draw on the sciences of human learning, cognition, memory, and implementation to inform their designs. Will the learning environment be a 2025 version of a Google Hangout, a team activity in an augmented reality space with interprofessional trainees, individualized adaptive practice exercises, or a rapid time-lapse quality improvement simulation? Key to this medical educator role is the underlying foundation in learning and implementation sciences.As education evolves, medical educators must embrace these role changes and a new professional identity. As noted by Catherine Lucey in her keynote address at the 2017 meeting of the Association for Medical Education in Europe in Helsinki, Finland, "The value of a faculty member can no longer be linked to superior knowledge or skills in all things health care and education."21 We share Lucey's vision that educators' value will lie in their "wisdom, structured approach to a problem, ability to model ongoing learning, and in their ability to create an environment where every learner is valued and supported to achieve their best."21As good educators, we hope we have left you with more questions than answers: How will we learn these skills? How will we be compensated and rewarded for these new roles? What will be the optimal designs to maximize and streamline learning at minimal costs? As the first to outline the roles of medical educators in 2025, using hard trends to trigger discussion at an international medical education conference, we acknowledge that these are projected roles. We are however certain about 1 thing: the job of the medical educator in 2025 will require new skills, redevelopment and expansion of old skills, and the same commitment to graduating physicians who we will be proud to have care for a loved one.},
    url = "https://doi.org/10.4300/jgme-d-18-00253.1"
}

@article{ref479_1cc2f0,
    author = "Wiley, Korah and Gerard, Libby and Bradford, Allison and Linn, Marcia",
    title = "Teaching with Technology",
    year = "2023",
    doi = "10.1093/oxfordhb/9780199841332.013.52",
    url = "https://doi.org/10.1093/oxfordhb/9780199841332.013.52"
}

@article{ref480_1119a1,
    author = "Chong, Sylvia and Lee, Yew and Tang, Yoke",
    title = "Data Analytics and Visualization to Support the Adult Learner in Higher Education",
    year = "2020",
    doi = "10.1145/3421682.3421698",
    url = "https://doi.org/10.1145/3421682.3421698"
}

@article{ref481_57db77,
    author = "Gauthier, A. and Rizvi, Saman and Cukurova, Mutlu and Mavrikis, Manolis",
    title = "Is it time we get real? A systematic review of the potential of data-driven technologies to address teachers' implicit biases",
    year = "2022",
    issn = "2624-8212",
    doi = "10.3389/frai.2022.994967",
    abstract = "Data-driven technologies for education, such as artificial intelligence in education (AIEd) systems, learning analytics dashboards, open learner models, and other applications, are often created with an aspiration to help teachers make better, evidence-informed decisions in the classroom. Addressing gender, racial, and other biases inherent to data and algorithms in such applications is seen as a way to increase the responsibility of these systems and has been the focus of much of the research in the field, including systematic reviews. However, implicit biases can also be held by teachers. To the best of our knowledge, this systematic literature review is the first of its kind to investigate what kinds of teacher biases have been impacted by data-driven technologies, how or if these technologies were designed to challenge these biases, and which strategies were most effective at promoting equitable teaching behaviors and decision making. Following PRISMA guidelines, a search of five databases returned n = 359 records of which only n = 2 studies by a single research team were identified as relevant. The findings show that there is minimal evidence that data-driven technologies have been evaluated in their capacity for supporting teachers to make less biased decisions or promote equitable teaching behaviors, even though this capacity is often used as one of the core arguments for the use of data-driven technologies in education. By examining these two studies in conjunction with related studies that did not meet the eligibility criteria during the full-text review, we reveal the approaches that could play an effective role in mitigating teachers' biases, as well as ones that may perpetuate biases. We conclude by summarizing directions for future research that should seek to directly confront teachers' biases through explicit design strategies within teacher tools, to ensure that the impact of biases of both technology (including data, algorithms, models etc.) and teachers are minimized. We propose an extended framework to support future research and design in this area, through motivational, cognitive, and technological debiasing strategies.",
    url = "https://doi.org/10.3389/frai.2022.994967"
}

@article{ref482_03444e,
    author = "Klüsener, Marcus and Konitzer, Wojciech and Fortenbacher, Albrecht",
    title = "Interaktive Visualisierung zur Darstellung und Bewertung von Learning-Analytics-Ergebnissen in Foren mit vielen Teilnehmern",
    year = "2015"
}

@article{ref483_9dd8c4,
    author = "Berendt, Bettina and Vuorikari, Riina and Littlejohn, Allison and Margaryan, Anoush",
    title = "Learning Analytics and their Application in Technology-enhanced Professional Learning",
    year = "2013",
    doi = "10.4324/9780203745052-23",
    url = "https://doi.org/10.4324/9780203745052-23"
}

@article{ref484_685d70,
    author = "Santos, Jose and Pimentel, Edson and Dotta, Sílvia and Botelho, Wagner",
    title = "Estudo comparativo de plugins Moodle para Análise e Acompanhamento da Aprendizagem",
    year = "2019",
    doi = "10.5753/cbie.sbie.2019.189",
    abstract = "A aprendizagem é um produto de interação, que dependendo da epistemologia de seu processo, faz com que cada aprendiz interaja com professores, com outros estudantes e com o conteúdo. Em um Ambiente Virtual de Aprendizagem (AVA) é possível a realização de coletas de dados sobre as interações dos aprendizes, mas a forma bruta desses dados é de difícil interpretação pelos envolvidos no processo de aprendizagem. Aplicações de Learning Analytics Dashboards (LAD), têm sido desenvolvidas para apoiar a análise da aprendizagem. No entanto, há poucos relatos do uso dessas ferramentas no ambiente virtual Moodle, bastante utilizado por educadores. O problema que se levanta a partir dessa observação é se isso é decorrente da ausência desses ferramentas (plugins) para o Moodle, do seu desconhecimento ou funcionalidades. Este artigo tem por objetivo apresentar um estudo comparativo de plugins do tipo LAD para o ambiente Moodle relacionados destacando suas funcionalidades para o acompanhamento do processo de aprendizagem pelo professor e pelo estudante.",
    url = "https://doi.org/10.5753/cbie.sbie.2019.189"
}

@article{ref485_c23b9c,
    author = "Mohseni, Zeynab and Martins, Rafael and Masiello, Italo",
    title = "SBGTool v2.0: An Empirical Study on a Similarity-Based Grouping Tool for Students’ Learning Outcomes",
    year = "2022",
    issn = "2306-5729",
    doi = "10.3390/data7070098",
    abstract = "Visual learning analytics (VLA) tools and technologies enable the meaningful exchange of information between educational data and teachers. This allows teachers to create meaningful groups of students based on possible collaboration and productive discussions. VLA tools also allow a better understanding of students’ educational demands. Finding similar samples in huge educational datasets, however, involves the use of effective similarity measures that represent the teacher’s purpose. In this study, we conducted a user study and improved our web-based similarity-based grouping VLA tool, (SBGTool) to help teachers categorize students into groups based on their similar learning outcomes and activities. SBGTool v2.0 differs from SBGTool due to design changes made in response to teacher suggestions, the addition of sorting options to the dashboard table, the addition of a dropdown component to group the students into classrooms, and improvement in some visualizations. To counteract color blindness, we have also considered a number of color palettes. By applying SBGTool v2.0, teachers may compare the outcomes of individual students inside a classroom, determine which subjects are the most and least difficult over the period of a week or an academic year, identify the numbers of correct and incorrect responses for the most difficult and easiest subjects, categorize students into various groups based on their learning outcomes, discover the week with the most interactions for examining students’ engagement, and find the relationship between students’ activity and study success. We used 10,000 random samples from the EdNet dataset, a large-scale hierarchical educational dataset consisting of student–system interactions from multiple platforms at the university level, collected over a two-year period, to illustrate the tool’s efficacy. Finally, we provide the outcomes of the user study that evaluated the tool’s effectiveness. The results revealed that even with limited training, the participants were able to complete the required analysis tasks. Additionally, the participants’ feedback showed that the SBGTool v2.0 gained a good level of support for the given tasks, and it had the potential to assist teachers in enhancing collaborative learning in their classrooms.",
    url = "https://doi.org/10.3390/data7070098"
}

@article{ref486_3441bd,
    author = "Heredia-Jiménez, Vanessa and Yaguana, Jhony and Jiménez, Alberto and Ortiz‐Rojas, Margarita",
    title = "Using Design-Based Research for an Academic Dropout and Retention Dashboard",
    year = "2023",
    doi = "10.1109/icedeg58167.2023.10122065",
    url = "https://doi.org/10.1109/icedeg58167.2023.10122065"
}

@article{ref487_c9d7c3,
    author = "Costa, Laécio and da Silveira, Aleph and Souza, Marlo and Salvador, Laís and Santos, Celso",
    title = "Investigating Student and Teacher Perceptions in e-Learning with Learning Analytics and Ontologies",
    year = "2023",
    issn = "1863-0383",
    doi = "10.3991/ijet.v18i08.32411",
    abstract = "This work is an approach that brings together Learning Analytics and Ontologies for a data classification that promotes improvements and behavioral changes for students and teachers on e-Learning platforms. Combining training courses, dashboards, user's evaluations, and based on Design Science Research (DSR) methodology, artifacts were created. One of the most important artifacts of our work is the Sapes tool that aims to improve students’ perceptions of their learning path and to promote a better teacher overview to follow their students' progress. The results showed high approval by the participating students and teachers, who perceived the Sapes tool as a good facilitator of the teaching-learning process, with possibilities for self-monitoring, dynamization of the learning sequence and better interactivity with colleagues, highlighted as absent in standard e-Learning courses. In addition, the application changed the behavior of users towards the content provided by the teacher, with students performing self-management and self-regulation that were not commonly performed previously.",
    url = "https://doi.org/10.3991/ijet.v18i08.32411"
}

@article{ref488_b4d47d,
    author = "Prescott, David",
    title = "Situated learning, pedagogic models and structured tasks in blended course delivery",
    year = "2016",
    doi = "10.4324/9781315621586-10",
    url = "https://doi.org/10.4324/9781315621586-10"
}

@article{ref489_daa50e,
    author = "Viberg, Olga and Kizilcec, René and Wise, Alyssa and Jivet, Ioana and Nixon, Nia",
    title = "Advancing equity and inclusion in educational practices with <scp>AI</scp>‐powered educational decision support systems (<scp>AI</scp>‐<scp>EDSS</scp>)",
    year = "2024",
    issn = "0007-1013",
    doi = "10.1111/bjet.13507",
    url = "https://doi.org/10.1111/bjet.13507"
}

@article{ref490_c3db19,
    author = "White, Mathew and Dave, Kashmira and Huijser, Henk",
    title = "The missing link in Learning Analytics",
    year = "2023",
    issn = "2653-665X",
    doi = "10.14742/apubs.2023.630",
    abstract = "Learning Analytics and staff-facing dashboards enable educators to gain insights into student learning, enhancing teaching practices and learning outcomes. Student-facing dashboards (SFDs) can also prove advantageous for students as they empower them to better interpret their results, promote self-reflection with the goal of improving their academic performance. Whilst there is no currently preferred software solution for implementing dashboards. Through the authors’ reflections of creating SFDs, using assessment and ePortfolio asset data, this can be achieved with free or readily available tools. This concise paper used initiatives for the author to reflect upon the process creating SFDs. Investigations for opportunities to refine dashboards are also presented.",
    url = "https://doi.org/10.14742/apubs.2023.630"
}

@article{ref491_a4789a,
    author = "Kim, Jeonghyun and Park, Yeonjeong and Huh, Dami and Jo, Il‐Hyun",
    title = "Interaction of Learning Motivation with Dashboard Intervention and Its Effect on Learning Achievement",
    year = "2017"
}

@article{ref492_974d9e,
    author = "Ifenthaler, Dirk and Yau, Jane",
    title = "Analytics for Supporting Teaching Success in Higher Education: A Systematic Review",
    year = "2022",
    doi = "10.1109/educon52537.2022.9766734",
    url = "https://doi.org/10.1109/educon52537.2022.9766734"
}

@article{ref493_17f01a,
    author = "Montuori, Lina and Alcázar-Ortega, Manuel and Vargas‐Salgado, Carlos and Alfonso-Solar, David",
    title = "Learning Analytics as Data driven decision making in High Education: a case study",
    year = "2023",
    doi = "10.4995/inn2022.2022.15750",
    abstract = "This work leads with the application of Learning Analytics (LA) methods in high education as data driven for making decision, where a real case study is presented. The LA method has been applied to a course on HVAC Facilities and Energetic Certification of the Master in Industrial Engineering at the Polytechnic University of Valencia (UPV), Spain. Thus, data collected during the whole academic year 2020-2021 by the institutional LMS platform of UPV have been analyzed. Results show that LA can be a successful data driven for detecting the students at risk, boosting their retention, personalizing contents and improving students learning. Finally, a tailored dashboard is proposed for students’ improvement monitoring.",
    url = "https://doi.org/10.4995/inn2022.2022.15750"
}

@article{ref494_47d9f4,
    author = "Tackett, Sean and Green, David and Dyal, Michael and O'Keefe, Erin and Thomas, Tanya and Nguyen, Tiffany and Vo, Duyen and Patel, Mausam and Murdock, Christopher and Wolfe, Erin and Shehadeh, Lina",
    title = "Use of Commercially Produced Medical Education Videos in a Cardiovascular Curriculum: Multiple Cohort Study (Preprint)",
    year = "2021",
    doi = "10.2196/preprints.27441",
    abstract = "<sec> <title>BACKGROUND</title> Short instructional videos can make learning more efficient through the application of multimedia principles, and video animations can illustrate the complex concepts and dynamic processes that are common in health sciences education. Commercially produced videos are commonly used by medical students but are rarely integrated into curricula. </sec> <sec> <title>OBJECTIVE</title> Our goal was to examine student engagement with medical education videos incorporated into a preclinical Cardiovascular Systems course. </sec> <sec> <title>METHODS</title> Students who took the first-year 8-week Cardiovascular Systems course in 2019 and 2020 were included in the study. Videos from Osmosis were recommended to be watched before live sessions throughout the course. Video use was monitored through dashboards, and course credit was given for watching videos. All students were emailed electronic surveys after the final exam asking about the course’s blended learning experience and use of videos. Osmosis usage data for number of video views, multiple choice questions, and flashcards were extracted from Osmosis dashboards. </sec> <sec> <title>RESULTS</title> Overall, 232/359 (64.6\%) students completed surveys, with rates by class of 81/154 (52.6\%) for MD Class of 2022, 39/50 (78\%) for MD/MPH Class of 2022, and 112/155 (72.3\%) for MD Class of 2023. Osmosis dashboard data were available for all 359 students. All students received the full credit offered for Osmosis engagement, and learning analytics demonstrated regular usage of videos and other digital platform features. Survey responses indicated that most students found Osmosis videos to be helpful for learning (204/232, 87.9\%; \&lt;i\&gt;P\&lt;/i\&gt;=.001) and preferred Osmosis videos to the traditional lecture format (134/232, 57.8\%; \&lt;i\&gt;P\&lt;/i\&gt;\&amp;lt;.001). </sec> <sec> <title>CONCLUSIONS</title> Commercial medical education videos may enhance curriculum with low faculty effort and improve students’ learning experiences. Findings from our experience at one medical school can guide the effective use of supplemental digital resources for learning, and related evaluation and research. </sec>",
    url = "https://doi.org/10.2196/preprints.27441"
}

@article{ref495_8274fd,
    author = "Sadallah, Madjid and Gilliot, Jean-Marie",
    title = "Generating LADs that Make Sense",
    year = "2023",
    doi = "10.5220/0011839800003470",
    abstract = "Learning Analytics Dashboards (LADs) deliver rich and actionable representations of learning data to support meaningful and insightful decisions that ultimately leverage the learning process.Yet, because of their limited adoption and the complex nature of learning data, their design is still a major area of inquiry.In this paper, we propose to expand LAD codesign approaches.We first investigate how the user makes sense of the data delivered by LADs and how to support this sensemaking process at design.Second, we propose a generative tool, supporting sensemaking and decision making process, that extends end-users participation during the prototyping phase and empowers LAD designers.We also present an evaluation of the tool, including usability and user experience, demonstrating its effectiveness in supporting the design and prototyping of LADs.",
    url = "https://doi.org/10.5220/0011839800003470"
}

@article{ref496_caf074,
    author = "Singh, Manjeet and Bangay, Shaun and Sajjanhar, Atul",
    title = "An Architecture for Capturing and Presenting Learning Outcomes using Augmented Reality Enhanced Analytics",
    year = "2022",
    doi = "10.1109/ismar-adjunct57072.2022.00126",
    url = "https://doi.org/10.1109/ismar-adjunct57072.2022.00126"
}

@article{ref497_d65e96,
    author = "Bahari, Mahadi and Arpacı, İbrahim and Azmi, Nurulhuda and Shuib, Liyana",
    title = "Predicting the Intention to Use Learning Analytics for Academic Advising in Higher Education",
    year = "2023",
    issn = "2071-1050",
    doi = "10.3390/su152115190",
    abstract = "Learning analytics (LA) is a rapidly growing educational technology with the potential to enhance teaching methods and boost student learning and achievement. Despite its potential, the adoption of LA remains limited within the education ecosystem, and users who do employ LA often struggle to engage with it effectively. As a result, this study developed and assessed a model for users’ intention to utilize LA dashboards. The model incorporates constructs from the “Unified Theory of Acceptance and Use of Technology”, supplemented with elements of personal innovativeness, information quality, and system quality. The study utilized exploratory research methodology and employed purposive sampling. Participants with prior experience in LA technologies were selected to take part in the study. Data were collected from 209 academic staff and university students in Malaysia (59.33\% male) from four top Malaysian universities using various social networking platforms. The research employed “Partial Least Squares Structural Equation Modeling” to explore the interrelationships among the constructs within the model. The results revealed that information quality, social influence, performance expectancy, and system quality all positively impacted the intention to use LA. Additionally, personal innovativeness exhibited both direct and indirect positive impacts on the intention to use LA, mediated by performance expectancy. This study has the potential to offer valuable insights to educational institutions, policymakers, and service providers, assisting in the enhancement of LA adoption and usage. This study’s contributions extend beyond the present research and have the potential to positively impact the field of educational technology, paving the way for improved educational practices and outcomes through the thoughtful integration of LA tools. The incorporation of sustainability principles in the development and deployment of LA tools can significantly heighten their effectiveness, drive user adoption, and ultimately nurture sustainable educational practices and outcomes.",
    url = "https://doi.org/10.3390/su152115190"
}

@article{ref498_849893,
    author = "Villagrán, Ignacio and Hernández, Rocío and Schuit, Gregory and Neyem, Andrés and Fuentes, Javiera and Larrondo, Loreto and Margozzini, Elisa and Hurtado, María and Iriarte, Zoe and Miranda, Constanza and Varas, Julián and Hilliger, Isabel",
    title = "Enhancing Feedback Uptake and Self-Regulated Learning in Procedural Skills Training",
    year = "2024",
    issn = "1929-7750",
    doi = "10.18608/jla.2024.8195",
    abstract = "Remote technology has been widely incorporated into health professions education. For procedural skills training, effective feedback and reflection processes are required. Consequently, supporting a self-regulated learning (SRL) approach with learning analytics dashboards (LADs) has proven beneficial in online environments. Despite the potential of LADs, understanding their design to enhance SRL and provide useful feedback remains a significant challenge. Focusing on LAD design, implementation, and evaluation, the study followed a mixed-methods two-phase design-based research approach. The study used a triangulation methodology of qualitative interviews and SRL and sensemaking questionnaires to comprehensively understand the LAD’s effectiveness and student SRL and feedback uptake strategies during remote procedural skills training. Initial findings revealed the value students placed on performance visualization and peer comparison despite some challenges in LAD design and usability. The study also identified the prominent adoption of SRL strategies such as help-seeking, elaboration, and strategic planning. Sensemaking results showed the value of personalized performance metrics and planning resources in the LAD and recommendations to improve reflection and feedback uptake. Subsequent findings suggested that SRL levels significantly predicted the levels of sensemaking. The students valued the LAD as a tool for supporting feedback uptake and strategic planning, demonstrating the potential for enhancing procedural skills learning.",
    url = "https://doi.org/10.18608/jla.2024.8195"
}

@article{ref499_a7c235,
    author = "Karademir, Onur and Di Mitri, Daniele and Schneider, Ján and Jivet, Ioana and Allmang, Jörn and Gombert, Sebastian and Kubsch, Marcus and Neumann, Knut and Drachsler, Hendrik",
    title = "I don't have time! But keep me in the loop: <scp>Co‐designing</scp> requirements for a learning analytics cockpit with teachers",
    year = "2024",
    issn = "0266-4909",
    doi = "10.1111/jcal.12997",
    abstract = "Abstract Background Teacher dashboards can help secondary school teachers manage online learning activities and inform instructional decisions by visualising information about class learning. However, when designing teacher dashboards, it is not trivial to choose which information to display, because not all of the vast amount of information retrieved from digital learning environments is useful for teaching. Information elicited from formative assessment (FA), though, is a strong predictor for student performance and can be a useful data source for effective teacher dashboards. Especially in the secondary education context, FA and feedback on FA, have been extensively studied and shown to positively affect student learning outcomes. Moreover, secondary teachers struggle to make sense of the information displayed in dashboards and decide on pedagogical actions, such as providing feedback to students. Objectives To facilitate the provision of feedback for secondary school teachers via a teacher dashboard, this study identifies requirements for designing a Learning Analytics Cockpit (LA Cockpit), that is, (1) a teacher dashboard that provides teachers with visualisations of results from formative assessment (FA) and (2) a feedback system that supports teachers in providing feedback to students. Methods This study was conducted in the context of STEM classes and is based on semi‐structured co‐design interviews with German secondary school teachers. In these interviews, we first explored challenges teachers encountered in monitoring students' learning and providing feedback. Second, in the ideation phase, teachers were asked to define features an LA Cockpit for FA should have. Finally, in the evaluation phase, we provided teachers with a design template for an LA Cockpit, the LAC\_Template, which was built upon our previous work and feedback theory, and asked them to evaluate and improve it. Further design requirements were derived based on the evaluation of the LAC\_Template and teachers' suggestions for improvement. Results We derived 16 requirements for designing an LA Cockpit for FA in secondary schools. Findings from the interviews indicated that the feedback system of an LA Cockpit should address teachers' time limitations in giving students individualised feedback. It should therefore be designed to minimise the steps required to deliver feedback. To reduce workload, teachers requested an automated reminder to send feedback, but with the ability to adjust feedback to the learning context. Such a semi‐automated feedback system can help teachers support students individually but also underline the importance of actively involving teachers in the feedback loop and giving them control when using such technologies in secondary school practice. A challenge for future teacher dashboard designs could be to find a balance between technology and teacher control that utilises the strengths of both in a beneficial combination.",
    url = "https://doi.org/10.1111/jcal.12997"
}

@article{ref500_ff2a25,
    author = "Tepgeç, Mustafa and Heil, Joana and Ifenthaler, Dirk",
    title = "Feedback literacy matters: unlocking the potential of learning analytics-based feedback",
    year = "2024",
    issn = "0260-2938",
    doi = "10.1080/02602938.2024.2367587",
    url = "https://doi.org/10.1080/02602938.2024.2367587"
}

@article{ref501_7e745e,
    author = "Liu, Yuchen and Pozdniakov, Stanislav and Martínez‐Maldonado, Roberto",
    title = "The effects of visualisation literacy and data storytelling dashboards on teachers’ cognitive load",
    year = "2024",
    issn = "1449-3098",
    doi = "10.14742/ajet.8988",
    abstract = "Learning analytics (LA) dashboards are becoming increasingly available in various learning settings. However, teachers may face challenges in understanding and interpreting the data visualisations presented on those dashboards. In response to this, some LA researchers are incorporating visual cueing techniques, like data storytelling (DS), into LA dashboard design to reduce the data visualisation skills – often referred to as visualisation literacy (VL) – and cognitive effort required by teachers to effectively use dashboards. However, despite the potential of DS principles in simplifying data visualisations, there is limited evidence supporting their effectiveness in actually reducing teachers’ cognitive load. The study presented in this paper addresses this gap by investigating the potential impact of LA dashboards, with and without DS elements, on teachers with varying VL levels. Through a quasi-experimental study involving 23 teachers, we analysed changes in pupil dilation – a proxy for cognitive load – as they examined LA dashboards featuring student data captured while participating in synchronous, online collaborative learning tasks. Our findings suggest DS can reduce cognitive load, particularly for teachers with lower VL. These results provide insight into the effects of DS and VL on teachers’ cognitive load, thereby informing the design of LA dashboards. Implications for practice or policy:• Developers of LA dashboards need to pay more attention to incorporating visual and narrative elements that are easily comprehensible and target-oriented, based on users’ visualisation literacy levels.• Educational providers and LA designers can recommend dashboards with DS elements to teachers with low VL to enhance their work efficiency.",
    url = "https://doi.org/10.14742/ajet.8988"
}

@article{ref502_07cc98,
    author = "López‐Pernas, Sonsoles and Misiejuk, Kamila and Tikka, Santtu and Kopra, Juho and Heinäniemi, Merja and Saqr, Mohammed",
    title = "Visualizing and Reporting Educational Data with R",
    year = "2024",
    doi = "10.1007/978-3-031-54464-4\_6",
    abstract = "Abstract Visualizing data is central in learning analytics research, underpins learning dashboards, and is a prime method for reporting results and insights to stakeholders. In this chapter, the reader will be guided through the process of generating meaningful and aesthetically pleasing visualizations of different types of student data using well-known R packages. The main visualization types will be demonstrated with an explanation of their usage and use cases. Furthermore, learning-related examples will be discussed in detail. For instance, readers will learn how to visualize learners’ logs extracted from learning management systems to show how trace data can be used to track students’ learning activities. In addition to creating compelling plots, readers will also be able to generate professional-looking tables with summary statistics.",
    url = "https://doi.org/10.1007/978-3-031-54464-4\_6"
}

@article{ref503_1e38d1,
    author = "Koh, Elizabeth and Hu, Xiao",
    title = "Learning Analytics for Learning: Emerging International Trends and Case Studies from the Asia-Pacific",
    year = "2023",
    doi = "10.1007/978-981-16-2327-1\_54-1",
    url = "https://doi.org/10.1007/978-981-16-2327-1\_54-1"
}

@article{ref504_bd1caa,
    author = "Ha, Kunhee and Jo, Il‐Hyun and Lim, Sohye and Park, Yeonjeong",
    title = "Tracking Students' Eye-Movements on Visual Dashboard Presenting Their Online Learning Behavior Patterns.",
    year = "2014"
}

@article{ref505_21f630,
    author = "Poëllhuber, Bruno and Roy, Normand and Lepage, Alexandre",
    title = "Artificial Intelligence in Higher Education",
    year = "2024",
    doi = "10.1201/9781003320791-17",
    abstract = "Since November 2022, ChatGPT has had very high visibility in higher education, raising an impressive amount of debate and discussion.These conversations have been focused on both the various risks and issues raised by such powerful AI tools but also on the diverse possibilities they offer to assist, facilitate, and even augment the work of learners and teachers.For many people, ChatGPT represents an eruption of AI in the field of education.Yet this sudden media attention obscures the fact that AI has been present in higher education for many years already.The field of learning analytics is growing significantly in education, resulting in descriptive or predictive analyses based on the traces left by learners in digital environments, and giving rise to predictive dropout models and dashboards that have been implemented in some universities (Ifenthaler \& Yau, 2020).Technological developments by large cloud providers make it much easier to accumulate data for analysis (data mining) or to develop intelligent conversational agents (chatbots) that can be used to support students (Heryandi, 2020).The field of AI in education (AIED) focuses on learning analytics, conversational robots and natural language processing, adaptive learning, speech and visual recognition, expert systems, and decision support systems.It now also encompasses generative AI.In this chapter, we propose to first situate the field of AIED historically, and then examine three areas that have considerable potential in higher education: learning analytics, adaptive learning, and generative artificial intelligence.We will look at each of these areas in detail and discuss how they can be used to enhance the educational experience, as well as the limitations and challenges associated with them.",
    url = "https://doi.org/10.1201/9781003320791-17"
}

@article{ref506_fe2475,
    author = "Chitsaz, Mahsa and Vigentini, Lorenzo and Clayphan, Andrew",
    title = "Toward the development of a dynamic dashboard for FutureLearn MOOCs",
    year = "2016",
    issn = "2653-665X",
    doi = "10.14742/apubs.2016.869",
    abstract = "Open Online Courses (MOOCs). With the increase of available MOOC data, there is an opportunity tp provide insights to educators and developers into learners' behavior through learning analytics. Focusing on the FutureLearn platform (FL), standardized data files are offered to partner institutions. Additionally, a report is offered to stakeholders, but it is limited in a number of ways: it is static, it is limited in presenting relevant information and, most importantly, it does not provide 'real time' access to data. This paper provides an overview of the rationale and the development process of a dynamic and near real-time dashboard. It explores the viability of different types of visualizations with the available data, lessons learned, comparisons with similar efforts, and future directions are discussed.",
    url = "https://doi.org/10.14742/apubs.2016.869"
}

@article{ref507_f4a4f0,
    author = "Clemente, Félix and Wee, Loo and Esquembre, Francisco and Leong, Tze and Tan, Darren",
    title = "Development of Learning Analytics Moodle Extension for Easy JavaScript Simulation (EjsS) Virtual Laboratories",
    year = "2019",
    doi = "10.48550/arxiv.1911.06654",
    abstract = "Easy JavaScript Simulations (EjsS) is a popular and powerful authoring toolkit for the creation of open source HTML5 compliant JavaScript simulations. This paper focuses on developing a Learning Analytics extension in Moodle for EjsS, capable of monitoring interactions with the simulation (e.g. mouse clicks, states of buttons and sliders, variable assignments). This extension was piloted with educational physics simulations. Data on learners can be visualised in real-time on the instructor dashboard, allowing instructors to better understand the learning process and modify classroom instruction accordingly.",
    url = "https://doi.org/10.48550/arxiv.1911.06654"
}

@article{ref508_c5f3a5,
    author = "Kickmeier-Rust, Michael and Steiner, Christina and Albert, Dietrich",
    title = "Uncovering Learning Processes Using Competence-based Knowledge Structuring and Hasse Diagrams.",
    year = "2015"
}

@article{ref509_50d656,
    author = "Jerez, Alex",
    title = "Scala: supporting competency assessment through learning analytics",
    year = "2015"
}

@article{ref510_eb39e6,
    author = "Buseyne, Siem and Rajagopal, Kamakshi and Danquigny, Thierry and Depaepe, Fien and Heutte, Jean and Raes, Annelies",
    title = "Assessing verbal interaction of adult learners in computer‐supported collaborative problem solving",
    year = "2023",
    issn = "0007-1013",
    doi = "10.1111/bjet.13391",
    abstract = "Abstract The objective of this study is to explore new ways of assessing collaborative problem solving (CPS) processes based on different modalities of audio data and their combination. The data collection took place in an educational lab setting during an experiment with adult teams from professional contexts who collaboratively solved multiple problems as part of a CPS training. From audio data, both verbal (ie, speech) and non‐verbal (ie, pitch) aspects were extracted. Four analysis methods were used, including (a) content analysis; (b) linguistic inquiry and word count; (c) verbal entrainment analysis; and (d) acoustic–prosodic entrainment based on pitch data. Insights are given into the CPS processes of the participating groups using these measures and relevant relationships between some of these measures are further investigated. Based on content analysis, it was found that most of the interactions during the CPS process are task oriented, whereas team‐oriented interactions are less present. Second, three measures of proportion of contribution in CPS were investigated and clear differences in participation patterns between and within teams were found. We suggest that a combination of utterance count and words per sentence could provide valuable insights for quantity and equality of participation. Third, the study explored pronoun use and found that the most frequently used personal pronouns were first‐person singular. Next, the results indicated a relationship between pronoun use and the relative frequency of interactions. Fourth, a rather weak relationship between lexical entrainment measures and the acoustic–prosodic measures were found, suggesting that these measures are indicative of separate communicative aspects in CPS. This study contributes to a better understanding of which type of audio‐based data is most informative to teachers and students as a feedback or assessment tool. This study complements previous research as it focuses on spoken human‐to‐human communication collected in an authentic context. Practitioner notes What is already known about this topic Support and guidance systems for learning coaches, teachers and learners are needed to foster the educational quality of collaborative problem solving (CPS) activities. CPS is a complex process and measuring the quality of CPS processes remains challenging. Multimodal learning analytics, focusing on verbal and non‐verbal data sources and using content analysis, linguistic inquiry and word count and verbal and acoustic entrainment measures could be valuable to measure the quality of CPS. What this paper adds The majority of interactions during CPS processes are task oriented or cognitive of nature, whereas team‐oriented interactions are less present. Utterance count and words per sentence should be used in combination, as they are indicative of different aspects. Pronoun use in learners' discourse is related to the types of CPS interactions. Lexical entrainment measures and acoustic–prosodic are indicative of distinctive communicative aspects in CPS. Implications for practice and/or policy Quality indicators of CPS processes should include both verbal and non‐verbal measures of students' interactions. Educational researchers and the (Edtech) industry should further leverage their forces to foster the development of (semi‐)automated systems for measuring the quality of CPS processes. It should be further investigated how quality indicators of CPS processes can be most meaningful to trainers, teachers and learners, for example, through the use of dashboards.",
    url = "https://doi.org/10.1111/bjet.13391"
}

@article{ref511_6e45f8,
    author = "Campoberde, Jonnathan and Macías, Miguel and Maldonado‐Mahauad, Jorge",
    title = "Proposal for the Design and Implementation of a XBlock in Open edX to Support Learning Analytics",
    year = "2021",
    doi = "10.1109/laclo54177.2021.00088",
    url = "https://doi.org/10.1109/laclo54177.2021.00088"
}

@article{ref512_613ca7,
    author = "Masiello, Italo and Fixsen, Dean and Nordmark, Susanna and Mohseni, Zeynab and Holmberg, Kristina and Rack, John and Davidsson, Mattias and Andersson-Gidlund, Tobias and Augustsson, Hanna",
    title = "Digital transformation in schools of two southern regions of Sweden through implementation-informed approach: A mixed-methods study protocol",
    year = "2023",
    issn = "1932-6203",
    doi = "10.1371/journal.pone.0296000",
    abstract = "The enhancement of-or even a shift from-traditional teaching and learning processes to corresponding digital practices has been rapidly occurring during the last two decades. The evidence of this ongoing change is still modest or even weak. However, the adaptation of implementation science in educational settings, a research approach which arose in the healthcare field, offers promising results for systematic and sustained improvements in schools. The aim of this study is to understand how the systematic professional development of teachers and schools principals (the intervention) to use digital learning materials and learning analytics dashboards (the innovations) could allow for innovative and lasting impacts in terms of a sustained implementation strategy, improved teaching practices and student outcomes, as well as evidence-based design of digital learning material and learning analytics dashboards.This longitudinal study uses a quasi-experimental cluster design with schools as the unit. The researchers will enroll gradually 145 experimental schools in the study. In the experimental schools the research team will form a School Team, consisting of teachers/learning-technologists, school principals, and researchers, to support teachers' use of the innovations, with student achievement as the dependent variable. For the experimental schools, the intervention is based on the four longitudinal stages comprising the Active Implementation Framework. With an anticipated student sample of about 13,000 students in grades 1-9, student outcomes data are going to be analyzed using hierarchical linear models.The project seeks to address a pronounced need for favorable conditions for children's learning supported by a specific implementation framework targeting teachers, and to contribute with knowledge about the promotion of improved teaching practices and student outcomes. The project will build capacity using implementation of educational technology in Swedish educational settings.",
    url = "https://doi.org/10.1371/journal.pone.0296000"
}

@article{ref513_007799,
    author = "Berková, Kateřina and Frendlovská, Dagmar and Chalupová, Martina and Kubišová, Andrea and Hrmo, Roman and Krelová, Katarína",
    title = "Pilot Research into the Perceived Importance of Educational Elements and an Application for Detecting Progress through the Perspective of Practice",
    year = "2022",
    issn = "2227-7102",
    doi = "10.3390/educsci12100669",
    abstract = "Data analysis and the development of learning skills based on monitoring students’ progress are aspects in demand by schools and students. Quite a lot of studies deal with Learning Analytics Dashboards. There is a limited number of studies that take into account the supply of such tools on the market. In this pilot study, the researchers present findings on the attitudes of 19 higher education institutions from the Czech Republic, Belgium, Germany, Greece, the Netherlands and Poland, along with 14 secondary schools from the Czech Republic, towards the proposed web-based application for supporting learning and providing automated feedback on student progress in accounting education. The aim of this section was to find out how schools perceive the importance of the proposed application and its specific parameters. The study also presents the current product offer on the Czech market and the interest among 112 companies in developing such an application. The findings revealed that there is no such tool offered on the Czech market, and the majority of the analyzed companies are interested in its development. The schools evaluated the learning tool as being most important in the area of distance learning, and most useful for the visualization of accounting methods based mainly on imagination. The value of such an application is seen in supporting self-study, providing information on attitudes and current abilities, and tracking students’ learning progress.",
    url = "https://doi.org/10.3390/educsci12100669"
}

@article{ref514_b091ac,
    author = "Gonzalez, N. and Chiappe, Andrés",
    title = "Learning analytics and personalization of learning: a review",
    year = "2024",
    issn = "0104-4036",
    doi = "10.1590/s0104-40362024003204234",
    abstract = "Abstract Education in the 21 st century is increasingly mediated by digital technologies in a context in which enormous amounts of information are daily generated. Regarding this and considering the imminent application of emerging trends such as “Internet of Things” (IoT), the study of its educational effects becomes a matter of great relevance for both educational researchers and practitioners. In this context, “Learning Analytics” takes on special importance as a perspective to approach the aforementioned issue, especially from a very relevant topic: the personalization of learning. In this sense, a systematic review of literature about learning analytics published in the last two decades was carried out to identify its potential as a factor in strengthening the personalization of learning. The results show a set of key factors that include aspects related to assessment, the use of dashboards, social learning networks, and intelligent tutoring, and the importance of monitoring, feedback, and support.",
    url = "https://doi.org/10.1590/s0104-40362024003204234"
}

@article{ref515_93a9b3,
    author = "Trevisan, Ottavia and Christensen, Rhonda and Drossel, Kerstin and Friesen, Sharon and Forkosh‐Baruch, Alona and Phillips, Michael",
    title = "Drivers of Digital Realities for Ongoing Teacher Professional Learning",
    year = "2024",
    issn = "2211-1662",
    doi = "10.1007/s10758-024-09771-0",
    abstract = "Abstract In an era marked by the widespread use of digital technology, educators face the need to constantly learn and develop their own new literacies for the information era, as well as their competencies to teach and apply best practices using technologies. This paper underscores the vital role of ongoing teacher professional learning (OTPL) with a focus on reflective practices and pedagogical reasoning and action (PR\&amp;A) in shaping education quality and equity. Examining three key drivers of educational transformation—big data and learning analytics, Artificial Intelligence (AI), and shifting teacher identities—the paper explores their overall impact on teacher practices. This paper emphasizes technology as a crucial boundary object, a catalyst of educational transformation, when used to foster communication and professional growth. To this end, three boundary objects are identified, namely dashboards, AI-driven professional learning environments, and digital communities of practice. These tools illustrate technology’s capacity to mediate relationships between transformative educational drivers and teacher practices, offering a pathway to navigate shifting perspectives on OTPL. With a theoretical foundation in equitable education, the paper provides insights into the intricate relationship between boundary objects and evolving educational dynamics. It highlights technology's pivotal role in achieving both quality and equitable education in the contemporary educational landscape. It presents a nuanced understanding of how specific tools may contribute to effective OTPL amid rapid educational transformations.",
    url = "https://doi.org/10.1007/s10758-024-09771-0"
}

@article{ref516_a9c78c,
    author = "Wezendonk, Anouk and Veldhuis, Michiel",
    title = "Adaptieve leersystemen en de didactische besluitvorming van basisschoolleerkrachten bij rekenen-wiskunde",
    year = "2024",
    issn = "2773-0360",
    doi = "10.54657/tops.13844",
    abstract = "Adaptieve leersystemen worden veel gebruikt in het Nederlandse basisonderwijs. In dergelijke systemen voeren leerlingen antwoorden in, op basis waarvan het systeem feedback aan de leerling geeft en nieuwe opdrachten selecteert. Daarnaast krijgen leerkrachten informatie over de voortgang en prestatie van de leerlingen, wat vaak wordt weergegeven op dashboards. Deze informatie kunnen leerkrachten gebruiken om hun didactische keuzes vorm te geven. De vraag die in deze multiple casestudy centraal staat is: Hoe kan het gebruik van learning analytics de reken-wiskundig didactische besluitvorming van de leerkracht ondersteunen? Hiertoe zijn drie leerkrachten uitgebreid bevraagd en geobserveerd tijdens een rekenles en is met de leerkrachten gereflecteerd aan de hand van een video stimulated recall interview. De getranscribeerde data is iteratief geanalyseerd met open en axiale coderingen. Na verschillende codeerrondes kwamen we uit op drie perspectieven: differentiatie, learning analytics en didactiek van rekenen-wiskunde. De leerkrachten gebruikten de learning analytics in de blokvoorbereiding om te bepalen of het geplande onderwijsaanbod voldoende aansloot bij de onderwijsbehoeften van hun groep, en maakten eventueel wijzigingen hierin. Daarnaast ondersteunden de learning analytics de leerkrachten in de selectie van leerlingen voor bijvoorbeeld verlengde instructie. Geen enkele keer kwam naar voren dat leerkrachten beslissingen voor bepaalde modellen of strategieën maakten op basis van de gegevens uit dashboard. Op basis van deze resultaten concluderen we voorzichtig dat leerkrachten zich in veel organisatorische aspecten van differentiatie laten ondersteunen door informatie uit adaptieve leersystemen, maar zich voor de reken-wiskundige didactische besluitvorming vooral laten leiden door de voorschriften in lesmethodes.",
    url = "https://doi.org/10.54657/tops.13844"
}

@article{ref517_081ef1,
    author = "Sridhar, K. and Shinde, Govind and Chaurasia, Amrita and R., Asha",
    title = "Data science: simulating and development of outcome based teaching method",
    year = "2023",
    doi = "10.1109/iceconf57129.2023.10083713",
    url = "https://doi.org/10.1109/iceconf57129.2023.10083713"
}

@article{ref518_938f37,
    author = "Sie, Rory and de Laat, Maarten",
    title = "Longitudinal methods to analyse networked learning",
    year = "2014",
    issn = "2794-7661",
    doi = "10.54337/nlc.v9.9000",
    url = "https://doi.org/10.54337/nlc.v9.9000"
}

@article{ref519_a1d23b,
    author = "A., D. and Cristiano, Maia and Martin, Loomes and C., Simpson and Kathleen, S and Robert, Baud and Victor, Lopez and N., Alpaslan and Allen, Mary",
    title = "Creating Smarter Teaching and Training Environments: Innovative Set-Up for Collaborative Hybrid Learning",
    year = "2016",
    issn = "1875-4163",
    doi = "10.3233/978-1-61499-690-3-238",
    url = "https://doi.org/10.3233/978-1-61499-690-3-238"
}

@article{ref520_f5c5e9,
    author = "Vigentini, Lorenzo and Swibel, Brad and Hasler, Garth",
    title = "Developing a Growth Learning Data Mindset",
    year = "2022",
    issn = "1929-7750",
    doi = "10.18608/jla.2022.7377",
    abstract = "While Learning Analytics (LA) have gained momentum in higher education, there are still few examples of application in the school sector. Even fewer cases are reported of systematic, organizational adoption to drive the support of student learning trajectories that includes teachers, pastoral leaders, and academic managers. This paper presents one such case — at the intersection of praxis, governance, and evaluation — from a practitioner perspective. The paper describes the added value of data-driven approaches to create a culture of improvement in students and teachers in a comprehensive coeducational independent day school in Sydney. Evaluating the work done over the past five years to develop LA dashboards, the authors reflect on the process, the inspirations coming from theory, and the impact of the dashboards in the secondary school context. The data presented is not experimental in nature but supplies tangible evidence for the systematic evaluation scaffolded using the SHEILA policy framework. The main contribution of the paper is a practical demonstration of how managers in a secondary school drew from existing literature and observed data to 1) reflect on the adoption of LA in schools and 2) connect the dots between theory and practice to support teachers grappling with the trajectories of student learning and development, thus encouraging students to self-regulate their learning",
    url = "https://doi.org/10.18608/jla.2022.7377"
}

@article{ref521_0dc74b,
    author = "Majumdar, Rwitajit and Bakilapadavu, Geetha and Majumder, Reek and Chen, Mei and Flanagan, Brendan and Ogata, Hiroaki",
    title = "Learning analytics of critical reading activity: Reading Hayavadana during lockdown",
    year = "2020"
}

@article{ref522_17f01a,
    author = "Montuori, Lina and Alcázar-Ortega, Manuel and Vargas‐Salgado, Carlos and Alfonso-Solar, David",
    title = "Learning Analytics as Data driven decision making in High Education: a case study",
    year = "2023",
    doi = "10.4995/inn2022.2023.15750",
    abstract = "This work leads with the application of Learning Analytics (LA) methods in high education as data driven for making decision, where a real case study is presented. The LA method has been applied to a course on HVAC Facilities and Energetic Certification of the Master in Industrial Engineering at the Polytechnic University of Valencia (UPV), Spain. Thus, data collected during the whole academic year 2020-2021 by the institutional LMS platform of UPV have been analyzed. Results show that LA can be a successful data driven for detecting the students at risk, boosting their retention, personalizing contents and improving students learning. Finally, a tailored dashboard is proposed for students’ improvement monitoring.",
    url = "https://doi.org/10.4995/inn2022.2023.15750"
}

@article{ref523_012ab9,
    author = "Ali, Syed and Shaikh, Mohammad and Kelkar, Parineeta and Chowdhury, Soumitra",
    title = "Analyzing and Visualizing Learning Data",
    year = "2025",
    doi = "10.4018/979-8-3693-8593-7.ch004",
    url = "https://doi.org/10.4018/979-8-3693-8593-7.ch004"
}

@article{ref524_652548,
    author = "Osman, Shahinaz and Ahmed, Zeinab",
    title = "Navigating AI Integration",
    year = "2024",
    issn = "2326-8905",
    doi = "10.4018/979-8-3693-2728-9.ch011",
    url = "https://doi.org/10.4018/979-8-3693-2728-9.ch011"
}

@article{ref525_a160a5,
    author = "da Silva, Euler and de Magalhães Netto, José and de Souza, Ricardo",
    title = "VLA Dashboard: Um Mecanismo para Visualização do Desempenho de Estudantes de Matemática no Ensino Médio",
    year = "2018",
    issn = "1679-1916",
    doi = "10.22456/1679-1916.89234",
    abstract = "Este artigo descreve as contribuições do uso de um mecanismo VLA Dashboard para visualização do desempenho de estudantes do Ensino Médio em avaliações de Matemática Básica aplicadas no Moodle. Diante disso, empregou-se uma pesquisa qualitativa do tipo Estudo de Caso. Em seguida, a coleta foi aplicada em três etapas: pré-Teste, oficina de capacitação e aplicação de avaliação diagnóstica pelos professores. Também, foi usado o Modelo de Aceitação de Tecnologia para os questionários. Os resultados apontam que o uso das técnicas de Visualização da Informação, learning analytics e abordagem multiagente contribuem para melhorar os diagnósticos das atividades avaliativas do professor.",
    url = "https://doi.org/10.22456/1679-1916.89234"
}

@article{ref526_36650e,
    author = "Dafoulas, George and Loveday, Joanna and Neilson, David",
    title = "Using data mining for assessing the impact of social media in higher education: the case of integrating social media in the curriculum",
    year = "2015"
}

@article{ref527_5836be,
    author = "Majumdar, Rwitajit and Takami, Kyosuke and Ogata, Hiroaki",
    title = "Learning with Explainable AI-Recommendations at School: Extracting Patterns of Self-Directed Learning from Learning Logs",
    year = "2023",
    doi = "10.1109/icalt58122.2023.00078",
    url = "https://doi.org/10.1109/icalt58122.2023.00078"
}

@article{ref528_656898,
    author = "Ávila-Pesantez, Diego and Usca, Brandon and Angamarca, Bryan and Avila, L.",
    title = "Improving the Serious Game design using Game Learning Analytics and Eye-tracking: A pilot study",
    year = "2021",
    doi = "10.1109/urucon53396.2021.9647058",
    url = "https://doi.org/10.1109/urucon53396.2021.9647058"
}

@article{ref529_a3669f,
    author = "Ruipérez‐Valiente, José",
    title = "Unveiling the Potential of Learning Analytics in Game-Based Learning",
    year = "2022",
    issn = "2328-1316",
    doi = "10.4018/978-1-7998-9732-3.ch023",
    url = "https://doi.org/10.4018/978-1-7998-9732-3.ch023"
}

@article{ref530_d8a7c5,
    author = "Paz, Fábio and Cazella, Sí­lvio",
    title = "Integrando Sistemas de Recomendação com Mineração de Dados Educacionais e Learning Analytics: Uma revisão sistemática da Literatura",
    year = "2018",
    issn = "1679-1916",
    doi = "10.22456/1679-1916.85925",
    abstract = "Apesar de modelos de previsão de desempenho educacional ser um assunto novo, está crescendo o interesse de profissionais da área em sua utilização. Este artigo objetiva apresentar uma revisão sistemática da literatura para identificar publicações científicas sobre Sistemas de Recomendação com Learning Analytics (LA) e Mineração de Dados Educacionais (MDE) a fim de obter uma visão do estado da arte. A revisão foi realizada utilizando as bases do Portal de Periódicos da Capes o qual possui mais de 38 mil publicações, foram encontrados 251 estudos internacionais para a revisão, sendo 7 estudos incluídos para uma análise aprofundada. Entre seus principais resultados destaca-se a apresentação de indicadores gerados por LA e MDE apresentados através de Dashboards que auxiliam no processo de ensino e aprendizagem, também que não foram encontrados sistemas de recomendações para coordenadores de cursos e gestores de instituições. Em síntese, é possível que este mapeamento permita aos pesquisadores ter um panorama sobre o tema objeto de estudo e que o mesmo aponta tendências de pesquisa que poderão ser foco de futuras investigações.",
    url = "https://doi.org/10.22456/1679-1916.85925"
}

@article{ref531_d7f070,
    author = "Corrigan, Owen and Glynn, Mark and McKenna, Aisling and Smeaton, Alan and Smyth, Sinéad",
    title = "Student data: data is knowledge – putting the knowledge back in the students’ hands",
    year = "2015"
}

@article{ref532_5f5731,
    author = "Kuhnel, Matthias and Seiler, Luisa and Honal, Andrea and Ifenthaler, Dirk",
    title = "Mobile Learning Analytics in Higher Education: Usability Testing and Evaluation of an APP Prototype.",
    year = "2017"
}

@article{ref533_da40c4,
    author = "Pishtari, Gerti and Ley, Tobias and Khalil, Mohammad and Kasepalu, Reet and Tuvi, Iiris",
    title = "Model-Based Learning Analytics for a Partnership of Teachers and Intelligent Systems: A Bibliometric Systematic Review",
    year = "2023",
    issn = "2227-7102",
    doi = "10.3390/educsci13050498",
    abstract = "This paper presents a bibliometric systematic review on model-based learning analytics (MbLA), which enable coupling between teachers and intelligent systems to support the learning process. This is achieved through systems that make their models of student learning and instruction transparent to teachers. We use bibliometric network analysis and topic modelling to explore the synergies between the related research groups and the main research topics considered in the 42 reviewed papers. Network analysis depicts an early stage community, made up of several research groups, mainly from the fields of learning analytics and intelligent tutoring systems, which have had little explicit and implicit collaboration but do share a common core literature. Th resulting topics from the topic modelling can be grouped into the ones related to teacher practices, such as awareness and reflection, learning orchestration, or assessment frameworks, and the ones related to the technology used to open up the models to teachers, such as dashboards or adaptive learning architectures. Moreover, results show that research in MbLA has taken an individualistic approach to student learning and instruction, neglecting social aspects and elements of collaborative learning. To advance research in MbLA, future research should focus on hybrid teacher–AI approaches that foster the partnership between teachers and technology to support the learning process, involve teachers in the development cycle from an early stage, and follow an interdisciplinary approach.",
    url = "https://doi.org/10.3390/educsci13050498"
}

@article{ref534_dc8cef,
    author = "Milesi, Mikaela and Martínez‐Maldonado, Roberto",
    title = "Data Storytelling in Learning Analytics? A Qualitative Investigation into Educators’ Perceptions of Benefits and Risks",
    year = "2024",
    doi = "10.1145/3636555.3636865",
    abstract = "Emerging research has begun to explore the incorporation of data storytelling (DS) elements to enhance the design of learning analytics (LA) dashboards. This involves using visual features, such as text annotations and visual highlights, to help educators and learners focus their attention on key insights derived from data and act upon them. Previous studies have often overlooked the perspectives of educators and other stakeholders on the potential value and risks associated with implementing DS in LA to guide attention. We address this gap by presenting a case study examining how educators perceive the: i) potential value of DS features for teaching and learning design; ii) role of the visualisation designer in delivering a contextually appropriate data story; and iii) ethical implications of utilising DS to communicate insights. We asked educators from a first-year undergraduate program to explore and discuss DS and the visualisation designer by reviewing sample data stories using their students' data and crafting their own data stories. Our findings suggest that educators were receptive to DS features, especially meaningful use of annotations and highlighting important data points to easily identify critical information. Every participant acknowledged the potential for DS features to be exploited for harmful or self-serving purposes.",
    url = "https://doi.org/10.1145/3636555.3636865"
}

@article{ref535_1ff357,
    author = "Singh, Shaveen and Meyer, Bernd and Wybrow, Michael",
    title = "UserFlow: A Tool for Visualizing Fine-grained Contextual Analytics in Teaching Documents",
    year = "2020",
    doi = "10.1145/3341525.3387410",
    url = "https://doi.org/10.1145/3341525.3387410"
}

@article{ref536_91b4da,
    author = "Skiba, Diane",
    title = "On the Horizon: Implications for Nursing Education",
    year = "2015",
    issn = "1536-5026",
    doi = "10.1097/00024776-201507000-00015",
    url = "https://doi.org/10.1097/00024776-201507000-00015"
}

@article{ref537_5cd5d4,
    author = "Psathas, Georgios and Τέγος, Στέργιος and Demetriadis, Stavros and Tsiatsos, Τhrasyvoulos",
    title = "Exploring the impact of chat-based collaborative activities and SRL-focused interventions on students’ self-regulation profiles, participation in collaborative activities, retention, and learning in MOOCs",
    year = "2023",
    issn = "1556-1607",
    doi = "10.1007/s11412-023-09394-0",
    abstract = "Abstract Despite their potential to deliver a high-quality learning experience, massive open online courses (MOOCs) pose several issues, such as high dropout rates, difficulties in collaboration between students, low teaching involvement, and limited teacher–student interaction. Most of these issues can be attributed to the large number, diversity, and variation in self-regulated learning (SRL) skills of participants in MOOCs. Many instructional designers try to overcome these issues by incorporating collaborative activities. Others try to scaffold students’ SRL levels by making SRL-focused interventions. However, limited research combines the study of SRL-focused interventions with students’ engagement in collaborative activities, course retention, and learning outcomes of MOOC environments. We deployed a programming-oriented MOOC in which we incorporated chat-based collaborative activities, supported by a learning analytics dashboard. Students were asked to complete SRL-focused questionnaires at the beginning and the end of the course. Based on their score, we calculated an average score that forms their SRL level, creating three groups: (a) control, (b) general intervention, and (c) personalized intervention in which we provided personalized interventions. We compared the students’ learning outcomes, participation in collaborative activities, and retention in the MOOC. These comparisons provided evidence regarding the positive impact of different intervention modes on students’ engagement in collaborative activities and their learning outcomes, with respect to their various SRL profiles. Students allocated to the general and personalized intervention groups displayed increased participation in the collaborative activities and learning outcomes, as compared to students assigned to the control group. We also documented that the SRL interventions positively affected students’ course retention.",
    url = "https://doi.org/10.1007/s11412-023-09394-0"
}

@article{ref538_a1cf04,
    author = "El-Khalili, Nuha and Abu Arqoub, Muhammad and Hasan, Mohammad and Banna, Abed and Arafah, Mohammad",
    title = "Empowering Learning Analytics with Business Intelligence",
    year = "2024",
    doi = "10.1109/iccr61006.2024.10533111",
    url = "https://doi.org/10.1109/iccr61006.2024.10533111"
}

@article{ref539_0b7979,
    author = "Zapata‐Rivera, Diego",
    title = "Introduction",
    year = "2018",
    doi = "10.4324/9781351136501-1",
    abstract = "The chapters in this volume provide a balance of research and practice in the field of score reporting. The first section includes foundational work on validity issues related the use and interpretation of test scores, design principles drawn from areas such as cognitive science, human-computer interaction and information visualization, and research on communicating assessment information to various audiences. The second section provides a select compilation of practical applications in real settings: large-scale assessment programs in K-12, credentialing and admissions tests in higher education, using reports to support formative assessment in K-12, applying learning analytics to provide teachers with class- and individual-level performance, and evaluating students' interpretation of dashboard data. These chapters highlight the importance of clearly communicating assessment results to the intended audience to support appropriate decisions based on the original purposes of the assessment. As more technology-rich, highly interactive assessment systems become available, the more important it is to keep in mind that the information provided by these systems should support appropriate decision making by a variety of stakeholders. Many opportunities for research and development involving the participation of interdisciplinary groups of researchers and practitioners lie ahead in this exciting field.",
    url = "https://doi.org/10.4324/9781351136501-1"
}

@article{ref540_48e9c4,
    author = "Rienties, Bart and Edwards, Chris and Gaved, Mark and Marsh, Vicky and Herodotou, Christothea and Clow, Doug and Cross, Simon and Coughlan, Tim and Jones, Jan and Ullmann, Thomas",
    title = "Scholarly insight 2016: a Data wrangler perspective",
    year = "2016"
}

@article{ref541_2bfa6c,
    author = "Clark, Andrew and Shephard, Craig and Robson, Andrew and McKechnie, Joel and Morrison, R. and Rankin, Abbie",
    title = "A Multifaceted Approach to Developing an Australian National Map of Protected Cropping Structures",
    year = "2023",
    issn = "2073-445X",
    doi = "10.3390/land12122168",
    abstract = "As the global population rises, there is an ever-increasing demand for food, in terms of volume, quality and sustainable production. Protected Cropping Structures (PCS) provide controlled farming environments that support the optimum use of crop inputs for plant growth, faster production cycles, multiple growing seasons per annum and increased yield, while offering greater control of pests, disease and adverse weather. Globally, there has been a rapid increase in the adoption of PCS. However, there remains a concerning knowledge gap in the availability of accurate and up-to-date spatial information that defines the extent (location and area) of PCS. This data is fundamental for providing metrics that inform decision making around forward selling, labour, processing and infrastructure requirements, traceability, biosecurity and natural disaster preparedness and response. This project addresses this need, by developing a national map of PCS for Australia using remotely sensed imagery and deep learning analytics, ancillary data, field validation and industry engagement. The resulting map presents the location and extent of all commercial glasshouses, polyhouses, polytunnels, shadehouses and permanent nets with an area of \&gt;0.2 ha. The outcomes of the project revealed deep learning techniques can accurately map PCS with models achieving F-Scores \&gt; 0.9 and accelerate the mapping where suitable imagery is available. Location-based tools supported by web mapping applications were critical for the validation of PCS locations and for building industry awareness and engagement. The final national PCS map is publicly available through an online dashboard which summarises the area of PCS structures at a range of scales including state/territory, local government area and individual structure. The outcomes of this project have set a global standard on how this level of mapping can be achieved through a collaborative, multifaceted approach.",
    url = "https://doi.org/10.3390/land12122168"
}

@article{ref542_36769f,
    author = "Chou, Chih‐Yueh and Chih, Wen and Tseng, Shu and Chen, Zhi",
    title = "Simulatable Open Learner Models of Core Competencies for Setting Goals for Course Performance",
    year = "2019",
    doi = "10.58459/icce.2019.294",
    url = "https://doi.org/10.58459/icce.2019.294"
}

@article{ref543_6b71d9,
    author = "Srivastava, Namrata and Nawaz, Sadia and Tsai, Yi‐Shan and Gašević, Dragan",
    title = "Curriculum Analytics of Course Choices:",
    year = "2024",
    issn = "1929-7750",
    doi = "10.18608/jla.2024.8095",
    abstract = "In a higher education context, students are expected to take charge of their learning by deciding “what” to learn and “how” to learn. While the learning analytics (LA) community has seen increasing research on the “how” to learn part (i.e., researching methods for supporting students in their learning journey), the “what” to learn part is still underinvestigated. We present a case study of curriculum analytics and its application to a dataset of 243 students of the bachelor’s program in the broad discipline of health sciences to explore the effects of course choices on students’ academic performance. Using curriculum metrics such as grading stringency, course temporal position, and duration, we investigated how course choices differed between high- and low-performing students using both temporal and sequential analysis methods. We found that high-performing students were likely to pick an elective course of low difficulty. It appeared that these students were more strategic in terms of their course choices than their low-performing peers. Generally, low-performing students seemed to have made suboptimal choices when selecting elective courses; e.g., when they picked an elective course of high difficulty, they were less likely to pick a following course of low difficulty. The findings of this study have design implications for researchers, program directors, and coordinators, because they can use the results to (i) update the course sequencing, (ii) guide students about course choices based on their current GPA (such as through course recommendation dashboards), (iii) identify bottleneck courses, and (iv) assist higher education institutions in planning a more balanced course roadmap to help students manage their workload effectively.",
    url = "https://doi.org/10.18608/jla.2024.8095"
}

@article{ref544_a85fab,
    author = "McEllistrem, Brian and Hennus, Marije and Fawns, Tim and Hanley, Karena",
    title = "Exploring the Irish general practice training community’s perceptions on how an entrustable professional activities dashboard implementation could facilitate general practice training in Ireland",
    year = "2023",
    issn = "1473-9879",
    doi = "10.1080/14739879.2023.2191340",
    url = "https://doi.org/10.1080/14739879.2023.2191340"
}

@article{ref545_9cfa85,
    author = "Rienties, Bart and Clow, Doug and Coughlan, Tim and Cross, Simon and Edwards, Chris and Gaved, Mark and Herodotou, Christothea and Hlosta, Martin and Jones, Jan and Rogaten, Jekaterina and Ullmann, Thomas",
    title = "Scholarly insight Autumn 2017:a Data wrangler perspective",
    year = "2017"
}

@article{ref546_6bd223,
    author = "Yau, Jane and Mah, Dana-Kristin and Ifenthaler, Dirk",
    title = "Utilising learning analytics for study success: A systematic review of five years of research (2013-17)",
    year = "2018"
}

@article{ref547_79626c,
    author = "Gunnars, Fabian",
    title = "Smartbands and Behavioural Interventions in the Classroom: Multimodal Learning Analytics Stress-Level Visualisations for Primary Education Teachers",
    year = "2024",
    issn = "1034-912X",
    doi = "10.1080/1034912x.2024.2355625",
    abstract = "Students' stress levels may affect their well-being, attentiveness and learning outcomes in primary education classrooms. Positive behavioural interventions and support actions conducted by teachers may alleviate students' stress levels, especially when addressing special educational needs. In this multimodal learning analytics study, students in a classroom were all given a smartband for their wrist during regular curriculum activities. Data comprised the semester of a single subject as a part of a research project conducted in Sweden. Biobehavioural stress-related arousal of students' autonomic nervous system was visualised and analysed through distinguished behavioural modes. Additional data include naturalistic observational notes and two short teacher interviews. Research methodology and strategies for innovative implementation were presented and discussed alongside contextual details. For example, stress-level visualisations can aid actionable adjustments of behavioural intervention intensity and provide students' attentiveness overview for teachers that sequence curricular activities during planning. Findings show an interdisciplinary basis for cost-effective real-time dynamic solutions that involve visual dashboards with advantages to understanding student learning, both at a school-wide system level and for the classroom, if viewed optimistically. However, research on the topic is still in its infancy, notably with ethical risks as a growing pain.",
    url = "https://doi.org/10.1080/1034912x.2024.2355625"
}

@article{ref548_dd5026,
    author = "Spletter, Christian and Eppler, Martin",
    title = "Knowledge Visualization for Learning in Higher Education Contexts: Systemizing the Field",
    year = "2023",
    issn = "2048-8963",
    doi = "10.34190/eckm.24.1.1495",
    abstract = "In contexts of higher learning, students must be supported effectively in developing their knowledge, skills, and competencies. Thus, faculty members (incl. lecturers and administrators) are faced with the management task to organize and align innovative teaching and learning formats. As we know from research, the use of knowledge visualization is both a facilitating tool for cognitive processing and learning itself, and for strategic decision-making processes within organizations. However, the literature on the types of (IT-enabled) knowledge visualization for learning in higher education contexts is highly fragmented and dispersed and includes different branches for researchers and practitioners. This makes it difficult to achieve an overview and find systematic and consistent visual approaches along students’ learning paths. By highlighting the role of knowledge visualization to support organizing innovative teaching and learning, we provide a systematic, structured overview of such approaches. The goal of this paper is thus to structure the field of knowledge visualization for lifelong and university-based learning based on seminal papers. For this purpose, we present a segmentation approach with six areas to analyse the role of knowledge visualization for learning in higher education contexts, namely: Visualizing Learning Offers (e.g., Curriculum Visualization Tools), Visual Learning Environments (e.g., Metaverse), Learning Content Visualization (e.g., Visual Variation Patterns), Visual Techniques for Learning (e.g., Concept Mapping), Visual Learning Analytics (e.g., Learner Dashboards), and Visualizing Learning Outcomes (e.g., Digital Course Badges). Based on systemizing key concepts, our paper concludes with promising future research avenues for each of the six areas, as well as for the domain of knowledge visualization for higher learning itself. We conclude with specific ideas how the area of visualizing learning offers can act as a spearhead for empirical research (and practice transfer) in the knowledge visualization domain. This should help practitioners and researchers from higher education contexts who consider lifelong learning as knowledge management task.",
    url = "https://doi.org/10.34190/eckm.24.1.1495"
}

@article{ref549_6d4231,
    author = "De Sousa Drumond, Jose and Kotsiopoulou, G.",
    title = "Building an Effective Ecosystem for an Integrated Community of Practice and Frustration-Free Technical Knowledge Hub that Maximizes Engagement and Participation of Local and Field Resources Across Multiple Operating Countries",
    year = "2022",
    doi = "10.2118/210844-ms",
    url = "https://doi.org/10.2118/210844-ms"
}

@article{ref550_1b34ac,
    author = "Hilton, Amy and Blunt, Shelly and Mitchell, Zane",
    title = "Capacity-Building to Transform STEM Education Through Faculty Communities in Learning Analytics and Inquiry",
    year = "2024",
    doi = "10.18260/1-2--42085",
    abstract = "Abstract This NSF IUSE ICT Capacity-building track project is designed to build capacity for future efforts to support STEM faculty in collaborative inquiry processes to explore questions on student learning and success and to inform changes to improve individual classes, student pathways, and curricula. The goal of this capacity-building project is to strengthen the data infrastructure for faculty use and to cultivate faculty buy-in for engaging in STEM education transformation at a public, regional, primarily undergraduate institution. Specifically, the project objectives are to: 1) expand learning analytics data tools that are relevant and actionable for faculty; 2) engage faculty in activities and learning communities that connect academic data with individual perspectives and values to motivate interest in evidence-based instructional strategies; 3) build community across STEM educators; and 4) refine theories of change and frameworks for a future change implementation project. The focus is on introductory, foundational, and gateway STEM courses. Towards these goals, two integrated and multidisciplinary faculty communities have been intentionally developed and implemented. The Inquiry in STEM Success community seeks to enhance faculty members' understanding of student learning, success, and retention in STEM and increase knowledge of evidence-based instructional practices. The questions generated are used by the Data Tools Co-design community to iteratively refine use cases for data analytics dashboard. These activities aim to increase the collective understanding of faculty members in identifying bottlenecks and barriers to student success in STEM. Preliminary results indicate that the explorations and discussions facilitated through the faculty communities have started to create a sense of community and prompt further conversations and considerations of STEM student success issues by the 17 faculty participants across 7 departments. Moreover, three assumptions, grounded in theories of change, guide this project. First, providing faculty with multiple ways to engage with student success challenges and evidence-based teaching will cultivate motivation to consider change in instruction and curricular design. Second, data alone will not drive change, but rather developing connections with data and evidence will help motivate transformation. Third, systems thinking establishes an effective framework to organize efforts to implement change. This project is funded by the Division of Undergraduate Education (EHR/DUE).",
    url = "https://doi.org/10.18260/1-2--42085"
}

@article{ref551_f4f675,
    author = "Schumacher, Daniel and Turner, David",
    title = "Entrustable Professional Activities: Reflecting on Where We Are to Define a Path for the Next Decade",
    year = "2021",
    issn = "1040-2446",
    doi = "10.1097/acm.0000000000004097",
    abstract = "Leaders of competency-based medical education (CBME) are drawn to the concept of entrustable professional activities (EPAs), which are likely the most widespread worldwide approach to CBME at this time. 1–9 Despite the seemingly universal affinity for EPAs, empirical evidence for their use in assessment and results of implementation in clinical contexts are just emerging. 10–15 This supplement collates papers from international leaders across a range of specialties and countries for the purpose of presenting the leading edge knowledge and thinking regarding empiric evidence and implementation experiences for EPA-based curricula and assessment. In this Foreword, we highlight key concepts presented in this supplement for consideration and action as the medical education community more broadly implements EPAs, fills remaining gaps with continued EPA-based research, and develops the vision and agenda for the next decade of CBME advancement. EPA Phenotypes Vary: Opportunities for Aligning Across Training Stages and Specialties With prolific efforts to define sets of EPAs over the past decade, it is clear that varied EPA phenotypes exist. These phenotypes may vary based on whether they define a stage of training, or a profession. Phenotype differences may also reflect the regulatory oversight in various countries, with some having a single regulatory body that facilitates alignment across the continuum and others having different regulatory bodies overseeing the different phases of training and practice. Representing a centralized authority, Karpinski and Frank detail the delineation of the Royal College of Physicians and Surgeons of Canada EPAs across stages of training and practice for all but one specialty in the country. 16 A hallmark of their approach is defining EPAs for stages of training (transition to discipline, foundations of discipline, core of discipline, and transition to practice). In contrast, the regulatory design in the United States, featuring individual authorities at each phase of training and practice, creates challenges in aligning EPAs across specialties and stages of training. This structure requires backward visioning from the specific EPAs of a profession to the phases of graduate medical education (GME) (i.e., fellowship and residency) and ultimately to the generic core EPAs for undergraduate medical education (UME). Schwartz et al illustrate how it is possible to connect EPAs across the full continuum of training, even in the context of the fragmented design currently in place in the United States. 17 Their article is the first published work to assess individual trainees across the full continuum of medical education from UME through GME (both residency and fellowship), linking EPAs designed for stages of training with those that were intentionally designed to define a profession. This article describing the American pediatric experience reinforces previously published efficiencies afforded by aligning stages of training in a manner that has already enabled time-variable advancement of medical students to residency earlier than time-based norms. 18 While this pilot work has not yet been scaled, it opens the door for broader implementation by demonstrating the reality of time-variable transitions even in the current structure of medical education in the United States. Indeed, ten Cate and colleagues present the Dutch experience with implementing EPAs across specialties nationally and how these efforts have similarly allowed time-variable training, meeting the goals of educators, while also facilitating the Dutch government goal of cutting costs for medical education as a whole. 19 We believe competency-based, time-variable training is the future, and the articles in the supplement demonstrate the practical use of EPAs in ultimately achieving this goal. Our vision for the future is continued expansion and escalation of this type of work to align EPA phenotypes worldwide in the next decade as more wide-ranging implementation continues. One step toward seeing this vision come to fruition is underscored by the call made by ten Cate and colleagues to increase focus on EPAs that cross professions and specialties. 20 An example they provide are EPAs focused on ultrasound imaging that is now commonly performed by several specialties beyond radiology. Lindeman and colleagues provide another example from surgery. 21 One of the initial 5 EPAs that they created to define their specialty is: “Provide general surgical consultation to other health care providers.” Providing consultation is likely applicable to most, if not all, specialties with a single-word substitution for the name of the specialty. We also suspect that mastery of this EPA across specialties is quite similar. Thus, sharing the tasks associated with performing this EPA with other disciplines could benefit their efforts to define EPAs and enhance collaboration across specialties. Such work may result in economies of scale to advance EPAs across specialties in a more expeditious manner. We believe the work of Lindeman and colleagues also highlights how learning and alignment of care can be facilitated across specialties even if EPAs are not shared. 21 They took an approach to developing EPAs for general surgery that firmly rejected reducing the work of a surgeon to a technician that performs a series of procedures. Rather, they view the surgeon as a professional that cares for patients before, during, and following surgeries; performs important activities that are not related to procedures; and uses clinical reasoning to approach symptomatology before a diagnosis is reached. Following these guiding principles, they did not develop an EPA focused on “performing an appendectomy” and rather developed one focused on managing a patient presenting with right lower quadrant pain. This approach lends itself to cross-disciplinary alignment that can benefit multiple disciplines and specialties given that many types of providers and specialists routinely manage patients with right lower quadrant pain, including pediatrics, emergency medicine, internal medicine, obstetrics and gynecology, and radiology. We envision the curricular aspects of this EPA facilitating cross-disciplinary learning and patient management approaches that not only align care across specialties but streamline and improve care to patients in the process. EPA Genotypes Are Consistent: Identifying the Core Components Needed to Distinguish a Professional Activity as an EPA and Implementing It as Intended While phenotypes of EPAs differ to meet needs of specialties and stages of training based on regulatory and logistic needs, over a decade into work with EPAs, it appears clear that EPA genotypes should not vary. The basic structure has been detailed by ten Cate et al and is not typically debated. 1,16 The international EPA genome project is also underway and includes researchers throughout the world, and a recent article seeks to clarify key terminology in entrustment language with the goal of encouraging a common lexicon. 22 Furthermore, an attempt at identifying core components for EPA implementation has been completed. 23 As a common genome for EPAs crystallizes, it will be important to surveil for both advantageous and deleterious mutations that promote long-term sustainability of EPAs or threaten their longevity, respectively. EPAs Bring the Patient Into the Training and Assessment Equation Only If We Make Sure They Do Scholars have written about the ability of EPAs to bring the patient into the assessment equation in a manner not afforded by other approaches to training and assessment because their main focus is based on the outcome of safe and effective care delivery. 3,24 To this end, ten Cate and colleagues argue that their impact on patients is a key consideration for the coming years as we advance EPA-based assessment efforts. 20 Indeed, a just emerging approach to trainee assessment, using resident-sensitive quality measures, seeks to link patient outcomes with education outcomes. 25,26 However, as Sebok-Seyer and colleagues note, patients only find themselves in the foreground of our EPA-based training and assessment efforts if we ensure they are positioned there. 27 This placement cannot be left to chance. Rather, patients must be intentionally brought to the table where EPAs for a specialty are being defined and be involved in training and improving care related to these activities. Equally important, EPAs should seek to explicitly evoke a focus on patients rather than placing undue balance of focus on trainees alone. However, even the best EPA descriptions can foster patient-centered or non–patient-centered care, and both intentional design and implementation are required to achieve the former. Ensuring Learner Agency in EPA-Based Assessment Efforts As Sebok-Syer and colleagues note, trainees may currently be unintentionally effacing patients in EPA-based efforts. However, bringing patients to the foreground must also not efface trainees in the process. CBME advocates placing the learner in the driver’s seat for learning. 28 While giving learner’s control in the assessment process, recent work by Schut and colleagues have found that increasing trainees’ ownership and agency in their assessments can facilitate lifelong learning skill development and also shift their perceptions of assessment from higher stakes to lower stakes. 29 When learners perceive assessment as high stakes, they are more likely to lose sight of its formative aspects and less likely to use it to promote learning. 30,31 The Minnesota Method presented by Hobday and colleagues is an exemplar approach to granting agency to learners by giving them primary responsibility for collecting frequent EPA-based workplace assessments. 32 This approach generates a greater number and variety of assessments, serving to enrich feedback and help each student develop an individual leaning plan. In the aggregate, these assessments inform a dashboard that is helpful to both students and programs. Furthermore, the work of Violato and colleagues demonstrates validity evidence for this approach to assessment. 33 Navigating Tensions in Assessment and Feedback Kinnear and colleagues present a model approach for developing and refining validity arguments for a program of assessment. 34 While programmatic assessment is important to ensuring robust data that enables committees and programs to make summative decisions about trainee advancement, articles from Kinnear et al and Ginsburg et al highlight the challenge that placing an undue emphasis on summative decision making (assessment of learning) can have on limiting the educational value of assessment efforts (assessment for learning) such as promoting learner agency. 35,36 In the former article, they note the tension in using assessment data to perform the double duty of providing formative feedback to trainees as well as inform summative decisions about performance. 35 In the latter article, they question whether written comments in assessment should even be asked to serve both purposes, arguing that the lexicon used to help trainees develop is not necessarily the same as that used to report their performance to others. 36 They posit opportunity should be made for both uses. Managing these tensions in the coming decade should receive high priority as both programmatic assessment and qualitative assessment continue to receive substantial focus. 37–43 The need for learning analytics is greatly magnified by this proposed need for additional qualitative assessment data capture for each trainee assessment. 44–46 Does Supervised Practice End at Graduation? Two articles in the supplement highlight trainees finishing stages of training without being deemed ready to transition to the next stage based on EPA assessments. 47,48 This is not the first time such end-of-training gaps in EPA-based assessment efforts have been described. 49 As Amiel and colleagues note, one explanation for deficiencies in some EPAs may be that trainees are not afforded the opportunity to advance or perhaps only afforded the opportunity to demonstrate performance at higher levels shortly before graduation such that gathering sufficient evidence about performance is challenging or impossible. 47 In addition, we are also concerned that these findings may represent gaps in training that lead to graduating learners who are not ready for the next stage of training or for unsupervised practice. This concern seems to be borne out over the past few decades by fellowship director concerns that recent residency graduates are not ready to enter fellowship, 50 department chair reports that new faculty are not prepared to perform routine tasks of the specialty, 51 residents conveying their lack of readiness to provide care routine in their field, 52 and training directors commenting they would not want their own graduates to care for their family members. 53 If these performance gaps are not anticipated and addressed, the implications for training and practice are great. As Turner and colleagues highlight, this issue may require us to rethink what transitions from training to practice look like and what supervisory structures may need to be in place to ensure these transitions are safe for patients. 48 This challenge also has remarkable implications for the structure and approach of current training programs. EPAs can only help us ensure we meet the needs of patients if they drive necessary curriculum to ensure trainees achieve educational outcomes that equip them to meet those needs. As Sebok-Syer and colleagues note, this is only possible if we view EPAs as informing both our curricular efforts and our assessment efforts. 27 EPAs, as important contributors to curriculum, are also highlighted as a component of the research agenda by ten Cate et al, where they note the importance of “using EPAs across the educational continuum and into practice.” 20 We believe this topic deserves crucial focus given the current known limitations of training, which are further compounded by the explosion of knowledge and technology that abolishes any notion that structured learning ends with training. In the current world, EPAs provide a bridge across the abyss that exists in learning and assessment between training and practice and create an opportunity for such a structure. Fluidity of EPAs Over Time Articles from Lindeman and colleagues as well as Amiel and colleagues highlight that EPAs for a given field should not be viewed as static. 21,47 In the case of Lindeman and colleagues, they took a pragmatic approach to developing EPAs for general surgery, electing to start with 5 that had consensus as core to the specialty while they negotiated additional EPAs that broadly define the specialty. 21 The experience of Amiel and colleagues follows extensive work implementing the Association of American Medical Colleges’ (AAMC) Core EPAs for Entering Residency (CEPAERs) at multiple schools. 47 These experiences led them to reflect on EPAs that should perhaps be added, including educating patients about their illness, motivational interviewing, and providing telemedicine services. For us, these examples highlight both EPAs not included in the first iteration that are worthy of consideration as well as clear changes in the practice of medicine (telemedicine) since the AAMC CEPAERs were published. Moving forward, it will be important for groups defining EPAs for given specialties to remain sensitive to EPAs that should be added and removed based on fundamental changes in practice. One possible way to ensure that a profession’s EPAs keep pace with the discipline is to align them with a regulatory oversight process intended to accomplish updating for other purposes. For example, in the United States, some member boards of the American Board of Medical Specialties perform recurring practice analyses to ensure that content specifications for certifying examinations are up to date. Ensuring that EPAs are always aligned with these, practice analysis updates would provide a system of checks and balances to keep EPAs focused on what is important for the profession, achieving a feedback loop between assessing EPAs and using identified gaps to improve curriculum, as advocated by ten Cate and colleagues. 20 What Is Preposterous, Possible, and Plausible in the Next 10 Years? Looking ahead to the next decade, it is important to go beyond the plausible and even the possible and begin envisioning what some would call preposterous. 54 As the articles in this supplement illustrate, we have gone from thinking of learning analytics as a dream and linking patient and learner outcomes as the unreachable Holy Grail a decade ago, to seeing this uncharted territory appear on the map as plausible. 46 van der Vleuten and Schuwirth recently wrote about phases of assessment over the past 5 decades, detailing 3 phases. 43 All workplace-based assessments, including EPAs, fit well in the second phase, assessment as judgment, and current third phase focused on assessment as a system. Looking forward, Lentz et al predict an evolution of the third phase to focus on assessment as a sociotechnical system where artificial intelligence becomes a collaborator amongst learners, faculty, patients, and the clinical learning environment. As medical educators, we must work collectively to thoughtfully integrate these technologies in ways that further assessment for learning, that ultimately leads to improved patient outcomes and experiences. 55 What seems preposterous at this moment in time is putting aside the politics involved in medical education; the competition among us and our institutions; and working together to make competency-based education, training, and practice a seamless continuum with meaningful assessment and feedback. A central focus on the patient must be a hallmark of every phase. As ten Cate and colleagues elaborate, “the increasingly complex and variable learning environment where the work of CBME and EPA implementation occurs constitutes an intriguing yet daunting area for further exploration” in the coming decade. 20 The foundation of CBME is taking a learner-centered and patient-focused approach, 28,56 both of which are fully achievable if we desire to achieve them. Our hope is that we can look back on the ensuing decade as we have just done for this past decade and emerge with feelings of gratitude and a sense of accomplishment for what we have been able to do together for the benefit of our patients and learners. Acknowledgments: The authors wish to thank Dr. Carol Carraccio for her thoughtful review and suggestions for previous versions of this Foreword.",
    url = "https://doi.org/10.1097/acm.0000000000004097"
}

@article{ref552_5f8e9b,
    author = "Bennett, Liz and Folley, Sue",
    title = "Critical evaluation of the features on one student dashboard",
    year = "2016"
}

@article{ref553_46f644,
    author = "Suárez, Ángel and Ternier, Stefaan and Helbig, René and Specht, Marcus",
    title = "DojoAnalytics",
    year = "2017",
    doi = "10.1145/3136907.3136939",
    abstract = "DojoIBL1 is a cloud based platform that provides flexible support for collaborative inquiry-based learning processes. It expands the learning process beyond the classroom walls and brings it to an online setting. Such transition requires teachers and learners to have more means to track and to follow up their progress. Learning Analytics dashboards provide such functionality in form of meaningful visualizations. In this paper we present the DojoAnalytics, a new module of DojoIBL that enables connections with third party Learning Analytics dashboards. In order to demonstrate interoperability with the external dashboards, two use case implementations will be described.",
    url = "https://doi.org/10.1145/3136907.3136939"
}

@article{ref554_1d61b9,
    author = "Vemula, Suchith and Moraes, Márcia",
    title = "earning Analytics Dashboards for Advisors – A Systematic Literature Review",
    year = "2024",
    issn = "2277-548X",
    doi = "10.5121/ijci.2024.130101",
    abstract = "Learning Analytics Dashboard for Advisors is designed to provide data-driven insights and visualizations to support advisors in their decision-making regarding student academic progress, engagement, targeted support, and overall success. This study explores the current state of the art in learning analytics dashboards, focusing on specific requirements for advisors. By examining existing literature and case studies, this research investigates the key features and functionalities essential for an effective learning analytics dashboard tailored to advisor needs. This study also aims to provide a comprehensive understanding of the landscape of learning analytics dashboards for advisors, offering insights into the advancements, opportunities, and challenges in their development by synthesizing the current trends from a total of 21 research papers used for analysis. The findings will contribute to the design and implementation of new features in learning analytics dashboards that empower advisors to provide proactive and individualized support, ultimately fostering student retention and academic success.",
    url = "https://doi.org/10.5121/ijci.2024.130101"
}

@article{ref555_871dc7,
    author = "Harrington, Susan and Sellberg, Charlott",
    title = "User wants, needs and resistance in the postdigital university: prioritizing pedagogical concerns in learning analytics design",
    year = "2024",
    doi = "10.21203/rs.3.rs-5561942/v1",
    abstract = "<title>Abstract</title> Despite the increasing interest in learning analytics over the past decade, there remains relatively little integration with pedagogical practice in higher education. The limited engagement of teachers in the design and development of teacher facing dashboards has been cited as a significant issue. As the purpose of a teacher facing dashboard is to enable deep insights into student learning as a basis for data-driven decision making, successful integration depends on compatibility with pedagogical needs and intentions. In this paper, we outline findings from a series of focus groups with nautical simulator instructors (n=12) from three different universities in Sweden, Norway and Finland. During focus groups, the design concept of multimodal learning analytics was presented and discussed, with focus on the usefulness of a teacher-facing learning analytics dashboard in addressing typical instructional concerns during simulation-based training. In the analysis, we focus on the juxtaposition between user wants, needs and resistance, prioritizing the simulator instructors’ pedagogical concerns in learning analytics design. The findings highlight a crucial gap between the technological capabilities of learning analytics and the practical, everyday challenges of teaching in simulation-based environments. To bridge this gap, it is essential to design dashboards that are not only professionally intuitive and data-rich but also adaptable to the specific pedagogical strategies of instructors. Ultimately, successful implementation depends on a user-centered design process that integrates instructors’ perspectives, ensuring that learning analytics tools align with the complexities of simulation-based training.",
    url = "https://doi.org/10.21203/rs.3.rs-5561942/v1"
}

@article{ref556_1a6b87,
    author = "Dawson, Phillip and Apperley, Thomas",
    title = {Open-Source Learning Analytics and "what the student does"},
    year = "2012"
}

@article{ref557_06f4d8,
    author = "Čelar, Angela",
    title = "Nadzorna ploča analitike učenja",
    year = "2019"
}

@article{ref558_469c9d,
    author = "Sharma, Priya and Akgün, Mahir and Li, Qiyuan",
    title = "Learning Analytics to Support Student Interaction and Learning Design",
    year = "2021",
    doi = "10.4324/9781003089704-4",
    url = "https://doi.org/10.4324/9781003089704-4"
}

@article{ref559_8b27c2,
    author = "Miteva, Dafinka and Stefanova, Eliza",
    title = "eAnalytics: a model of a learning analytics visualization system",
    year = "2022",
    issn = "1313-9215",
    doi = "10.60063/gsu.fmi.109.99-119",
    abstract = {Collecting big data from e-learning is already a fact. Huge opportunities open up for learning analytics to deeply understand and effectively optimize the process of teaching and learning. At the same time, large amounts of data require more time, efforts and resources to extract useful information from them. This paper presents a research proposing a model of a learning analytics system aiming through advances in big data visualization methods to facilitate data interpretation and assist timely and accurate decision making. The study describes an architectural model called eAnalytics. It consists of three main components: Data sources connection, Data management and Learning analytics visualization. The last component provides a choice of three methods for visualizing analytics: via a ready-to-use virtual dashboard, via a virtual dashboard template, or by composing personalized analytics. The paper depicts the prototyping of the model and its validation by conducting two types of testing: for functional compliance and by applying "Think aloud" protocol. Finally, important conclusions are drawn and some directions for future work are outlined.},
    url = "https://doi.org/10.60063/gsu.fmi.109.99-119"
}

@article{ref560_d90307,
    author = "Ifenthaler, Dirk",
    title = "Drivers for successful adoption of learning analytics",
    year = "2021"
}

@article{ref561_c01b17,
    author = "Manske, Sven and Cao, Yiwei",
    title = "Go-Lab Specifications of the Learning Analytics, Scaffolding Services, and Add-on Services -Final",
    year = "2015",
    abstract = "This deliverable describes the final specification of learning analytics, scaffolding, and add-on services of Go-Lab. It follows the initial specification reported in D4.2 (M18) and reflects the development and evaluation of the release in D4.4 (M24). It serves as a guideline for final release of the learning analytics, scaffolding, and add-on services in D4.8 (M36).  This deliverable consists of two major parts: (i) the learning analytics and scaffolding services and (ii) the add-on services including a booking system and a tutoring platform (which was called the Bartering Platform in D4.2).  The learning analytics and scaffolding services consist of a rich back and several options to develop new learning analytics apps according to the specified infrastructure and architecture. To support the two main stakeholders of learning analytics in Go-Lab, namely teachers and learners, a teacher dashboard and several apps, e.g. to support learners’ reflection, will be presented in this deliverable. Participatory design activities, particularly framed by the Go-Lab Summer School 2015, have been used to evaluate initial versions of the learning analytics apps to gather useful feedback aligned to the stakeholders.  The booking system offers Go-Lab remote labs an appropriate booking service using a calendar managed by lab owners. The tutoring platform helps build up a virtual community where teachers could share their expertise in inquiry learning with online labs and help each other grow their teaching skills."
}

@article{ref562_723513,
    author = "Ulfa, Saida and Fatawi, Izzul and Surahman, Ence",
    title = "Enhancing Students’ Learning Motivation Through Learning Analytics Approach in Online Learning System",
    year = "2019"
}

@article{ref563_7f2ce8,
    author = "Fleur, Damien and Marshall, Max and Pieters, Miguel and Brouwer, Nataša and Oomens, Gerrit and Konstantinidis, Angelos and Winnips, Koos and Moes, Sylvia and van den Bos, Wouter and Bredeweg, Bert and van Vliet, Erwin",
    title = "IguideME:",
    year = "2023",
    issn = "1929-7750",
    doi = "10.18608/jla.2023.7853",
    abstract = "Personalized feedback is important for the learning process, but it is time consuming and particularly problematic in large-scale courses. While automatic feedback may help for self-regulated learning, not all forms of feedback are effective. Social comparison offers powerful feedback but is often loosely designed. We propose that intertwining meaningful feedback with well-designed peer comparison using a learning analytics dashboard provides a solution. Third-year bachelor students were randomly assigned to have access to the learning analytics dashboard IguideME (treatment, n=31) or no access (control, n=31). Dashboard users were asked to indicate their desired grade, which was used to construct peer-comparison groups. Personalized peer-comparison feedback was provided via the dashboard. The effects were studied using quantitative and qualitative data, including the Motivated Strategies for Learning Questionnaire (MSLQ) and the Achievement Goal Questionnaire (AGQ). Compared to the control group, the treatment group achieved higher scores for the MSLQ components “metacognitive self-regulation” and “peer learning,” and for the AGQ component “other-approach” (do better than others). The treatment group performed better on reading assignments and achieved higher grades for high-level Bloom exam questions. These data support the hypothesis that personalized peer-comparison feedback can be used to improve self-regulated learning and academic achievement.",
    url = "https://doi.org/10.18608/jla.2023.7853"
}

@article{ref564_5ff6d3,
    author = "Curran, F. and Di Carlo, Stefano and Harris-Walls, Katharine",
    title = "Making the Data Visible: A Systematic Review of Systems-Level Data Dashboards for Leadership and Policy in Education",
    year = "2024",
    issn = "0034-6543",
    doi = "10.3102/00346543241288249",
    url = "https://doi.org/10.3102/00346543241288249"
}

@article{ref565_65cd38,
    author = "Foung, Dennis and Kohnke, Lucas",
    title = "Rediscovering the Uptake of Dashboard Feedback: A Conceptual Replication of Foung (2019)",
    year = "2022",
    issn = "2071-1050",
    doi = "10.3390/su142316169",
    abstract = "Learning analytics has been widely used in the context of language education. Among the studies that have used this approach, many have developed a dashboard that aims to provide students with recommendations based on data so that they can act on these suggestions and improve their performance. To further our understanding of dashboard research, this study aims to replicate an earlier study using a new data mining strategy, association rule mining, to explore if the new strategy can (1) generate comparable results; and (2) provide new insights into feedback uptake in dashboard systems. The original study was conducted with 423 students at a Hong Kong university and implemented a dashboard for a suite of first-year composition courses. It used a classification tree to identify factors that could predict the uptake of tool-based and general recommendations made by the dashboard. After performing association rule mining with the original data set, this study found that this approach allowed for the identification of additional useful factors associated with the uptake of general and tool-based recommendations with a higher accuracy rate. The results of this study provide new insights for dashboard research and showcase the potential use of association rule mining in the context of language education.",
    url = "https://doi.org/10.3390/su142316169"
}

@article{ref566_7a442d,
    author = "Bourguet, Marie-Luce",
    title = "Methodology for the Participatory Design of a Learner-Facing Analytics Dashboard",
    year = "2023",
    doi = "10.58459/icce.2023.1475",
    url = "https://doi.org/10.58459/icce.2023.1475"
}

@article{ref567_b04429,
    author = "Pan, Zilong and Li, Chenglu and Zou, Wenting and Liu, Min",
    title = "Applying Learning Analytics Approaches to Detect and Track Students' Cognitive States During Virtual Problem-Solving Activities",
    year = "2023",
    issn = "2326-8905",
    doi = "10.4018/978-1-6684-9527-8.ch002",
    url = "https://doi.org/10.4018/978-1-6684-9527-8.ch002"
}

@article{ref568_afbffb,
    author = "Liu, Qian and Gladman, Tehmina and Muir, Julia and Wang, Chen and Grainger, Rebecca",
    title = "Analytics-Informed Design: Exploring Visualization of Learning Management Systems Recorded Data for Learning Design",
    year = "2023",
    issn = "2158-2440",
    doi = "10.1177/21582440231193590",
    abstract = "One apparent challenge associated with learning analytics (LA) has been to promote adoption by university educators. Researchers suggest that a visualization dashboard could serve to help educators use LA to improve learning design (LD) practice. We therefore used an educational design approach to develop a pedagogically useful and easy-to-use LA visualization solution to support data-informed LD. We interviewed four staff in a medical degree program at a New Zealand university, designed and piloted a dashboard, and evaluated it through interviews. As a proof-of-concept project, our study showed that educational design research could be meaningfully used to develop a visualization dashboard that is easy-to-use and useful. In particular, the preliminary design principles identified provide implications for practitioners who are seeking to use LA to inform LD. Finally, we reflect on the purpose of visualization dashboards in relation to the literature and identify areas for future developments.",
    url = "https://doi.org/10.1177/21582440231193590"
}

@article{ref569_377a89,
    author = "CORBU, EMILIA",
    title = "CĂTRE UN MODEL BAZAT PE SENZORI ÎN ANALIZA ÎNVĂȚĂRII",
    year = "2022",
    doi = "10.56177/epvl.cap10.2022.ro",
    url = "https://doi.org/10.56177/epvl.cap10.2022.ro"
}

@article{ref570_4cbb43,
    author = "Urrutia, Manuel and Wilde, Adriana and Tang, Darron and Cheng, Jasmine",
    title = "The MOOC Dashboard: Visualising MOOC data for everyone",
    year = "2016"
}

@article{ref571_3ed732,
    author = "Ritz, Eva and Grueneke, Timo",
    title = "Learn Smarter, Not Harder – Exploring the Development of Learning Analytics Use Cases to Create Tailor-Made Online Learning Experiences",
    year = "2022",
    issn = "1530-1605",
    doi = "10.24251/hicss.2022.114",
    abstract = "Our world is significantly shaped by digitalization, fostering new opportunities for technology-mediated learning.Therefore, massive amounts of knowledge become available online.However, concurrently these formats entail less interaction and guidance from lecturers.Thus, learners need to be supported by intelligent learning tools that provide suitable knowledge in a tailored way.In this context, the use of learning analytics in its multifaceted forms is essential.Existing literature shows a proliferation of learning analytics use cases without a systematic structure.Based on a structured literature review of 42 papers we organized existing literature contributions systematically and derived four use cases: learning dashboards, individualized content, tutoring systems, and adaptable learning process based on personality.Our use cases will serve as a basis for a targeted scientific discourse and are valuable orientation for the development of future learning analytics use cases to give rise to the new form of Learning Experience Platforms.",
    url = "https://doi.org/10.24251/hicss.2022.114"
}

@article{ref572_1530ff,
    author = "Wiley, Korah",
    title = "Developing Learning Analytics to Promote Knowledge Integration in a Technology-enhanced Learning Environment",
    year = "2020"
}

@article{ref573_44356f,
    author = "Toyokawa, Yuko and Prasad, Prajish and Horikoshi, Izumi and Majumdar, Rwitajit and Ogata, Hiroaki",
    title = "Supporting program comprehension with data-enhanced active reading",
    year = "2025",
    issn = "1793-2068",
    doi = "10.58459/rptel.2025.20039",
    abstract = "This study proposes a novel pedagogical approach to applying active reading (AR) strategies and the learning analytics dashboard to program comprehension (PC) in programming courses. The objective was to visualize the code-reading behaviors of novice programming learners using learning logs and promote code comprehension. The strategy was applied to students in computer science classes at a liberal arts college in India. The results show that the utilization of the dashboard positively influenced students’ learning behaviors outside the classroom. It was recognized as an effective means of supporting PC, highlighting the need to elaborate on how to adopt dashboards in code reading tasks. The study confirms that reflecting on learning using the dashboard can promote learners’ metacognitive skills, regardless of subject or language. It contributes to AR research by demonstrating new practical benefits of AR strategies for PC.",
    url = "https://doi.org/10.58459/rptel.2025.20039"
}

@article{ref574_5cf1a6,
    author = "Rohloff, Tobias",
    title = "Learning analytics at scale : supporting learning and teaching in MOOCs with data-driven insights",
    year = "2021",
    doi = "10.25932/publishup-52623",
    url = "https://doi.org/10.25932/publishup-52623"
}

@article{ref575_e239b2,
    author = "Campen, C.A.N. and Molenaar, Inge and Hasselman, Fred",
    title = "The effects of learning analytics empowered technology on the students' arithmetic skills learning",
    year = "2017"
}

@article{ref576_1e38d1,
    author = "Koh, Elizabeth and Hu, Xiao",
    title = "Learning Analytics for Learning: Emerging International Trends and Case Studies from the Asia-Pacific",
    year = "2023",
    doi = "10.1007/978-981-19-6887-7\_54",
    url = "https://doi.org/10.1007/978-981-19-6887-7\_54"
}

@article{ref577_bcf64a,
    author = "Popescu, Elvira",
    title = "Plenary Talk 4 : Learning Analytics – A Multiple Perspectives Analysis of Student Data",
    year = "2021",
    doi = "10.1109/miucc52538.2021.9447614",
    url = "https://doi.org/10.1109/miucc52538.2021.9447614"
}

@article{ref578_439433,
    author = "Vas, Réka and Szücs, Patrícia",
    title = "EXPLORING THE POSSIBILITIES OF LEARNING ANALYTICS IN HIGHER EDUCATION",
    year = "2020",
    issn = "2340-1079",
    doi = "10.21125/inted.2020.2325",
    url = "https://doi.org/10.21125/inted.2020.2325"
}

@article{ref579_5f335a,
    author = "O’Riordan, Tim",
    title = "How should we measure and show online learning",
    year = "2015"
}

@article{ref580_ade382,
    author = "Majumdar, Rwitajit and Warriem, Jayakrishnan and Kuromiya, Hiroyuki and APINAR, Gökhan and Flanagan, Brendan and Ogata, Hiroaki",
    title = "Learning Evidence Analytics Framework (LEAF) in Practice: A2I2 based Teacher Adoption Approach",
    year = "2019",
    doi = "10.58459/icce.2019.334",
    url = "https://doi.org/10.58459/icce.2019.334"
}

@article{ref581_4e1f11,
    author = "Rienties, Bart and Herodotou, Christothea",
    title = "Making sense of learning data at scale",
    year = "2022",
    doi = "10.4337/9781800888494.00032",
    abstract = "In many higher educational institutions teachers and managers have unprecedented levels of access to data of learners and their learning. In particular during COVID-19 teachers suddenly had to make sense of what their learners were doing away from the classroom. In this chapter we will provide a range of evidence-based guidelines and best-practices based upon implementing online learning and making sense of learning data on a large-scale institutional level. The Open University UK has been implementing learning analytics at scale and has made learning analytics dashboards available to thousands of teachers. How to make sense of these interactive data streams is complex at the best of times, especially in a situation when most teachers are working remotely. We will reflect on our lived experiences of helping teachers to make sense of learning data, and how their voices are essential for senior managers to build powerful data practices.",
    url = "https://doi.org/10.4337/9781800888494.00032"
}

@article{ref582_cb70e3,
    author = "Chaudy, Yaëlle and Connolly, Thomas",
    title = "Integrating Assessment, Feedback, and Learning Analytics in Educational Games",
    year = "2021",
    doi = "10.4018/978-1-6684-3710-0.ch088",
    url = "https://doi.org/10.4018/978-1-6684-3710-0.ch088"
}

@article{ref583_c9e7c4,
    author = "Davalos, Eduardo and Srivastava, N. and Zhang, Yike and GOODWIN, Amanda and Biswas, Gautam",
    title = "GazeViz: A Web-Based Approach for Visualizing Learner Gaze Patterns in Online Educational Environment",
    year = "2024",
    doi = "10.58459/icce.2024.4974",
    url = "https://doi.org/10.58459/icce.2024.4974"
}

@article{ref584_cab767,
    author = "Becerra, Álvaro and Daza, Roberto and Cobos, Ruth and Morales, Aythami and Fiérrez, Julián",
    title = "Estudio de la Experiencia de Usuario mediante un Sistema de Dashboards de Análisis de Aprendizaje Multimodal",
    year = "2023",
    doi = "10.48550/arxiv.2307.10346",
    abstract = "In the article, we present a Web-based System called M2LADS, which supports the integration and visualization of multimodal data recorded in user experiences (UX) in a Learning Analytics (LA) system in the form of Web-based Dashboards. Based on the edBB platform, the multimodal data gathered contains biometric and behavioral signals including electroencephalogram data to measure learners' cognitive attention, heart rate for affective measures and visual attention from the video recordings. Additionally, learners' static background data and their learning performance measures are tracked using LOGGE tool. M2LADS provides opportunities to capture learners' holistic experience during their interactions with the learning analytic system in order to improve the system and the user experience of the learners. -- En este art\'iculo, presentamos M2LADS, un sistema que permite la integraci\'on y visualizaci\'on de datos multimodales en forma de Dashboards Web. Estos datos provienen de sesiones de experiencia de usuario en un sistema de Learning Analytics (LA) llevadas a cabo por estudiantes de MOOCs. Los datos multimodales incluyen se\\textasciitilde nales biom\'etricas y de comportamiento monitorizados por la plataforma edBB, como electroencefalogramas (EEG) de 5 canales, frecuencia card\'iaca, atenci\'on visual, videos en el espectro visible y NIR, entre otros. Adem\'as, se incluyen datos de interacci\'on de los estudiantes con el sistema de LA a trav\'es de la herramienta LOGGE. Toda esta informaci\'on proporciona una comprensi\'on completa de la experiencia del usuario al utilizar el sistema de LA, lo que ha permitido tanto mejorar el sistema LA como la experiencia de aprendizaje de los estudiantes de MOOCs.",
    url = "https://doi.org/10.48550/arxiv.2307.10346"
}

@article{ref585_bbb7f8,
    author = "Johnson, Connie and Burrington, Debra and O’Donnell, Karen and Wren, J.",
    title = "Learning Analytics: The Impact of Digital Tools in Upper-Level and Graduate Courses",
    year = "2024",
    issn = "2994-7502",
    doi = "10.61643/c47674",
    abstract = "Colorado Technical University (CTU) harnesses adaptive learning, accompanied by learning analytics in online courses that engage thousands of students quarterly. Approximately 600 CTU faculty have participated in the use of adaptive learning technology in the classroom. Learning analytics applied to information gleaned from data dashboards provides visibility and insight concerning our students’ level of progress when enrolled in a course. This study focused on learning analytics capturing student success factors, including student engagement, timeliness of assignment submission, assignment grades, and student progression (persistence to next term) in four upper-level and graduate business courses. Additional information analyzed consisted of CTU Messenger data, end-of-course surveys with insights extracted from instant messages, and recorded student and instructor feedback throughout a course. Recommended strategies informed by findings that can lead to improved student outcomes are also detailed.",
    url = "https://doi.org/10.61643/c47674"
}

@article{ref586_54ba2e,
    author = "السلمي, خلود and الجندي, علياء",
    title = "توظيف تحليلات التعلم في بيئات التعلم عبر الإنترنت ما بين 2018- 2022: مراجعة منهجية",
    year = "2024",
    issn = "2535-2083",
    doi = "10.21608/jsrep.2024.339190",
    abstract = "The aim of the current study was to explore the practical use of learning analytics in online learning environments from 2018 to 2022.This study employed a systematic review approach to scrutinize previous studies that incorporated learning analytics in online learning environments within the specified time frame.A total of 86 studies were selected for review, which included four in Arabic and 82 in English.The study found that Learning Management Systems (LMS) were the most prevalent platforms for utilizing learning analytics in both Arabic and English studies.Performance data emerged as the most commonly used type of data in Arabic studies.In English-language studies learner interaction data had the highest percentage.The objectives of using learning analytics showed variation in Arabic studies, while monitoring and analysis were the primary objectives in English studies.Data mining was the preferred methods of data analysis in Arabic studies, with data mining being the most utilized method in English studies.The study revealed that both researcher and learners were equally represented as stakeholders in Arabic studies.However, in English studies, teachers were more prominently represented.Based on the findings of the study, several recommendations were proposed.These include the design of intelligent learning environments that leverage learning analytics to personalize and adapt the learning process.It is also recommended to adopt a modern approach that places the learner at the core of the educational process by employing learning analytics (dashboards) in personalized learning settings.",
    url = "https://doi.org/10.21608/jsrep.2024.339190"
}

@article{ref587_94fa93,
    author = "Munde, Vyankat and Kumar, Bınod and Vaidya, Anagha and Shirwaikar, Shailaja",
    title = "Analytics Dashboard on Talent search Examination Data using Structure of Intellect Model",
    year = "2021",
    issn = "0976-5034",
    doi = "10.47164/ijngc.v12i2.766",
    url = "https://doi.org/10.47164/ijngc.v12i2.766"
}

@article{ref588_ac44e8,
    author = "Lei, Tao and Song, Yanjie",
    title = "Developing a Multimodal Learning Analytics Approach for Collaborative Learning and Metacognitive Strategies in Virtual Learning Environments for Primary Science Education",
    year = "2024",
    doi = "10.58459/icce.2024.5050",
    url = "https://doi.org/10.58459/icce.2024.5050"
}

@article{ref589_10c891,
    author = "Misiejuk, Kamila and López‐Pernas, Sonsoles and Kaliisa, Rogers and Saqr, Mohammed",
    title = "Mapping the Landscape of Generative Artificial Intelligence in Learning Analytics",
    year = "2025",
    issn = "1929-7750",
    doi = "10.18608/jla.2025.8591",
    abstract = "Generative artificial intelligence (GenAI) has opened new possibilities for designing learning analytics (LA) tools, gaining new insights about student learning processes and their environment, and supporting teachers in assessing and monitoring students. This systematic literature review maps the empirical research of 41 papers utilizing GenAI and LA and interprets the results through the lens of the LA/EDM process cycle. Currently, GenAI is mostly implemented to automate discourse coding, scoring, or classification tasks. Few papers used GenAI to generate data or to summarize text. Classroom integrations of GenAI and LA mostly explore facilitating human–GenAI collaboration, rather than implementing automated feedback generation or GenAI-powered learning analytics dashboards. Most papers use Generative Adversarial Network models to generate synthetic data, BERT models for classification or prediction tasks, BERT or GPT models for discourse coding, and GPT models for tool integration. Although most studies evaluate the GenAI output, we found examples of using GenAI without the output validation, especially when its output feeds into an LA pipeline aiming to, for example, develop a dashboard. This review offers a comprehensive overview of the field to aid LA researchers in the design of research studies and a contribution to establishing best practices to integrate GenAI and LA.",
    url = "https://doi.org/10.18608/jla.2025.8591"
}

@article{ref590_338f18,
    author = "Pozdniakov, Stanislav and Martínez‐Maldonado, Roberto and Tsai, Yi‐Shan and Echeverría, Vanessa and Swiecki, Zachari and Gašević, Dragan",
    title = "Investigating the Effect of Visualization Literacy and Guidance on Teachers' Dashboard Interpretation",
    year = "2025",
    issn = "1929-7750",
    doi = "10.18608/jla.2024.8471",
    abstract = "Recent research on learning analytics dashboards has focused on designing user interfaces that offer various forms of \textit{visualization guidance} (often referring to notions such as \textit{data storytelling} or \textit{narrative visualization}) to teachers (e.g., emphasizing data points or trends with colour and adding annotations), aiding them in interpreting visual elements to gain a comprehensive understanding of students' learning processes. Yet, while some studies have explored how teachers interpret students' data through these dashboards, many have overlooked the diverse technical capabilities of teachers, which can significantly impact their use of LA dashboards. In particular, \textit{visualization literacy} (VL) skills can greatly influence how effectively teachers interpret dashboards. To the best of our knowledge, no comprehensive account exists that details how teachers with varying VL skills interpret visual representations of students' data. In this paper, we address this gap by investigating how teachers interpret LA dashboards, both with and without visualization guidance, taking into account their VL. We illustrate this by analyzing teachers' think-aloud sessions as they engage with dashboards in the context of monitoring synchronous online learning tasks undertaken by student groups using Zoom and Google Docs. Using epistemic network analysis, we examine the differences in interpretations between teachers with varying VL levels. Our findings revealed that teachers with low VL exhibited shallower dashboard interpretations than those with high VL. However, the association of VL with successful task completion rate was not significant. Also, visualization guidance did not enable teachers to deepen their interpretations. While some visualization guidance helped teachers to complete tasks correctly, excessive visualization guidance can also be detrimental.",
    url = "https://doi.org/10.18608/jla.2024.8471"
}

@article{ref591_c6ea9a,
    author = "Adijaya, Bayu and Kusumawardani, S.T. and Aji, M.T.",
    title = "LEARNING ANALYTICS: IMPLEMENTASI DAN VISUALISASI MACHINE LEARNING UNTUK MEMPREDIKSI HASIL BELAJAR PADA LEARNING MANAGEMENT SYSTEM (LMS) / MODEL REGRESI",
    year = "2020"
}

@article{ref592_6d9fb4,
    author = "Brun, Armelle and Bonnin, Geoffray and Castagnos, Sylvain and Roussanaly, Azim and Boyer, Anne",
    title = "Learning Analytics Made in France: The METALproject",
    year = "2019",
    doi = "10.48550/arxiv.1904.02528",
    abstract = "This paper presents the METAL project, an ongoing French open Learning Analytics (LA) project for secondary school, that aims at improving the quality of the learning process. The originality of METAL is that it relies on research through exploratory activities and focuses on all the aspects of a Learning Analytics implementation. This large-scale project includes many concerns, divided into 4 main actions. (1) data management: multi-source data identification, collection and storage, selection and promotion of standards, and design and development of an open-source Learning Record Store (LRS); (2) data visualization: learner and teacher dashboards, with a design that relies on the co-conception with final users, including trust and usability concerns; (3) data exploitation: study of the link between gaze and memory of learners, design of explainable multi-source data-mining algorithms, including ethics and privacy concerns. An additional key of originality lies in the global dissemination of LA at an institution level or at a broader level such as a territory, at the opposite on many projects that focus on a specific school or a school curriculum. Each of these aspects is a hot topic in the literature. Taking into account all of them in a holistic view of education is an additional added value of the project.",
    url = "https://doi.org/10.48550/arxiv.1904.02528"
}

@article{ref593_6d9fb4,
    author = "Brun, Armelle and Bonnin, Geoffray and Castagnos, Sylvain and Roussanaly, Azim and Boyer, Anne",
    title = "Learning Analytics Made in France: The METALproject",
    year = "2019"
}

@article{ref594_477301,
    author = "Dyulicheva, Yulia",
    title = "Application of Learning Analytics in Higher Education: Datasets, Methods and Tools",
    year = "2024",
    issn = "0869-3617",
    doi = "10.31992/0869-3617-2024-33-5-86-111",
    abstract = "The accumulation of big educational data on the platforms of universities and social media leads to the need to develop tools for extracting regularities from educational data, which can be used for understanding the behavioral patterns of students and teachers, improve teaching methods and the quality of the educational process, as well as form sound strategies and policies for universities development. This article provides an analysis and systematization of datasets on available repositories, taking into account the learning analytics problems solved on their basis. In particular, the article notes the predominance of datasets aimed at solving analytical problems at the level of student’s behavior understanding, Datasets aimed at solving analytical problems at the level of understanding the needs of teachers and administrative and managerial staff of universities are practically absent. Meanwhile, the full potential of learning analytics tools can only be revealed by introducing an integrated approach to the analysis of educational data, taking into account the needs of all participants and organizers of the educational process. This review article discusses learning analytics methods related to the study of social interaction patterns between students and teachers, and learning analytics tools from the implementation of simple dashboards to complex frameworks that explore various levels of learning analytics. The problems and limitations that prevent learning analytics from realizing its potential in universities are considered. It is noted that universities are generally interested in introducing learning analytics tools that can improve the quality of the educational process by developing strategies for targeted support for individual groups of students, however, teachers treat such initiatives with caution due to a lack of data analysis skills and correct interpretation of analysis results. The novelty of this analytical review is associated with the consideration of learning analytics at different levels of its implementation in the context of approaches to openness, processing and analysis of educational data. This article will be of interest to developers of learning analytics tools, scientific and pedagogical workers, and administrative and managerial staff of universities from the point of view of forming an idea of the integrity of the university analytics process, taking into account various levels of analytics implementation aimed at understanding the needs and requirements of all participants in the educational process.",
    url = "https://doi.org/10.31992/0869-3617-2024-33-5-86-111"
}

@article{ref595_f1ceca,
    author = "Sadallah, Madjid",
    title = "Models and Tools for Usage-based e-Learning Documents Reengineering",
    year = "2019",
    abstract = "Providing high-quality content is of utmost importance to drive successful reading. Besides, designing documents that are received the way the author wishes has always been difficult, and the digital world increases this difficulty by multiplying the possibilities related to mixed medias and interactivity. This compels authors to continuously review the delivered content to meet readers’ needs. Yet it remains challenging for them to detect the comprehension barriers that may exist within their documents, and to identify how these latter can be improved accordingly. This compels authors to continuously review the delivered content to meet readers’ needs. Yet it remains challenging for them to detect the comprehension barriers that may exist within their documents, and to identify how these latter can be improved accordingly. In this thesis, we focus on an educational context, where reading is a fundamental activity and the basis of many other learning activities. We propose a learning analytics approach for assisting course authors to maintain their courses to sustain learning. The proposals are based on theoretical background originated from research on learning analytics, reading comprehension and content revision. We advocate “usage-based document reengineering”, a process defined as a kind of reengineering that changes document content and structures based on the analysis of readers’ usages as recorded in their reading traces. We model reading activity using the concept of reading-session and propose a new session identification method. Using learners’ reading sessions, a set of indicators related to different aspects of the reading process are computed and used to detect comprehension issues and to suggest corrective content revisions. The results of the analytics process are presented to authors through a dashboard empowered with assistive features. We instantiate our proposals using the logs of a major European e-learning platform, and validate it through a series of studies. The results show the effectiveness of the approach and related dashboards to enhance authors awareness learners’ needs, and to provide them with guidance in improving their courses accordingly."
}

@article{ref596_56fb95,
    author = "Mahmud, Malissa and Ramli, Nor and Zakaria, Siti and Rusli, Rusreena and Manap, Mohammad and Wong, Shiau",
    title = "Learning analytics and tech-tools: Insights of the practical implications for stakeholders’ perspectives",
    year = "2023",
    issn = "2224-4441",
    doi = "10.55493/5007.v13i10.4896",
    abstract = "The aim of the current study is to perform a systematic review of the literature to determine how learning analytics and technological tools used in education relate to one another. The review looked at 30 samples from 15 (n=15) academic databases, and found that the recent learning analytics research typically used web-based applications, Web 2.0 tools, dashboard and visualization tools, and eye-tracking devices. These technological tools, such as learning management systems and social networking websites, are widely employed in order to facilitate online learning and communication as well as to analyze and visualize data for informed decision-making. One of the practical implications of this research for learning analytics stakeholders, such as educators, policymakers, and researchers, is the ability to use these technologies tools to enhance teaching and learning. By utilizing these tools, policymakers can learn information that can be applied to the development of strategies and policies relating to the integration of technology in education. Researchers can utilize these tools to generate data that will aid in the study of teaching and learning, which will lead to the development of new teaching strategies and technological advancements. Stakeholders must establish appropriate policies and practices, be aware of the potential risks and limitations associated with their usage, and work to ensure that these tools are used in the classroom in an ethical and efficient manner. This study shows, in general, how effectively these technological tools can enhance teaching and learning when used in educational settings.",
    url = "https://doi.org/10.55493/5007.v13i10.4896"
}

@article{ref597_2a6e53,
    author = "Nöel, Yves and Bouchet, François and Mergoil, Roland and Luengo, Vanda",
    title = "Towards a modular and flexible Learning Analytics framework",
    year = "2020",
    abstract = "This paper introduces a Learning Analytics platform which aims at being modular, evolving and flexible. The general framework architecture is completely independent from the digital systems to which it is connected. It collects learning data of various origins in data storages. Then it extracts a subset of the data which is aggregated into a data warehouse. Finally, these data are processed through various algorithms. Such a framework reinforces the control of data integrity in an experimental context and allows the students to refine the authorizations they give about their data. These data processing lead to indicators that will be used in student and teacher dashboards allowing a clear and fast access to learning information. In a second step, the platform will compute student profiles, facilitating the design of adaptive courses for each student."
}

@article{ref598_b6ce3f,
    author = "Dale, Nell and Weems, Chip",
    title = "Navigate 2 Advantage Access For Programming \& Problem Solving With C++: Comprehensive",
    year = "2014"
}

@article{ref599_9546eb,
    author = "Schumacher, Clara",
    title = "Cognitive, metacognitive and motivational perspectives on learning analytics : Synthesizing self-regulated learning, assessment, and feedback with Learning Analytics",
    year = "2019"
}

@article{ref600_997826,
    author = "Kuromiya, Hiroyuki and Majumdar, Rwitajit and Horikoshi, Izumi and Ogata, Hiroaki",
    title = "Learning analytics for student homework activities during a long break: Evidence from K-12 education in Japan",
    year = "2024",
    issn = "1793-2068",
    doi = "10.58459/rptel.2024.19034",
    abstract = "Learning Analytics (LA) is an emergent field that aims to better understand students and provide intelligence to learners, teachers, and administrators using learning log data. Although the use of technology in class is increasing in the K-12 sector and tertiary education, cases of effective implementation of LA in secondary schools have rarely been reported. This study offers an example of LA implemented in a junior high Math class during long vacations in Japan. This paper comprises two studies: first, we analyzed 121 students’ answer logs and their exam performance after vacation by the K-means clustering method. We found that students’ progress patterns were categorized into four types of engagement—early, late, high, and low—and the early and high-engagement groups obtained significantly higher scores than the low-engagement group. In the second study, we implemented a real-time dashboard that visualizes students’ progress patterns and gives students insights about their progress during the vacation period. We found that the dashboard significantly increased students’ interactions with the assignment, and the questionnaire survey determined that the LA dashboard motivated students to learn during the long vacation period. Considering the previous studies of LA, we estimate that LA-based interventions enhance students’ self-regulation skills, which is crucial for learning during long vacation periods. Our study offers a novel approach to implementing LA in K-12 education.",
    url = "https://doi.org/10.58459/rptel.2024.19034"
}

@article{ref601_d2a8ac,
    author = "Battaglin, Ricardo and Muñoz, Roberto and Ramos, Vinícius and Cechinel, Cristian",
    title = "Predicting at-risk students with LMS data: a comparison between Adaboost and LSTM algorithms",
    year = "2022",
    doi = "10.1109/laclo56648.2022.10013469",
    url = "https://doi.org/10.1109/laclo56648.2022.10013469"
}

@article{ref602_5375d7,
    author = "Geng, Xuewang and Yamada, Masanori",
    title = "The Role of Dashboards in Augmented-Reality-Based Language Learning: Enhancing Language Learning and Metacognitive Awareness",
    year = "2025",
    issn = "2158-2440",
    doi = "10.1177/21582440251341675",
    abstract = "The development of information and communication technologies has created limitless prospects for using augmented reality (AR) in various fields. Unfortunately, the multitasking nature of AR systems prevents learners from successfully reflecting and retaining knowledge. This study developed and designed a learning analytics dashboard (LAD) with three components: personal learning data, social comparison, and visualization of the learning process, to promote knowledge acquisition and metacognitive awareness among AR learners. A total of 31 intermediate-level Japanese learners participated in an experiment involving pre-tests, post-tests, and a delayed test to assess the LAD. Learners engaged with the AR learning system for Japanese compound verb learning and utilized the LAD to monitor and reflect on their AR learning activities. Behavioral data were analyzed using Lag Sequential Analysis (LSA), while learning performance was evaluated through one-way repeated-measures ANOVA tests. The findings indicate that the use of the LAD significantly improves learning outcomes and metacognitive processes, such as knowledge of cognition and regulation of cognition. Additionally, there were different usage patterns of the dashboard among learners, which corresponded to significant differences in their learning outcomes and changes in metacognitive awareness. Learners who primarily focused on the learning process and social comparison components of the dashboard demonstrated improved knowledge retention. Conversely, those who mainly concentrated on personal learning data experienced the most significant gains in metacognitive awareness. This study also provides crucial design insights on the integration of dashboard components with cognitive efforts to maximize learning outcomes.",
    url = "https://doi.org/10.1177/21582440251341675"
}

@article{ref603_14cf21,
    author = "Hilton, Amy",
    title = "Board 429: Work in Progress: Capacity-Building for Change Through Faculty Communities Exploring Data and Sharing Their Stories",
    year = "2024",
    doi = "10.18260/1-2--47019",
    abstract = "Abstract This NSF Improving Undergraduate STEM Education (IUSE: EHR) Institutional and Community Transformation (ICT) capacity-building project is designed to support faculty to collaboratively explore questions on student learning and success in introductory and gateway undergraduate STEM courses, such as early engineering courses as well as prerequisite math and science courses. The project motivates faculty to consider evidence-based teaching strategies by including them as co-designers of learning analytics tools and storytellers inspired by data and their reflections. Learning analytics uses data about learners and learning to draw inferences to inform actions and changes to achieve a goal, which for this project is improving student success and retention in early STEM courses. Learning analytics is an emerging approach to motivating STEM faculty to implement evidence-based teaching practices. The project also builds and strengthens faculty communities and develops a culture of inquiry and conversations that are evidence-based and data-informed – all to build readiness for transformation. We are exploring how a change framework for intentional capacity building by creating faculty communities with similar interests across disciplines and course-level data dashboards can establish the foundation for implementing change in their instructional practices and curriculum, with faculty members becoming change agents. While most transformation projects and use of learning analytics have been conducted at large research institutions, the findings from this project will contribute to the knowledge of engineering education change in the context of a public, regional, primarily undergraduate institution in the Midwest. This paper describes the grounding, planning, and implementation of these activities to build capacity for change and shares the challenges encountered and strategies used.",
    url = "https://doi.org/10.18260/1-2--47019"
}

@article{ref604_383998,
    author = "Jones, Kyle",
    title = "Learning Analytics and Higher Education: A Proposed Model for Establishing Informed Consent Mechanisms to Promote Student Privacy and Autonomy",
    year = "2019",
    issn = "1556-5068"
}

@article{ref605_b98f58,
    author = "Wardhono, Wibisono and Kharisma, Agi and Jonemaro, Eriq",
    title = "Pengembangan Desain Solusi Dasbor Learning Analytics sebagai Input pada Model Personalized Learning",
    year = "2022",
    issn = "2964-8653",
    doi = "10.57096/edunity.v1i04.23",
    url = "https://doi.org/10.57096/edunity.v1i04.23"
}

@article{ref606_b9eb03,
    author = "Mohseni, Zeynab",
    title = "Development of Visual Learning Analytic Tools to Explore Performance and Engagement of Students in Primary, Secondary, and Higher Education",
    year = "2024",
    doi = "10.15626/lud.532.2024",
    abstract = "Schools and educational institutions collect large amounts of data about students and their learning, including text, grades, quizzes, timestamps, and other activities. However, in primary and secondary education, this data is often dispersed across different digital platforms, lacking standardized methods for collection, processing, analysis, and presentation. These issues hinder teachers and students from making informed decisions or strategic and effective use of data. This presents a significant obstacle to progress in education and the effective development of Educational Technology (EdTech) products. Visual Learning Analytics (VLA) tools, also known as Learning Analytics Dashboards (LADs), are designed to visualize student data to support pedagogical decision-making. Despite their potential, the effectiveness of these tools remains limited. Addressing these challenges requires both technical solutions and thoughtful design considerations, as explored in Papers 1 through 5 of this thesis. Paper 1 examines the design aspects of VLA tools by evaluating higher education data and various visualization and Machine Learning (ML) techniques. Paper 2 provides broader insights into the VLA landscape through a systematic review, mapping key concepts and research gaps in VLA and emphasizing the potential of VLA tools to enhance pedagogical decisions and learning outcomes. Meanwhile, Paper 3 delves into a technical solution (data pipeline and data standard) considering a secure Swedish warehouse, SUNET. This includes a data standard for integrating educational data into SUNET, along with customized scripts to reformat, merge, and hash multiple student datasets. Papers 4 and 5 focus on design aspects, with Paper 4 discussing the proposed Human-Centered Design (HCD) approach involving teachers in co-designing a simple VLA tool. Paper 5 introduces a scenario-based framework for Multiple Learning Analytics Dashboards (MLADs) development, stressing user engagement for tailored LADs that facilitate informed decision-making in education. The dissertation offers a comprehensive approach to advancing VLA tools, integrating technical solutions with user-centric design principles. By addressing data integration challenges and involving users in tool development, these efforts aim to empower teachers in leveraging educational data for improved teaching and learning experiences.",
    url = "https://doi.org/10.15626/lud.532.2024"
}

@article{ref607_6bb9f7,
    author = "Banihashem, Seyyed and Gašević, Dragan and Noroozi, Omid",
    title = "A Critical Review of Using Learning Analytics for Formative Assessment: Progress, Pitfalls and Path Forward",
    year = "2025",
    issn = "0266-4909",
    doi = "10.1111/jcal.70056",
    url = "https://doi.org/10.1111/jcal.70056"
}

@article{ref608_98ac25,
    author = "Amarasinghe, S. and Thalakumbura, T. and Wijewardena, M.D.N.K. and Perera, Dulshan and Manathunga, Kalpani and Senaweera, Oshada",
    title = "Remotify: The Emergency Remote Learning Solution using Learning Analytics",
    year = "2022",
    doi = "10.1109/i2ct54291.2022.9824707",
    url = "https://doi.org/10.1109/i2ct54291.2022.9824707"
}

@article{ref609_11b2e1,
    author = "Augustijn, P.W.M. and Verkroost, M.J. and de Oliveira, Ivan",
    title = "Gardening with the living textbook: navigation via learning pathways and a concept map",
    year = "2020"
}

@article{ref610_7b31b1,
    author = "Jabeen, Zahra and Mishra, Khushboo and Dayal, Rajeshwar and Mishra, Binay",
    title = "Transforming Education in the World of Artificial Intelligence",
    year = "2024",
    issn = "3046-403X",
    doi = "10.62486/latia2024113",
    url = "https://doi.org/10.62486/latia2024113"
}

@article{ref611_c3c8b5,
    author = "Laksitowening, Kusuma and Fahrudin, Tora and Insani, Rokhmatul and Umar, U",
    title = "Incorporating Learning Analytics and Business Intelligence into Higher Education E-Learning",
    year = "2025",
    issn = "2477-698X",
    doi = "10.23917/khif.v10i2.4142",
    abstract = "Business Intelligence (BI) represents a pivotal advancement in leveraging information technology to enhance organizational performance. BI tools serve as crucial aids in decision-making processes by furnishing requisite insights. In higher education institutions, BI can contribute to leaders and managers in providing perspectives related to academics, learning, and management. Central to BI development is the meticulous gathering of requirements, a process pivotal in identifying organizational informational and knowledge needs. This involves employing various methods such as interviews, observation, and analysis, including leveraging learning analytics to discern data utility for enhanced learning processes. Various studies show that learning analytics contributes to improving the learning and education process. On the other hand, learning analytics requires activity data that is integrated, subject oriented, and time series which are aligned with the characteristics of the data warehouse (DWH) as the main component of BI. This research endeavors to develop BI utilizing academic and e-learning data, exemplified through a case study of Telkom University's Academic Systems and Learning Management Systems (LMS). This study aims to provide actionable insights into the intersection of BI and learning analytics, ultimately enhancing educational processes and organizational decision-making capabilities. By integrating learning analytics into BI development, the resultant BI systems can cater not only to current managerial demands but also anticipate future analytical needs. The implementation of the multidimensional schema was successfully executed. This process involved mapping data from the academic information system and the LMS as data sources to the data warehouse, the Extract, Transform, and Load (ETL) process, and development of the prototype. The testing on the prototype indicated that the prototype meets the intended requirements and provides valuable insights through its comprehensive reporting capabilities. This demonstrates the effectiveness of the implemented multidimensional schema, ETL process, and the overall design of the reporting dashboard.",
    url = "https://doi.org/10.23917/khif.v10i2.4142"
}

@article{ref612_511645,
    author = "Shaikh, Mohammad and Ali, Syed",
    title = "The Joint Creation of an Instructor Dashboard for Online Learning Environments in College and University",
    year = "2025",
    doi = "10.4018/979-8-3693-8593-7.ch032",
    url = "https://doi.org/10.4018/979-8-3693-8593-7.ch032"
}

@article{ref613_f05e3b,
    author = "Gourlay, Lesley",
    title = "Is A Star A Document? Catalogued Students and Learning Analytics",
    year = "2024",
    issn = "2524-485X",
    doi = "10.1007/s42438-024-00489-x",
    abstract = "Abstract The media theorist Suzanne Briet proposed that through the recording of information about entities in the world, these entities are not only documented, but they themselves are rendered into documents . She asks us to consider the case of an antelope which is captured, brought to Europe, put in a zoo, and examined by experts and members of the public. She argues that the zoo is effectively a laboratory in which the antelope is analysed, displayed, and therefore itself becomes a document due to these material analytical assemblages around it. In this paper, I propose that Briet’s notion of the document can be applied to data visualisation used in learning analytics, and its effect on students. With reference to a philosophical discussion of the status of data visualisation in terms of Kant’s theory of the sublime versus Deleuze’s notion of the diagram, I argue that a learning analytics dashboard designed for individual student use not only renders the student into a document but also imbricates the student in a co-constitutive form of relationality with that document, which explicitly encourages and rewards a very particular form of action in the world in relation to the learning management system. I conclude that this has real-world effects not only in this inculcation, but in the reification of a particular neoliberal ideology of student engagement as a performance of observable, traceable, self-optimisation in a highly individualised educational worldview.",
    url = "https://doi.org/10.1007/s42438-024-00489-x"
}

@article{ref614_7d1abd,
    author = "Miriam, Navarro and Álvaro, Becerra and Roberto, Daza and Cobos, Ruth and Aythami, Morales and Julian, Fierrez",
    title = "Visual Attention Analysis in Online Learning",
    year = "2024",
    doi = "10.48550/arxiv.2405.20091",
    abstract = "In this paper, we present an approach in the Multimodal Learning Analytics field. Within this approach, we have developed a tool to visualize and analyze eye movement data collected during learning sessions in online courses. The tool is named VAAD (an acronym for Visual Attention Analysis Dashboard). These eye movement data have been gathered using an eye-tracker and subsequently processed and visualized for interpretation. The purpose of the tool is to conduct a descriptive analysis of the data by facilitating its visualization, enabling the identification of differences and learning patterns among various learner populations. Additionally, it integrates a predictive module capable of anticipating learner activities during a learning session. Consequently, VAAD holds the potential to offer valuable insights into online learning behaviors from both descriptive and predictive perspectives.",
    url = "https://doi.org/10.48550/arxiv.2405.20091"
}

@article{ref615_9466e0,
    author = "Muñoz-Porras, Valentina and Xolocotzin, Ulises",
    title = "Developing Structure Sense with Digital Technologies",
    year = "2022",
    doi = "10.4324/9781003197867-4",
    url = "https://doi.org/10.4324/9781003197867-4"
}

@article{ref616_4dcb2a,
    author = "Seeling, Patrick and McGarry, Michael and Johnson, Matthew",
    title = "Reveal Online Learning Clickstream Data to Provide Actionable Intelligence",
    year = "2023",
    doi = "10.1109/fie58773.2023.10343069",
    url = "https://doi.org/10.1109/fie58773.2023.10343069"
}

@article{ref617_074e14,
    author = "Cechinel, Cristian and Queiroga, Emanuel and Primo, Tiago and Ramos, Vinícius and Muñoz, Roberto and Machado, Matheus and Stein, Mayara and Rocha, Juary and Rodrigues, Thomas and dos Santos, Henrique and Portelinha, Giovani and Targino, Rafael and Melgarejo, Valter and Almeida, Juliano",
    title = "Learning Analytics para Moodle em uma arquitetura na nuvem: uma solução escalável para predição de risco acadêmico",
    year = "2023",
    doi = "10.5753/wapla.2023.236214",
    url = "https://doi.org/10.5753/wapla.2023.236214"
}

@article{ref618_f35112,
    author = "Brdnik, Saša and Šumak, Boštjan and Podgorelec, Vili",
    title = "Aligning Learners' Expectations and Performance by Learning Analytics Systemwith a Predictive Model",
    year = "2022",
    doi = "10.48550/arxiv.2211.07729",
    abstract = "Learning analytics (LA) is data collection, analysis, and representation of data about learners in order to improve their learning and performance. Furthermore, LA opens the door to opportunities for self-regulated learning in higher education, a circular process in which learners activate and sustain behaviours that are oriented toward their personal learning goals. The potentials of LA and self-regulated learning are huge; however, they are not yet widely applied in higher education institutions. Slovenian higher education institutions have lagged behind other European countries in LA adoption. Our research aims to fill this gap by using a qualitatively and quantitatively led workflow for building a requirement-oriented LA solution, consisting of empirically gathering the students' expectations of LA and presenting a dashboard solution. Translated Student Expectations of Learning Analytics Questionnaire and focus groups were used to gather expectations from learners. Based on them, a user interface utilizing LA and grade prediction with an AI model was implemented for a selected course. The interface includes early grade prediction, peer comparison, and historical data overview. Grade prediction is based on a machine learning model built on users' interaction in the virtual learning environment, demographic data and lab grades. First, classification is used to determine students at risk of failing - its precision reaches 98\% after the first month of the course. Second, the exact grade is predicted with the Decision Tree Regressor, reaching a mean absolute error of 11.2grade points (on a 100 points scale) after the first month. The proposed system's main benefit is the support for self-regulation of the learning process during the semester, possibly motivating students to adjust their learning strategies to prevent failing the course. Initial student evaluation showed positive results.",
    url = "https://doi.org/10.48550/arxiv.2211.07729"
}

@article{ref619_dc7141,
    author = "Sadallah, Madjid",
    title = "Enhancing Course Revision: Introducing CoReaDa - an Advanced Reading Analytics Dashboard",
    year = "2023",
    doi = "10.35542/osf.io/9bkad",
    abstract = "Ensuring the delivery of high-quality courses that foster successful learning outcomes is crucial. Course authors are faced with the ongoing challenge of revising their content to meet the ever-evolving needs of learners. However, detecting barriers that hinder learners' reading experience and identifying effective strategies for course improvement remains a daunting task. In this paper, we present a learning analytics approach designed to assist course authors in tackling these challenges. Leveraging learners' activity logs, we employ a set of indicators that specifically capture course reading activity, enabling the detection of issues and offering suggestions for content revisions. Our approach culminates in CoReaDa, an innovative learning dashboard equipped with assistive features. To validate our proposals, we instantiate our approach using the logs from a prominent European e-learning platform and conduct a comprehensive study. The study's results unequivocally demonstrate the efficacy of our approach, equipping course authors with heightened awareness and precise guidance in enhancing their courses to better align with learners' evolving requirements.",
    url = "https://doi.org/10.35542/osf.io/9bkad"
}

@article{ref620_ee1776,
    author = "Pei, Bo and Cheng, Ying and Ambrose, Alex and Dziadula, Eva and Xing, Wanli and Lü, Jie",
    title = "LearningViz: a dashboard for visualizing, analyzing and closing learning performance gaps—a case study approach",
    year = "2024",
    issn = "2196-7091",
    doi = "10.1186/s40561-024-00346-1",
    abstract = "Abstract The availability of large-scale learning data presents unprecedented opportunities for investigating student learning processes. However, it is challenging for instructors to fully make sense of this data and effectively support their teaching practices. This study introduces LearningViz, an interactive learning analytics dashboard to help instructors identify, analyze, and close performance gaps among students in their classes. In this dashboard, we incorporated three modules to enhance human and computer interactions for better supporting the teaching practices: the Student Overall Performance Analysis Module, which provides a comprehensive understanding of students’ learning in the course; the Student Group Performance Analysis Module, which examines performance gaps across different groups and identifies factors contributing to these gaps; and the Final Exam Item Analysis Module, which evaluates the quality of exam questions and identifies strategies for closing performance gaps. The overall design of the platform follows a user-centered approach, integrating data analysis with various visualization strategies in a unified platform. A case study is then conducted to highlight the effectiveness of LearningViz in supporting instructors analyzing students’ learning patterns and associated factors impacting learning performance. We further conduct a usability test with several domain experts, to evaluate the usefulness and effectiveness of this platform in supporting the teaching practices. Our findings underscore the platform's ability to support instructors in detecting performance gaps among students, investigating influential factors, evaluating assessment quality and implementing targeted instructional strategies for closing performance gaps.",
    url = "https://doi.org/10.1186/s40561-024-00346-1"
}

@article{ref621_3fd747,
    author = "Abouelenein, Yousri and Selim, Shaimaa and Aldosemani, Tahani",
    title = "Impact of an adaptive environment based on learning analytics on pre-service science teacher behavior and self-regulation",
    year = "2025",
    issn = "2196-7091",
    doi = "10.1186/s40561-024-00340-7",
    abstract = "Abstract Learning analytics provides valuable data to inform the best decisions for each learner. This study, based on adaptive environment (AE) learning analytics dashboards, examines how instructor interventions affect student self-regulation abilities and academic performance. It identifies the self-regulation categories requiring the most support to correct learning paths. Little is known about how interventions in an AE can influence learners' self-regulation based on performance indicators, particularly in science education. The study included 95 Faculty of Education and the Department of Science students. Using a longitudinal clustering approach, researchers identified three unique self-regulated learning (SRL) profiles: oriented, adaptive, and minimally self-regulated learners. While the results showed that the learning analyses were useful in guiding the process of appropriate interventions through an adaptive environment for each student by providing indicators and raising the level of self-regulation for each group separately, the results also showed that there was no change in the classification of self-regulation into groups and that no students moved between groups. These findings highlight the complexity of SRL, suggesting that while interventions can impact engagement and behavior, they may not be sufficient to change the learner's underlying profile. In academic performance, statistically significant differences were found, with the oriented self-regulation group outperforming the adaptive and minimally self-regulated groups. The findings underscore the importance of learning analytics and their indicators for timely interventions in adaptive environments. Additionally, the AE was highly effective, offering students opportunities to review material, which improved their study techniques, test-taking strategies, and overall learning experience.",
    url = "https://doi.org/10.1186/s40561-024-00340-7"
}

@article{ref622_f042ac,
    author = "Harrison, Glenn",
    title = "Big student data: “I don’t believe it!” (Luke), “that is why you fail” (Yoda)",
    year = "2017"
}

@article{ref623_276ddc,
    author = "Sadallah, Madjid",
    title = "User-Centered Course Reengineering: An Analytical Approach to Enhancing Reading Comprehension in Educational Content",
    year = "2024",
    doi = "10.48550/arxiv.2412.11944",
    abstract = "Delivering high-quality content is crucial for effective reading comprehension and successful learning. Ensuring educational materials are interpreted as intended by their authors is a persistent challenge, especially with the added complexity of multimedia and interactivity in the digital age. Authors must continuously revise their materials to meet learners' evolving needs. Detecting comprehension barriers and identifying actionable improvements within documents is complex, particularly in education where reading is fundamental. This study presents an analytical framework to help course designers enhance educational content to better support learning outcomes. Grounded in a robust theoretical foundation integrating learning analytics, reading comprehension, and content revision, our approach introduces usage-based document reengineering. This methodology adapts document content and structure based on insights from analyzing digital reading traces-interactions between readers and content. We define reading sessions to capture these interactions and develop indicators to detect comprehension challenges. Our framework enables authors to receive tailored content revision recommendations through an interactive dashboard, presenting actionable insights from reading activity. The proposed approach was implemented and evaluated using data from a European e-learning platform. Evaluations validate the framework's effectiveness, demonstrating its capacity to empower authors with data-driven insights for targeted revisions. The findings highlight the framework's ability to enhance educational content quality, making it more responsive to learners' needs. This research significantly contributes to learning analytics and content optimization, offering practical tools to improve educational outcomes and inform future developments in e-learning.",
    url = "https://doi.org/10.48550/arxiv.2412.11944"
}

@article{ref624_bfb18b,
    author = "Tan, Fei and Chan, Weng",
    title = "Interpreting Student Performance through Predictive Learning Analytics",
    year = "2024",
    issn = "2180-4370",
    doi = "10.11113/ijic.v14n2.434",
    abstract = "In today's information-rich world, accurately predicting student performance is crucial for institutions seeking to support at-risk students and ensure their success, but this task can be challenging. Learning analytics (LA) can help identify students who are struggling and provide them with the tools and opportunities they need to succeed, benefiting both students and institutions. However, data integration from various sources can be challenging in learning analytics, causing educators to struggle with managing and keeping track of students' progress and dropouts. The goal of this project is to generate insights into student performance through the application of machine learning methods, including Random Forest (RF), Artificial Neural Network (ANN), and Support Vector Machine (SVM). These methods were used to predict students' future results and the likelihood of students' dropout based on predictive learning analytics. RF, ANN, and SVM predictive models were constructed to predict students' future final results and dropout. The dataset from The Open University is used in this study, which consists of information from multiple aspects including data about courses, student registration, results, and their interactions with virtual learning environment. RF, ANN, and SVM models were constructed to predict students' future final results based on their learning behaviour. The performance of the models is evaluated based on accuracy, precision, recall, and time taken for training. In this study, the RF model demonstrated the best performance among the three predictive models for predicting final results and dropout with the shortest training time and achieved the highest accuracy. The RF model achieved an accuracy of 87.8\% in predicting final results and 82.3\% in predicting dropout while maintaining an average training time of 3.6 seconds. At the end of the study, the dashboards visually presented the results, offering valuable insights into students' learning outcomes. This enables educators to effectively support their students by utilizing predictive analytics, which includes identifying potential dropouts and tailoring assistance based on these predictions.",
    url = "https://doi.org/10.11113/ijic.v14n2.434"
}

@article{ref625_a67bbf,
    author = "Escobar, Matheus and Bernardi, Giliane",
    title = "Game Learning Analytics e Visualização de Dados: um Estudo em um Quiz Educacional",
    year = "2025",
    issn = "1679-1916",
    doi = "10.22456/1679-1916.144993",
    abstract = "Este artigo explora a aplicação de Game Learning Analytics (GLA) no jogo educacional Lumni, um quiz projetado para apoiar a aprendizagem dos estudantes em diferentes conteúdos. A pesquisa foca na coleta e análise de dados gerados pelas interações dos alunos, com uma aplicação específica realizada em um contexto de Engenharia de Software, abordando temas relacionados ao conteúdo. Utilizando um dashboard para visualização dos dados, o estudo permite a identificação de padrões de engajamento e a possibilidade de ajustes na dificuldade das questões. Avaliações de usabilidade e aceitação da proposta indicam que tanto o jogo quanto a ferramenta de visualização foram bem recebidos, destacando melhorias na compreensão dos conteúdos abordados e fornecendo feedback em tempo real, o que reforça a eficácia do GLA no ensino.",
    url = "https://doi.org/10.22456/1679-1916.144993"
}

@article{ref626_313e88,
    author = "Sarbu, Danielaanca",
    title = "ELEARNING SOLUTION TO SUSTAIN THE DECISION MAKING PROCESS FOR PROVISIONING OPTIMIZED TELEMEDICINE SERVICES",
    year = "2014",
    issn = "2066-026X",
    doi = "10.12753/2066-026x-14-024",
    url = "https://doi.org/10.12753/2066-026x-14-024"
}

@article{ref627_4d73e4,
    author = "Echeverría, Vanessa and Zhao, Linxuan and Alfredo, Riordan and Milesi, Mikaela and Jin, Yanghua and Abel, Sophie and Yan, Jie and Yan, Lixiang and Li, Xinyu and Dix, Samantha and Wotherspoon, Rosie and Jaggard, Hollie and Osborne, Abra and Shum, Simon and Gašević, Dragan and Martínez‐Maldonado, Roberto",
    title = "TeamVision: An AI-powered Learning Analytics System for Supporting Reflection in Team-based Healthcare Simulation",
    year = "2025",
    doi = "10.48550/arxiv.2501.09930",
    abstract = "Healthcare simulations help learners develop teamwork and clinical skills in a risk-free setting, promoting reflection on real-world practices through structured debriefs. However, despite video's potential, it is hard to use, leaving a gap in providing concise, data-driven summaries for supporting effective debriefing. Addressing this, we present TeamVision, an AI-powered multimodal learning analytics (MMLA) system that captures voice presence, automated transcriptions, body rotation, and positioning data, offering educators a dashboard to guide debriefs immediately after simulations. We conducted an in-the-wild study with 56 teams (221 students) and recorded debriefs led by six teachers using TeamVision. Follow-up interviews with 15 students and five teachers explored perceptions of its usefulness, accuracy, and trustworthiness. This paper examines: i) how TeamVision was used in debriefing, ii) what educators found valuable and challenging, and iii) perceptions of its effectiveness. Results suggest TeamVision enables flexible debriefing and highlights the challenges and implications of using AI-powered systems in healthcare simulation.",
    url = "https://doi.org/10.48550/arxiv.2501.09930"
}

@article{ref628_51b34e,
    author = "Tlili, Adel and Chıkhı, Salim",
    title = "Computer science and educational games to enhancing students’ Islamic content learning",
    year = "2025",
    issn = "2252-8822",
    doi = "10.11591/ijere.v14i2.29459",
    url = "https://doi.org/10.11591/ijere.v14i2.29459"
}

@article{ref629_a1c321,
    author = "Teo, Tee and Heng, Jie and Kok, Chiang",
    title = "Learning Analytics Through Video Analytics and Wearable Sensors for Real-Time Attention Monitoring in Classrooms",
    year = "2025",
    doi = "10.31219/osf.io/q6d9r\_v1",
    url = "https://doi.org/10.31219/osf.io/q6d9r\_v1"
}

@article{ref630_578c08,
    author = "Fuller, Julia and Lokey-Vega, Anissa",
    title = "From Data to Action: Faculty Experiences with a University-Designed Learning Analytics System",
    year = "2024",
    issn = "1537-2456",
    doi = "10.70725/922708xxcbru",
    url = "https://doi.org/10.70725/922708xxcbru"
}

@article{ref631_a85fab,
    author = "McEllistrem, Brian and Hennus, Marije and Fawns, Tim and Hanley, Karena",
    title = "Exploring the Irish general practice training community’s perceptions on how an entrustable professional activities dashboard implementation could facilitate general practice training in Ireland",
    year = "2023",
    doi = "10.6084/m9.figshare.22331711",
    abstract = "The Irish General Practitioner Training (GP) Programme is currently moving to Competency-Based Medical Education (CBME), facilitated by Programmatic Assessment (PA) and Entrustable Professional Activities (EPAs). These new assessment and feedback mechanisms may provide a rich and much sought-after dataset. However, given the possible number of feedback and assessment events, and the variety of modalities used, aggregating and interpreting these can be costly and difficult. Dashboard implementations (DI) have been purposed as a solution to bridge the gap between the large datasets and the training community at all levels. To explore the Irish GP training community’s perceptions on how an EPAs DI could facilitate the delivery of GP training in Ireland. A qualitative approach was taken, using a focus group representative of different groups in the training community. Concurrently, an EPAs DI was developed. Focus group transcripts were analysed in an iterative fashion using Template Analysis to generate themes and subthemes. Numerous advantages were seen in relation to the implementation of an EPAs DI around entrustment decisions, constructive alignment and summative decision-making. These advantages, however, need to be tempered with the realisation that the EPAs DI is not and should not be misinterpreted as being the learning analytic panacea for GP training. This paper outlines the perceptions from a postgraduate medical education training community on an EPAs DI, which would be applicable to other training communities considering introducing similar mechanisms.",
    url = "https://doi.org/10.6084/m9.figshare.22331711"
}

@article{ref632_7c15e8,
    author = "Salleh, Siti and Yassin, Yamin",
    title = "Bayesian Model for Academic Performance Prediction in Learning Analytics",
    year = "2024",
    issn = "2226-6348",
    doi = "10.6007/ijarped/v13-i1/20364",
    abstract = "The learning analytics dashboard (LAD) enables the prediction, tracking, and early recommendation of actions based on academic performance, student conduct, cognitive abilities, and personality traits. The creation of a questionnaire, which involved expert validation and a pilot test, came before the data gathering. Students independent traits, such as behaviour, cognitive skill, and personality, and academic data are the contingent variables. The study flow comprises knowledge acquisition, data collection and analysis, implementation that executes experiments, and evaluation. An exploratory data analysis (EDA) has been performed to investigate the patterns and trends within the data collected on students' characteristics. Simultaneously, a Bayesian prediction model has been created and trained using the gathered data. The forecasts yielded an accuracy of 81\%. Additionally, the F1 scores of 0.74 indicate a moderate level of ability, while scores of 0.90 suggest a high level of performance, and scores of 0.98 indicate an excellent level of performance. The interpretation of the F1 score is conducted within the specific realm of the issue and compared to manual calculations. A satisfactory level of performance is considered acceptable. Within the scope of this investigation, a sensitivity score of 0.8 is deemed favourable due to its ability to accurately identify the related risk of misclassification.",
    url = "https://doi.org/10.6007/ijarped/v13-i1/20364"
}

@article{ref633_a98642,
    author = "Hoang, Nam and Tran, Katya and Dávila, Victorino and Nguyen, Hanh and Bengoechea, Josefina and Bell, Alex and Betts, Anastasia and Son, Ji‐Won and Di, Tania and Perasso, Giuseppe and Maculan, Alessandro and Vianello, Francesca and Paoletti, Patrizio and Paola, Jennyfer and Trujillo, Casas and Cummings-Koether, Michelle and Blanco, Óscar and Larson, James and Juanes, Ayelen and Loke, Rob and Takimoto, Masahiro and Андрес, Павел and Gutiérrez, Vladimir and Vallejo, Universidad and Moroshan, Yana and Brockmann, Patricia and Metwalli, Wafaa and Mbodila, Munienge and Sisulu, Walter and Kosolapov, Samuel",
    title = "The Paris Conference on Education 2022: Official Conference Proceedings",
    year = "2022",
    issn = "2758-0962",
    doi = "10.22492/issn.2758-0962.2022",
    abstract = "We extend a standard for doing agile scrum teamwork in education that permits individual assessment within teams (IAFOR ECE2020).Since the teacher's bandwidth in education is limited and increasingly under pressure, we focus on course design options that can be used to leverage the bandwidth.One economizing option in courses is to let teams prerecord prototype presentation videos before sprint review takes place.This allocates expensive teacher's time to team interrogation time which enriches interaction and engagement and enables effective sharing between teams to improve communication flow in sparse stakeholder feedback scenarios.We also describe three learning analytic pathways that can be smartly integrated into learning dashboards to monitor student and team progress or into learning recommender systems and chatbots to generate action-directed, just-in-time feedback and advice to students.The first one is for setup that enables control of important team diversity and student inclusion parameters such as demographic, personality and professional traits that are known from the student population in advance and that enables handy attribution of 21st-century skill sets within teams.The second one is the product pathway that builds on a datastream generated from qualitative, quantitative and immersive product features that are known from prototyping.The third one is the process pathway in which information on 21st-century skills is generated that are at play in individual and dynamic team processes.We are convinced that these extensions will further enable effective learning technology that is directed to applying agile scrum in education efficently, both for students as teachers.",
    url = "https://doi.org/10.22492/issn.2758-0962.2022"
}

@article{ref634_621e19,
    author = "Yu, Eduardo and Pindea, Elmerito and Tano, Isagani and Lagman, Ace and Victoriano, Jayson and Mababa, Jonilo and Pulumbarit, Jaime",
    title = "Student Information System for Computer Studies with Integrated Programming Anxiety Level Prediction and Balanced Group Recommendations",
    year = "2025",
    issn = "2278-2540",
    doi = "10.51583/ijltemas.2025.140300060",
    url = "https://doi.org/10.51583/ijltemas.2025.140300060"
}

@article{ref635_457a77,
    author = "Gilliot, Jean and Sadallah, Madjid",
    title = "A framework for co-designing effective LADs supporting sensemaking and decision making",
    year = "2024",
    issn = "1477-8386",
    doi = "10.1504/ijlt.2024.137899",
    url = "https://doi.org/10.1504/ijlt.2024.137899"
}

@article{ref636_cc2e71,
    author = "Nespoli, Pantaleone and Albaladejo‐González, Mariano and Valera, José and Ruipérez‐Valiente, José and Garcia‐Alfaro, Joaquin and Mármol, Félix",
    title = "SCORPION Cyber Range: Fully Customizable Cyberexercises, Gamification and Learning Analytics to Train Cybersecurity Competencies",
    year = "2024",
    doi = "10.48550/arxiv.2401.12594",
    abstract = "It is undeniable that we are witnessing an unprecedented digital revolution. However, recent years have been characterized by the explosion of cyberattacks, making cybercrime one of the most profitable businesses on the planet. That is why training in cybersecurity is increasingly essential to protect the assets of cyberspace. One of the most vital tools to train cybersecurity competencies is the Cyber Range, a virtualized environment that simulates realistic networks. The paper at hand introduces SCORPION, a fully functional and virtualized Cyber Range, which manages the authoring and automated deployment of scenarios. In addition, SCORPION includes several elements to improve student motivation, such as a gamification system with medals, points, or rankings, among other elements. Such a gamification system includes an adaptive learning module that is able to adapt the cyberexercise based on the users' performance. Moreover, SCORPION leverages learning analytics that collects and processes telemetric and biometric user data, including heart rate through a smartwatch, which is available through a dashboard for instructors. Finally, we developed a case study where SCORPION obtained 82.10\% in usability and 4.57 out of 5 in usefulness from the viewpoint of a student and an instructor. The positive evaluation results are promising, indicating that SCORPION can become an effective, motivating, and advanced cybersecurity training tool to help fill current gaps in this context.",
    url = "https://doi.org/10.48550/arxiv.2401.12594"
}

@article{ref637_a7de15,
    author = "Chung, Kwang and Kuo, Tony",
    title = "Studies Suspension Prevention System of Distance University using Analysis of Learning Activity and Learner’s Big Data",
    year = "2021",
    issn = "1742-6588",
    doi = "10.1088/1742-6596/1944/1/012001",
    abstract = "Abstract Due to the increase in withdrawals and temporary absences of students from changes in the external environment, distance learning universities are trying to establish various policies and increase the number of enrolled students, and at the same time, are trying to establish various policies and efforts to increase the enrollment. However, it is difficult to systematically diagnose the main cause of the interruption of students, and prior research efforts related to the problem of student suspension in distance learning universities have accumulated, since there are various reasons for the student suspensions. In the proposed distance university Studies Suspension Prevention System (SSPS), distance university students can use two types of learning analytics services. In order to analyze learning activities, we propose the asynchronous learning activity analysis module, and the synchronous learning activity analysis module. In the asynchronous analysis module and symchronous analysis module, quiz, LINE group chatting \&amp; discussion forum communication, and online lecture has a learning state score according to the lecturer’s directions. Learning activities in the learning management system have three kinds of learning states, passive activity state, negative activity state, and medium activity state. Learning activity states are used to predict the student learning state. In the proposed Student Support System, there are two types of learning support services connected to the smart learning portal server. One is the intelligent distance university chatbot service for personalized chatting and caring services. The other is push message services for alarm, warning, notices, and alerts, such as dashboard service.",
    url = "https://doi.org/10.1088/1742-6596/1944/1/012001"
}

@article{ref638_a1b1e8,
    author = "Pawitra, Mahendra and Hung, Hui-Chun and Jati, Handaru",
    title = "A Machine Learning Approach to Predicting On-Time Graduation in Indonesian Higher Education",
    year = "2024",
    issn = "2477-2399",
    doi = "10.21831/elinvo.v9i2.77052",
    url = "https://doi.org/10.21831/elinvo.v9i2.77052"
}

@article{ref639_8dda28,
    author = "Hsu, Chia-Yu and Horikoshi, Izumi and Li, Huiyong and Majumdar, Rwitajit and Ogata, Hiroaki",
    title = "Designing data-informed support for building learning habits in the Japanese K12 context",
    year = "2024",
    issn = "1793-2068",
    doi = "10.58459/rptel.2025.20014",
    abstract = "While building proper learning habits has been said to enhance academic performance, it is challenging to give long-term support for building habits in educational contexts due to the lack of continuous tracing of one’s habitual behaviors. With the accumulation of learning logs and the advancement of Learning Analytics (LA) techniques, this paper illustrates the data-informed support for building learning habits, which involves persuading one to change behaviors. Specifically, we tackled 2 research objectives following the Persuasive Systems Design (PSD) model. First, we defined indicators of learning habits from log data and analyzed 115,340 learning logs of 96 learners from a Japanese junior high school. As a result, the learners’ types of habits could be detected even though some might not be efficient and the stages of habits fluctuated over time. We also identified differences when comparing the learning habits extracted from the log data with those reported in the questionnaire. Second, based on the understanding of the learner profiles, we designed elements of an LA dashboard to support habit-building by applying the design principles from the PSD model. Overall, the learners recognized the feasibility of integrating data-informed support into their daily learning. Therefore, we look forward to the evidence of its effectiveness on the behavior change that can be depicted by the transition between stages of different types of learning habits.",
    url = "https://doi.org/10.58459/rptel.2025.20014"
}

@article{ref640_37b7ed,
    author = "Solano, Andrés and Peláez, Carlos and Ospina, Johann and Luna-García, Huizilopoztli and Valencia, Jorge and Villegas, Gabriel and Moreira, Fernando and Sotélo, Jesús and Villalba‐Condori, Klinge",
    title = "Work Route for the Inclusion of Learning Analytics in the Development of Interactive Multimedia Experiences for Elementary Education",
    year = "2024",
    issn = "2076-3417",
    doi = "10.3390/app14177645",
    abstract = "Interactive multimedia experiences (IME) can be a pedagogical resource that has a strong potential to enhance learning experiences in early childhood. Learning analytics (LA) has become an important tool that allows us to understand more clearly how these multimedia experiences can contribute to the learning processes of these students. This article proposes a work route that defines a set of activities and techniques, as well as a flow for their application, by taking into consideration the importance of including LA guidelines when designing IMEs for elementary education. The work route’s graphical representation is inspired by the foundations of the Essence standard’s graphical notation language. The guidelines are grouped into five categories, namely (i) a data analytics dashboard, (ii) student data, (iii) teacher data, (iv) learning activity data, and (v) student progress data. The guidelines were validated through two approaches. The first involved a case study, where the guidelines were applied to an IME called Coco Shapes, which was aimed at transition students at the Colegio La Fontaine in Cali (Colombia), and the second involved the judgments of experts who examined the usefulness and clarity of the guidelines. The results from these approaches allowed us to obtain precise and effective feedback regarding the hypothesis under study. Our findings provide promising evidence of the value of our guidelines, which were included in the design of an IME and contributed to the greater personalized monitoring available to teachers to evaluate student learning.",
    url = "https://doi.org/10.3390/app14177645"
}

@article{ref641_4e99bc,
    author = "Şahi̇n, Muhittin",
    title = "Advances in Video Analytics",
    year = "2024",
    issn = "2211-1662",
    doi = "10.1007/s10758-024-09768-9",
    abstract = "Abstract Learners interact with content, assessments, peers, and instructors in digital learning environments. Videos, which are popular due to internet technologies, capture learners’ attention, boost motivation, and enhance learning. Learning analytics broadly optimize educational environments by analyzing data, with video analytics focusing specifically on video interactions to enhance learning outcomes. Video-player interactions (e.g., play, pause) and video content interactions (e.g., true-false questions) provide insights into learner behaviors. Lack of interaction is a major reason for high dropout rates in video platforms and MOOCs. Video analytics can help address this issue by analyzing and improving engagement with video content. This special issue has a specific focus on video analytics and impact of this field to the learning experience. Four articles were included in this special issue. The findings reveal that I) the type, length, and purpose of the video are important for student engagement, ii) important tips on video-based learning design are presented, iii) when interacting with the video player, pause, play, rewind and fast forward are the most commonly used interaction types., iv) providing more information about video interaction processes with dashboards would provide much more insight, and v) dividing the videos into more than one section both creates the perception of better structuring of the process and the segmentation of the videos contributes more to learning.",
    url = "https://doi.org/10.1007/s10758-024-09768-9"
}

@article{ref642_f86cae,
    author = "Dekker, Andrew and Zimbardi, Kirsten and Long, Phil and Hay, Peter and Engstrom, Craig and Bugarčić, Andrea and Colthorpe, Kay and Lluka, Lesley and Chunduri, Prasad and Marrington, Justin and Worthy, Peter",
    title = "Examining the use of in-situ audio annotations to provide feedback to students",
    year = "2012"
}

@article{ref643_3b3157,
    author = "Nordmark, Susanna and Augustsson, Hanna and Davidsson, Mattias and Andersson-Gidlund, Tobias and Holmberg, Kristina and Mohseni, Zeynab and Rack, John and Masiello, Italo",
    title = "Piloting Systematic Implementation of Educational Technology in Swedish K-12 Schools – Two-Years-In Report",
    year = "2024",
    issn = "2662-9275",
    doi = "10.1007/s43477-024-00130-w",
    abstract = "Abstract Halfway through a four-year research project supported by implementation science and the Active Implementation Frameworks (AIF), this article reports on the status of the initial two implementation stages. Our research investigates the impact of systematically preparing educators and educational institutions to integrate digital learning materials and learning analytics dashboards to enrich teaching practices and improve student performance outcomes. Furthermore, it seeks to establish a foundation for the use of innovative and validated educational technology (EdTech) through sustainable implementation strategies, evidence-based evaluation, and continuous redesign of digital learning materials. By adopting this comprehensive approach, we aim to enhance the knowledge base regarding effective digital innovation integration within educational environments. We argue that applying implementation science in educational settings facilitates the adoption of effective innovations, promotes evidence-based decision-making, and helps identify and address obstacles to change. Our ongoing research underscores the transformative impact of implementation science in education. Thus far, we have highlighted the crucial role of teacher perspectives and the necessity of co-designing technology aligned with teaching and learning objectives. This nuanced approach refutes the notion of a one-size-fits-all solution or a quick fix achievable in a single academic year. Instead, it advocates a dynamic, collaborative model that acknowledges the multifaceted nature of implementation. Our journey has reaffirmed the dedication of teachers, showcasing their readiness to invest time and effort when their professionalism is respected, and their input is genuinely valued and acted upon.",
    url = "https://doi.org/10.1007/s43477-024-00130-w"
}

@article{ref644_14cf6f,
    author = "Costa, P and de Oliveira, Tiago and Mariano, Flávia",
    title = "Métodos e aplicações de learning analytics no ensino corporativo: uma revisão sistemática literária",
    year = "2023",
    issn = "1679-1916",
    doi = "10.22456/1679-1916.134357",
    abstract = "Learning Analytics (LA) representa a transformação dos dados gerados por Sistemas Gerenciadores de Aprendizagem (LMS) em conhecimento, tornando-se subsídio para tomada de decisões. Essa abordagem ocorre em forma de painéis, dashboards, relatórios, entre outras ferramentas que proporcionem a apresentação do conhecimento dos dados, e assim, possibilitam a disposição destes para a tomada de decisão das partes interessadas. Pela representatividade da aprendizagem formal, o conceito de LA é amplamente aplicado, surgindo comumente oportunidades e necessidades de interpretação dos dados impulsionados pelos avanços na utilização dos LMS como centralizadores de informação. Porém, no Ensino Corporativo (EC) essa utilização é discreta e propicia a oportunidade de personalização do método para este aprendiz corporativo que possui características únicas. Com isso, o objetivo deste artigo é entender as formas empregadas de LA envolvendo LMS em diferentes formas de ensino a fim de entender sua aplicabilidade no EC. O método aplicado foi uma Revisão Sistemática da Literatura (RSL). O resultado foi estruturado em quatro taxonomias para identificar métodos e estratégias, ferramentas utilizadas, identificação das partes interessadas e medição por métricas e indicadores. Este artigo avaliou 1296 materiais científicos publicados entre 2017 e 2021, selecionando 50 exemplares relevantes ao tema, onde, mesmo com poucos exemplares correspondendo diretamente LA ao EC, foi possível abstrair informações aplicáveis aos objetivos das questões de pesquisa propostas.",
    url = "https://doi.org/10.22456/1679-1916.134357"
}

@article{ref645_d610f0,
    author = "Mccarthy, Aslıhan and McNally, Clare and Bailey, Denise and White, Matt",
    title = "Adopting self-directed learning principles in clinical education with Pebblepad",
    year = "2023",
    issn = "2653-665X",
    doi = "10.14742/apubs.2023.567",
    abstract = "Self-directed learning (SDL), alternatively known as self-regulated learning (SRL), is an umbrella term that can be broadly defined as the learner’s ability to make plans according to their individual needs and use learning resources and methods to master a knowledge or necessary skills (Van der Walt, 2016; Russell et al. 2022). It is a systematic control of motivation and tightly linked to self-awareness, agency, and the sense of being in control of the learning process (Russell et al. 2022). Despite ever increasing reference to SDL in health professions education, it is not well-studied in clinical settings (Lui \&amp; Sullivan 2021, Murad et al. 2010, Yeo and Jang 2023). In 2021-2022 we adopted some of the main principles of the SDL approach, namely goal setting, self-monitoring, self-reflection, self-evaluation, to clinical education in Bachelor of Oral Health and Doctor of Dentistry programs at the University of Melbourne by utilising a digital e-portfolio platform, Pebblepad. In this presentation, we will walk you through the iterative design process of digital clinical assessment forms. This process allowed us to refine our materials and strategies in consultation with our students, teaching and learning staff and clinical supervisors as we go. We supported our students to a) identify gaps in application of their clinical knowledge via learning analytics dashboards, b) generate goals for improvement through structured reflection and c) assess their practice through self-evaluation rubrics. We are still improving our concept to foster SDL in our programs. We take this opportunity to reflect on what went well as opposed to areas for improvement, and demonstrate the power of using different digital tools in clinical education settings. We believe “thinking outside the box” can help students become self-directed learners through collaboration, continuous improvement, and flexibility.",
    url = "https://doi.org/10.14742/apubs.2023.567"
}

@article{ref646_0b4973,
    title = "Integrating Digital Assessment Innovations In Social Sciences: A Comprehensive Analysis Of AI, Virtual Reality, And Adaptive Learning Systems",
    year = "2024",
    issn = "1660-6795",
    doi = "10.62441/nano-ntp.v20i6.79",
    abstract = "Objective: The integration of digital technologies in assessment practices has emerged as a critical challenge in social sciences. This study aimed to investigate the effectiveness and impact of digital innovations in assessment and evaluation methodologies within social science disciplines, focusing on the implementation of integrated digital assessment systems. Methods: A mixed-methods research design was employed over 18 months, involving 450 participants across 15 universities. The study implemented three integrated digital systems: an AI-Enhanced Assessment Platform, a Virtual Reality Social Assessment Environment, and an Adaptive Learning Analytics Dashboard. Performance metrics were compared between traditional and digital assessment methods. Results: Digital assessment methods demonstrated significant improvements in accuracy (+11.4 percentage points, p<0.001) and efficiency (28.8\% reduction in completion time). The Virtual Reality environment achieved 94.8\% completion rates with 8.7/10 user satisfaction. The AI platform demonstrated 89.0\% accuracy across assessment types. Cross-platform integration yielded synergistic benefits in user experience (+14.1\%) and feedback quality (+9.0\%). Psychology (94.2\%) and education (92.8\%) showed the highest adoption rates. Conclusion: The integration of digital assessment tools significantly enhances evaluation practices in social sciences, offering improved accuracy, efficiency, and user engagement compared to traditional methods. Disciplinary variations suggest the need for tailored implementation strategies across different fields. Significance: This research establishes a comprehensive framework for implementing digital assessment systems in social sciences, contributing to the advancement of evaluation methodologies while highlighting important considerations for ethical implementation and accessibility. The findings provide valuable insights for educational institutions seeking to modernize their assessment practices.",
    url = "https://doi.org/10.62441/nano-ntp.v20i6.79"
}

@article{ref647_d95b50,
    author = "Jin, Flora and Maheshi, Bhagya and Lai, Wenhua and Li, Yuheng and Gašević, Danijela and Chen, Guanliang and Charwat, Nicola and Chan, Philip and Martínez‐Maldonado, Roberto and Gašević, Dragan and Tsai, Yi‐Shan",
    title = "Students’ Perceptions of GenAI-powered Learning Analytics in the Feedback Process:",
    year = "2025",
    issn = "1929-7750",
    doi = "10.18608/jla.2025.8609",
    abstract = "This paper explores the integration of generative AI (GenAI) in the feedback process in higher education through a learning analytics (LA) tool, examined from a feedback literacy perspective. Feedback literacy refers to students’ ability to understand, evaluate, and apply feedback effectively to improve their learning, which is crucial for fostering self-regulated learning and academic growth. GenAI has the potential to open new avenues of research and design in augmenting feedback practices by providing innovative, personalized, and scalable feedback solutions. The study investigates how GenAI functionalities, specifically ChatGPT explanation features and GenAI-powered dashboard visualizations, can support students in engaging with feedback. Using feedback literacy theory, a thematic analysis was conducted and triangulated with usage trace data to assess students’ perceptions of these functionalities. The study involved three key activities: introductory lab sessions, in-semester use of the GenAI-powered LA feedback tool, and post hoc interviews, with data collected from 18 students from various disciplines (information technology, education, business and economics, and engineering) throughout all phases. Initial findings from the lab sessions showed positive perceptions of the GenAI functionalities. However, trace data from in-semester use indicated modest engagement with GenAI. Post hoc interviews revealed that reduced engagement was due to a mismatch between the GenAI outputs and student expectations. While some students appreciated the GenAI functionalities, others found them redundant when they perceived the feedback as clear and easy to understand. This study highlights the potential of GenAI in the feedback process and underscores the challenges of aligning AI tools with diverse student needs. Future developments should focus on creating adaptive and discipline-specific GenAI solutions.",
    url = "https://doi.org/10.18608/jla.2025.8609"
}

@article{ref648_5391e6,
    author = "Berenguer, Daniel",
    title = "Un entorno para la creación de actividades de aprendizaje : uso de estándares de interoperabilidad,analíticas de aprendizaje y automatización basada en modelos",
    year = "2020"
}

@article{ref649_f0c6b7,
    author = "Naeem, Usman and Simmons, Matthew and Hathaway, Graeme and Alsadder, Lujain",
    title = "Use of Learner Engagement Analytics to empower medical educators to make data-informed decisions",
    year = "2024",
    issn = "1759-667X",
    doi = "10.47408/jldhe.vi32.1427",
    abstract = "Learner Engagement Analytics (LEA) has enabled Higher Education Institutions (HEIs) to identify learners who are not engaging with their studies and provide targeted support and help (Naeem and Bosman, 2023). It has also allowed educators to make data-informed decisions to inform their curriculum design and classroom practice (Cogliano et al., 2022). The LEA data is captured from a wide range of sources related to teaching and learning which offer meaningful insights into a learner’s learning habits (Eady et al., 2021). Previous research suggests that providing learning analytics to educators in Higher Education Institutions can improve learning outcomes for students (Aslan et al., 2019). At Queen Mary University of London, a multidisciplinary team was formed of medical educators, LEA experts, learning technologists, and learning innovation professionals to investigate how LEA can inform curriculum redesign in the early phase of the medical curriculum. Through a series of scholarship meetings on LEA using data dashboards from the Virtual Learning Environment (VLE), the team analysed learners’ engagement with the virtual pre-sessional resources. The VLE interactive resources were designed to allow medical learners to develop clinical interpretation skills during practical sessions, as recommended by the General Medical Council (GMC, 2018). However, medical educators lacked the metrics to evaluate learners’ interaction with these virtual resources. Therefore, it was inevitable to train educators on how to use LEA to optimise students’ learning. The team assessed the learners' engagement on the VLE, quantified engagement scores, and evaluated the results against key outcomes, including the learners' performance. The LEA data offered further insights into virtual engagement across multiple modules in the medical curriculum. Effectively, the outcome of this work empowered medical educators to make informed decisions regarding the future use of VLE resources in curriculum design and develop virtual resources to increase students’ engagement and enhance their learning.",
    url = "https://doi.org/10.47408/jldhe.vi32.1427"
}

@article{ref650_bd9695,
    author = "White, Matthew and Dal Santo, Katharine and Copley, Jodie and Mustchin, Claire and Jones, Bree",
    title = "Designing gamified branching scenarios on a technological platform to enhance clinical reasoning in dental education",
    year = "2025",
    issn = "2624-4705",
    doi = "10.24135/pjtel.v7i2.228",
    abstract = {Abstract Simulation and gamification offer valuable pedagogical approaches, providing safe environments for dental students to practice complex professional skills. Developing clinical reasoning is crucial as students perform irreversible procedures early in their training. Traditional methods may offer limited opportunities to experience decision consequences without risking patient harm. Simulation based learning in a branching scenario (BrSc) format, that incorporates gamification and productive failure (Kapur, 2008), bridges the gap between preclinical theory and clinical practice, addressing the need for engaging, relevant learning experiences (ASE Strategy). This project aligns with the University's ASE Strategy, 'A curriculum defined by quality and relevance' through innovative, inquiry-based learning and authentic assessment, and 'Environments and systems that enable innovation' by leveraging digital technologies. This presentation details the design, implementation, and preliminary evaluation of a gamified BrSc developed to support clinical reasoning and understanding decision consequences for second-year dental and oral health students managing atypical dental pain. Informed by a validated serious gaming model (Argueta-Muñoz, 2023) and based on an authentic patient case, the BrSc was designed as an interactive narrative where student choices determine progression through multiple pathways. Key gamified features included 17 strategically placed 'learning points' representing common clinical errors, branching pathways with unique information and a dashboard for tracking progress. The design supported productive failure, allowing students to navigate suboptimal pathways discovering learning points which provided tailored feedback prompting them to revisit decisions. A Design-Based Research (DBR) approach guided the project, emphasising iterative design and evaluation in a real-world classroom setting. A survey platform (Qualtrics) was selected for development due to its branching logic capabilities, built-in reporting for learning analytics, and low technical barrier to entry. Evaluation data included learning analytics automatically captured by the platform (tracking pathway choices, attempts, time, learning points unlocked), direct classroom observations of student engagement and interaction patterns (individual vs. collaborative), and qualitative feedback from students. Preliminary results from implementation with 149 students (775 attempts) indicate the gamified BrSc fostered high engagement. The gamified 'learning point' tracking appeared effective; classroom observations showed students, particularly when working collaboratively after an initial individual attempt, actively discussed pathways and were motivated to explore different options, including incorrect paths, to 'unlock' all points. Learning analytics provided valuable insights for educators, visualising common decision pathways via Sankey diagrams and identifying specific points of difficulty, informing potential teaching interventions (ASE Strategy: Use of analytics to support the student experience). Student feedback directly informed iterative improvements, such as adding skip functionality for previously completed sections. The tool facilitated peer learning and discussion, contributing to 'A community in which This work demonstrates how survey platforms can be effectively repurposed to create sophisticated, gamified BrSc for complex skill development. It offers insights into specific gamification design choices and their observed effects on student engagement and learning behaviour. It highlights the utility of learning analytics for informing educational practice and curriculum refinement, supporting evidence-based teaching (ASE Strategy: Valuing excellence in education). A brief demonstration of the branching scenario's gamified elements, discussion of the design decisions and challenges, and posing a reflective question: "How might the principles of gamified feedback and pathway exploration be adapted using technologies within your own teaching context to enhance student learning through simulation?" References Argueta-Muñoz FD, Olvera-Cortés HE, Durán-Cárdenas C, Hernández-Gutiérrez L, Gutierrez-Barreto SE. Instructional design and its usability for branching model as an educational strategy. Cureus [Internet]. 2023 May 18 [cited 2023 Sep 5];15(5). Available from: https://research.ebsco.com/linkprocessor/plink?id=18f651fd-54f2-316d-8038-e6d44d4b1e4c Kapur M. Productive failure. Cogn Instr [Internet]. 2008 Jul 1 [cited 2023 Sep 4];26(3):379–425. Available from: https://research.ebsco.com/linkprocessor/plink?id=80f6be87-d06d-3d8b-9aa7-f03a246ed424 White, M., Dal Santo, K., Copley, J., Mustchin, C., and Jones, B. (2024). Designing branching scenarios to support clinical reasoning in dental education. In Cochrane, T., Narayan, V., Bone, E., Deneen, C., Saligari, M., Tregloan, K., Vanderburg, R. (Eds.), Navigating the Terrain: Emerging frontiers in learning spaces, pedagogies, and technologies. Proceedings ASCILITE 2024. Melbourne (pp. 415-419). https://doi.org/10.14742/apubs.2024.1150},
    url = "https://doi.org/10.24135/pjtel.v7i2.228"
}

@article{ref651_e4fde6,
    author = "Göbel, Stefan and Rotter, Elisabeth and Brabänder, Wolfgang and Maier, Angelika and Ziegler, Birgit",
    title = "Serious Games for Vocational Training",
    year = "2024",
    issn = "2049-0992",
    doi = "10.34190/ecgbl.18.1.2710",
    abstract = "The research project ‘SG4BB’ (Serious Games for Vocational Education and Training; abbr.: VET) has developed an integrated platform for the description, search, retrieval, integration, and utilization of educational games. This paper summarizes project outcomes, including platform concepts, software components, and – as focus – practical insights derived from a case study involving the educational game ‘Corrugated’ as simulation and training environment for service technicians. SG4BB has followed a user-centered design in an interdisciplinary project team: Two learning providers specified their requirements for VET in their specific application domains. Based on those requirements, game developers, learning solution providers and researchers for VET have conceptualized and prototypically implemented both an integrated learning platform (SG4BB platform) and two case studies of Serious Games for VET: ‘Corrugated’ for service technicians and an IT security game. The platform follows a process pipeline: For search and retrieval of educational games for VET, an application profile for VET based on the standardized ‘Serious Games Metadata Format’ (DIN/SPEC 91380) has been elaborated. This format builds the semantic basis for the metadata-based catalog system ‘Serious Games Information Center’ (SG-IC) with filter functionality for VET. Learning providers and developers can use the SG-IC portal to describe and promote their educational games, enabling users to identify suitable games for their learning needs and integrate them via learning infrastructure. Educational games can interact with the backend (Learning Management System and Learning Record Store) through a middleware based on the xAPI standard, allowing for personalized gameplay and data collection for game-based learning analytics. The final evaluation of the SG4BB project focused on the utilization of the educational game ‘Corrugated’, targeting problem-solving skills for service technicians in the corrugated cardboard industry. Data from 26 participants playing the game for 60 minutes, along with problem-solving tests and user experience feedback, were analyzed to validate game-based assessment and to assess learning impact. Initial results reveal insights into specific game missions, playtimes, and success rates, indicating that participant behavior during gameplay influenced perceived learning progress, leading to varied learning paths. This paper provides valuable insights and technical information for VET practitioners on using educational games for training. Game interactions and learning outcomes can be monitored via a dashboard within the learning infrastructure, offering visualizations for user behavior (during play) and (learning) progress. Keywords: serious games, vocational education and training, metadata, middleware, case study and evaluation.",
    url = "https://doi.org/10.34190/ecgbl.18.1.2710"
}

@article{ref652_771e28,
    author = "Renz, Jan",
    title = "Lebensbegleitendes Lernen in einer digitalen Welt : Nutzerzentrierte Konzepte und Lösungen zum Optimieren digital gestützten Lernens in Schule und Arbeitsleben",
    year = "2020",
    doi = "10.25932/publishup-47257",
    url = "https://doi.org/10.25932/publishup-47257"
}

@article{ref653_ecd878,
    author = "Rupp, André",
    title = "Commentary: Modernizing Educational Assessment Training for Changing Job Markets",
    year = "2024",
    issn = "0731-1745",
    doi = "10.1111/emip.12629",
    abstract = {Being sufficiently "data literate" (e.g., Tableau, 2023) is a core competency across most professions in the world today. Being data literate may run the gamut from having more advanced computational training to having a more applied understanding of the power of use (and misuse) of data for different applications. The latter is nowadays formalized in the role of a "data champion" (e.g., Clare, 2019) whose primary goal is to advocate for and support the creation of a coherent "data culture" with associated processes for data architecture, engineering, and governance (e.g., Danese \& Gatti, 2023). Working with data is obviously an essential part of the discipline of educational measurement as well—in particular when it comes to latent-variable-based methodologies—but, as I will discuss in this commentary, it is increasingly not a uniquely differentiating characteristic. Specifically, in the last 20 years or so, the conceptual boundaries of the field of educational measurement have become increasingly fluid, starting to integrate more deeply with neighboring disciplinary areas within an increasingly interdisciplinary space. These neighboring areas include heavily computational domains such as general statistical methodology (e.g., Rabe-Hesketh \& Skrondal, 2004), computational psychometrics (e.g., Hao et al., 2024; von Davier et al., 2021), data science (e.g., AWS, 2023; He \& Lin, 2020), multimodal learning analytics (e.g., Ouhaichi et al., 2023), large language models (Naveed et al., 2023), automated scoring (e.g., Yan et al., 2019), and artificial intelligence (e.g., IBM, 2023) on the one hand and heavily foundational domains such as UI/UX design (e.g., Lamprecht, 2023), learning sciences (e.g., Tokuhama-Espinosa, 2019), sociocultural psychology (e.g., Mislevy, 2018), and justice-oriented antiracist assessment (e.g., Randall, 2021) on the other hand. So, what does this interdisciplinary proliferation of concepts, tools, and practices imply about the way we develop, nurture, and identify talent in educational measurement? The authors of the NCME Presidential Task Force report presented in this issue (Ackerman et al., 2024) did a fabulous job at painting a comprehensive and differentiated big picture of the connectedness of these issues with regard to relevant training, professional experiences, and the changing nature of our profession. I genuinely appreciate how they were able to pull various complicated conceptual strands together and paint an accessible picture for a wide readership on these issues. As such, I do not want to offer up a point-by-point professional critique of their work with an unnecessary nit-picky negative spin. Rather, I would like to embrace their report as an opportunity to reflect on experiences and perspectives that I have developed throughout my career across academia, industry, and K–12. Some of these connections will be a bit tighter while others will be a bit looser, given the limitations of space for the commentary. I offer up these additional expositions on issues raised in the main report in order to help those who are entering this interdisciplinary professional space—or those who are changing job trajectories within it - make some additional sense of the complexities of our heavily evolving work. I focus in particular on (1) the role of artificial intelligence (AI) for transforming educational experiences, assessment, and measurement and (2) tightening the connection between professional work in educational measurement writ large in professional occupations, postsecondary education, and K–12 education. Even though this commentary was not produced by ChatGPT—for better and for worse as some might say! —it is conceivable in 2024 that an initial draft could have been. As most of us can attest, recent advances of generative large language models, image, visual art, or storyboard generation, music creation, code production, and other AI-based tools across disciplines have been nothing short of mind-blowing. For example, in my recent work I have used ChatGPT to simulate and visualize data, summarize thematic ideas from open-ended responses in surveys, write draft text for publication materials, help me generate and refine ideas, create graphics for presentations, and identify possibly valuable resources. With appropriate checks for accuracy and appropriateness of course! These advances have already transformed various job markets, with both positive and unfortunate consequences for the lives of individuals (e.g., World Economic Forum, 2023). These conditions necessitate that anyone who wants to be competitive in the job market in the future develop a lifelong growth mindset to facilitate consistent upskilling based on technological advances more generally and AI advances more specifically. This is as true for most disciplines generally as it is for the discipline of educational measurement specifically. The good news is that this demand for constant upskilling creates a powerful opportunity space for innovations in learning experiences and, consequently, various forms of assessment experiences at scale (e.g., Rupp \& Lorié, 2023). In this regard, a recent report by the US Office of Educational Technology (Cardona et al., 2023) painted a comprehensive picture of the impact of AI on education. The authors of the report consistently underscored the power of AI for learning, especially formative instructional guidance, and assessment as and for learning. For example, there will likely be an increasing hunger by various stakeholders for flexible, adaptable, and (at least partially) automated learning and assessment systems that are coherent and well-integrated into curricular experiences. Beyond more advanced integrated systems, there will also be an ever-increasing desire for more authentic project-based learning activities and associated performance assessment events that can only be made possible by the thoughtful application of AI-supported digital tools due to the advances in the disciplinary areas from which the problems are borrowed. These advances will of course not make human educators obsolete, but, rather, require them to adapt and shift their roles and responsibilities in the classroom and when working with school leaders, parents, and community members. In addition to educators needing to adjust the way they approach education, the demand for stakeholder engagement during the design, implementation, and evaluation of these systems will continue to increase more generally. We can already see that broader societal awareness of biases, risks, and inequitable decision-making is rising, and more and more traditionally marginalized groups affected by these tools and the associated processes consequently want their voices to be heard and acted upon in the hopes of democratizing educational processes and outcomes (e.g., Mascareñas \& Tran, 2023). More narrowly, advances in AI require educational measurement specialists to re-examine the role of "humans in the loop" during the design, implementation, evaluation, and refinement of such technologies. Ironically, the "humans" referred to in these contexts are typically—either explicitly or implicitly—highly trained, specialized experts, which means that their expertise will likely have been vetted by people in power positions. Yet, even such expert involvement is not a global panacea as even experts can be fallible to ignoring biases while chasing technical innovations (e.g., Turner et al., 2023). Arguably, for certain kinds of judgment processes of AI-generated content (e.g., appropriateness, sociocultural biases) the humans in question should certainly include representatives from broader stakeholder groups so as to not perpetuate existing blindspots. Advances in AI and the applications that this advancement supports will probably lead to an(other) identity crisis for the discipline of educational measurement. Our field has historically been obsessed with norm-referenced interpretations and associated scaling technologies in application contexts with heavy design and implementation constraints. While there is clearly a role for place for these kinds of applications—consider, for instance, national and international educational surveys—there are also clearly design-based limits for what these assessments can tell us. For example, NAEP and PISA are predominantly designed to provide brief, coarse-grained monitoring snapshots of achievement and drivers for achievement for groups of students based on representative samples. Despite innovations in the way complex competencies are operationalized in these contexts (see, e.g., competencies such as collaborative problem-solving) and optimism for deeper transformations in the future (e.g., Foster \& Piacentini, 2023), there is still a large conceptual gap between the nature of these tasks and the richness of the evidence that is being collected about learners due to the complex constraints of the survey implementation context. One notable opportunity for growth in the discipline of educational measurement is in supporting the principled design, deployment, and evaluation of interactive, integrated learning and assessment environments that allow for individual and collaborative problem-solving at scale. This work requires embedded data analytics applied to logfiles/trace files that can be appropriately fine-tuned and then leveraged for feedback to support competency development over time. But it also requires a current understanding of what the learning and applied cognitive sciences as well as experiences from the field tell us about how learning happens for learners with different personalities, needs, and backgrounds (e.g., Brown, 2022). The interdisciplinary toolkit that is required to do this work rigorously and meaningfully in specific educational contexts can be generative and insightful. That is, it can in turn be applied to nationally and internationally scaled assessment endeavors for a broad variety of organizational decision-making purposes. At a US-national level, this is already happening with organizations such as Inq-ITS (Apprendis, 2023) or Curriculum Associates (CAINC, 2023) in the K–12 space. At the international level, this is already happening with organizations such as Duolingo (Burstein et al., 2022), Khan Academy (diCerbo, 2024), Amazon (Amazon, 2023), or Tesla (Tesla, 2023) in the adult learning space. All of this work undoubtedly requires some large-scale data-processing methods from somewhere in the intersection of general statistics, data science, and psychometrics, but I doubt that, in the long term, latent-variable-based scaling methodologies such as item response theory (e.g., van der Linden, 2021) or structural equation modeling (e.g., Kline, 2023) will be the exclusive or even dominant tool for data aggregation. Nor will the most innovative analytic methods most likely have been developed exclusively by specialists in educational measurement. For example, consider (dynamic) Bayesian networks, which is one powerful modeling framework for these kinds of applications. While they have been brought closer to the attention of educational assessment specialists through publications such as Almond et al. (2015), modern web-based resources quickly underscore how applications in education are just one of many examples of these (e.g., Bayes Server, 2023). Therefore, one may ask whether the value proposition for a graduate degree in educational measurement can remain strong enough for the job market of the future as it will have to compete with the value proposition offered by similar kinds of technical and computational degrees from neighboring disciplines. To help postsecondary students understand these value propositions more deeply so that they can continually make informed decisions about their career pathways, a closer connection of professional workspaces and postsecondary education is necessary. Learners of all ages, starting as early as upper middle and/or high school, are asked to make a variety of "bets" on what an appropriate set of skills to develop might be in order to have reasonable chances of entering different career pathways. This is reflected in the projects they choose in courses, the high school courses that they select, and the reflections they are asked to make about their plans for the future, to name but a few situations. That is a tough challenge for anyone at any given point in time, let alone repeatedly over time and with distal time horizons. Understanding how different professional areas are (currently) structured, what work in these areas (currently) looks like, at least prototypically, and what knowledge, skills, and competencies one (currently) requires is relatively hard to parse for most of us, at least in my experience. Moreover, not only are professional disciplines themselves transforming at a rapid pace due to advances in technology and AI, as discussed in the previous section, each discipline is also rather diverse within its own conceptual boundaries at any given point in time. It is helpful in this regard to recognize prototypical differences across the resulting professional subspaces that affect the nature of the work that is being conducted. The focal report by the NCME Presidential Task Force describes essential interdependencies between required competencies and different jobs in the section on Educational Measurement Careers. As the authors aptly noted, entry-level skills that one has developed through badges, certificates, or a master's degree are indeed just a starting point for a career trajectory that can take on very different shapes depending on a variety of factors. There are typically opportunities for utilizing a particular skill set in many different jobs, but the relative importance, time spent, and practical impact of the work will differ notably in these jobs. For example, while simulation studies are a common tool in our field, they are most frequently employed as part of PhD pursuits in heavily methodological programs, sometimes employed as part of analytic development processes in ed tech contexts, but relatively rarely employed as part of the work by district or state assessment experts. Such characterizations should obviously be taken with some grain of salt as the experiences of individuals will always differ in important ways. Beyond such interindividual differences, there are also additional structural distinctions within disciplinary subspaces that matter. That is, there is also no such thing as "the university experience" or "the industry experience" or "the K–12 experience." Rather, there are important differences across the organizations within these broader subspaces and typically even within certain departments, divisions, or units within these. For example, studying or working on educational measurement issues in a larger, research-centric university program is very different from studying or working on these issues in a smaller, teaching-centric university program, even though both research and teaching matter to some degree in each of these programs. Similarly, working on these issues in a small ed-tech start-up is very different from working on them in a larger, established organization that may function effectively like different independent suborganizations where this work is central in some of them and irrelevant in others. As a final example, working as an assessment or accountability specialist in a small or midsize district is very different than working in this role at a state department or independent consulting organization, partly due to the availability and structure of data and the nature of the problems that are being solved (e.g., local school improvement vs. state-wide system monitoring and resource allocation). The degree to which these differences are important and influential for one's career path at any given point in time depends on several factors, some within one's control and some beyond it. For example, one has essentially no control over the core demands of a job or how the characteristics of a given employment sector change over time, which affects internal reorganization or may lead to layoffs. However, one has much more influence over how one reacts to induced shifts as one can try to adapt, grow, and change to meet the emerging demands of a sector. I am definitely not saying that this is equally possible for everyone under all conditions, but I am hopeful that, with an increased modularization of learning experiences and associated recognition of skills and more freely available or, at the very least, low-cost educational opportunities, the playing field will slowly become more level for professionals with a variety of backgrounds and experiences. In this regard, NCME has started offering initial digital training modules under the umbrella of their ITEMS series (NCME, 2023), which offers a fruitful starting place for developing more formal courses, badges, and certificates in collaboration with partner organizations. One of the peculiar aspects of educational measurement as a discipline that I have observed over the years is a notable disconnect between the high degree of technical specialization in advanced modeling commonly afforded by a "prototypical" PhD education with a methodological focus and the much higher degree of reliance on a wide array of applied, interdisciplinary skill sets in real-life jobs. This perception was echoed, to some degree, by a recent curriculum review of educational measurement programs (Randall et al., 2021). Specifically, the authors pointed out that there is a solid coverage of core technical content in most programs in line with what experts believe is important but a notable lack of professional development opportunities and graduate courses in important areas that are relevant to decision-making with empirical assessment information such as educational policy. Some programs build such bridges rather effectively while others are much more narrowly focused and might simply offer elective courses in other departments that are not truly integrated with the learning in the core program. This affords very different opportunities for students across programs and, thus, different entry points to the professional space. Arguably, then, to better prepare students for the future, we need more opportunities for them to develop flexible but rigorous habits of mind along with experience with diverse digital tools through rich, authentic, contextualized decision-making problems. For example, I could see a strong value for expanded educational measurement programs at the master's level in the future that offer opportunities for learners to develop foundational computational and interdisciplinary competencies, enriched through a variety of rich problem-solving contexts, ideally structured in such a way that the outcomes of their work actually matter to real-life practice (e.g., helping an organization improve their reporting practices or shape educational policy). Of course, the best academic programs already offer such opportunities through the connections that faculty have with businesses, government organizations, and other institutions and more of that is needed. In such a world there would remain a role for PhD programs of course but I could see those more narrowly targeted toward the kinds of students who truly want to advance the quantitative methodological (or at least mixed-methods) machinery per se and enter a life of teaching and graduate student mentoring in these programs. Yet, I would argue that there is a risk in over-privileging (current) PhD training in educational measurement for certain professional positions when, in fact, a well-targeted master's degree coupled with diverse job experiences and strong qualifications in so-called "skills for the future" (a.k.a. "durable skills," "transportable skills," or "complex skills," among others; e.g., Prichard Committee, 2022) would do equally well, if not better. This mindset change in hiring practices would also reduce the undue financial burden that is placed on students by extensive, drawn-out graduate programs and open up career opportunities for a wider range of job candidates. In other words, if we do not change the ways we develop and nurture talent as well as look for talent when we need it, we will not be able to change our talent pool and leadership structures. Perhaps ironically, this argument mirrors debates on the importance of developing skills for the future in K–12 education. For example, in 2023, 17 states had officially adopted a portrait of a graduate (PoG) that can act as an inspirational north star and guardrail for districts across these states as they are designing their own PoGs (Stanford, 2023); this number has likely already increased. A PoG is a general description of the core competencies or profiles that students are expected to develop throughout their K–12 career, allowing them to apply foundational literacy skills in core domains to a variety of complex tasks by themselves and in collaboration with others. Consequently, the adoption of PoGs is often coupled with learner-centered, competency-based education (CBE) models (e.g., Erwin \& Silva-Padrón, 2022; Evans et al., 2020) and an associated renewed interest in modernized versions of project-based learning, portfolio-based demonstration of learning, and exhibitions or defenses of learning at essential transition grades and beyond (e.g., KAEC, 2023). These programs offer up opportunities for connecting K–12 students to learning and assessment issues that might open their eyes to how they can participate in, and importantly shape, the future of this work. With regards to the last point, learners who are discontent with the education system they are experiencing should be encouraged to co-design creative education and assessment solutions and tackle their points of dissatisfaction systematically in collaboration with education leaders. Increasing awareness about the relevance of educational assessment and measurement issues can include simple tactics such as having people from key local, state, or national organizations visit local schools and present real problems of practice along with the realistic constraints—either virtually or in person. These guests can also describe what a typical day, week, month, and year in their roles look like and how they are using the skills they have been learning in both predictable and unique or unexpected ways. Teachers can also leverage the availability of existing public data dashboards and data sets on educational issues to explore issues around data quality and governance, statistical analysis, and educational policy. Districts can develop more interdisciplinary courses that bridge educational and assessment issues by, for example, having students develop assessment processes and validation work around the projects they are completing. The best teachers are already doing this of course. In addition, CBE models make room for alternative models of formally recognizing the development and acquisition of these competencies in the form of badges and micocertificates (e.g., Allen et al., 2022; XQ, 2023). This customization and modularization of educational experiences is already commonplace in various spaces such as YouTube, Khan Academy, Coursera, or EdX, to name but a few. In the dedicated postsecondary space, companies and universities have similarly begun to compete for the financial resources of students by offering certificates and entire graduate degrees online (e.g., Credly, 2023; Western Governors University, 2023). In this regard, the focal report by the NCME Presidential Task Force in this issue lays out a powerful framework for developing such partnerships and resources. Educational measurement competencies will remain important in the future as AI-fueled routines are increasingly used to generate a wider variety of stimuli, complex sample responses, and response evaluation routines. However, an agile adaptation of best practices, professional standards, and general guidelines will be needed to manage the increasing complexities of detecting, understanding, and addressing undesirable side effects. Our collective historical status quo of what assessments looked like in the past, how they were evaluated, and how they were being (mis)used in various context cannot serve as the sole, or even primary, guardrail for this work. In this sense, the joint initiative by AERA, APA, and NCME to update the standards of the profession—and, I would expect, the processes and timelines by which these are reviewed and updated over time—is timely (AERA, APA, \& NCME, 2023). Moreover, there is certainly no way around it: Professionals who are interested in more advanced data-based reasoning competencies for solving educational problems of practice that matter at scale will have to continue to adapt to the changing nature of the interdisciplinary space that their work is situated in. Competencies in educational measurement as traditionally conceived will be part of this skill set but I imagine that the field itself will become less attractive as a stand-alone field with a unique value proposition. This seems to be signaled already by the challenges around attracting new students to the discipline that I have heard various professionals articulate within the NCME Educators of Educational Measurement SIGIMIE and more broadly in casual conversations at recent conferences or other gatherings. The good news is that the changes surrounding our field offer up rich playgrounds for advancement and growth; we just have to find productive and generative ways to leverage them.},
    url = "https://doi.org/10.1111/emip.12629"
}

@article{ref654_bed6e4,
    title = "Prelims",
    year = "2023",
    issn = "0749-7423",
    doi = "10.1108/s0749-742320230000022020",
    abstract = {Citation (2023), "Prelims", Urdan, T. and Gonida, E.N. (Ed.) Remembering the Life, Work, and Influence of Stuart A. Karabenick (Advances in Motivation and Achievement, Vol. 22), Emerald Publishing Limited, Bingley, pp. i-ix. https://doi.org/10.1108/S0749-742320230000022020 Publisher: Emerald Publishing Limited Copyright © 2023 Tim Urdan and Eleftheria N. Gonida. Published under exclusive licence by Emerald Publishing Limited Half Title Page Remembering the Life, Work, and Influence of Stuart A. Karabenick Series Title Page Advances in Motivation and Achievement Series Editor: Timothy C. Urdan Series Editors for Volumes 15–20: Stuart A. Karabenick and Timothy C. Urdan Series Editor for Volumes 1–15: Martin L. Maehr Recent Volumes: Volume 11: The Role of Context: Contextual Influences on Motivation Edited by Timothy C. Urdan Volume 12: New Directions in Measures and Methods Edited by Paul R. Pintrich and Martin L. Maehr Volume 13: Motivating Students, Improving Schools: The Legacy of Carol Midgley Edited by Paul R. Pintrich and Martin L. Maehr Volume 14: Motivation and Religion Edited by Martin L. Maehr and Stuart A. Karabenick Volume 15: Social Psychological Perspectives Edited by Martin L. Maehr, Stuart A. Karabenick and Timothy C. Urdan Volume 16A: The Decade Ahead: Theoretical Perspectives on Motivation and Achievement Edited by Timothy C. Urdan and Stuart A. Karabenick Volume 16B: The Decade Ahead: Theoretical Perspectives on Motivation and Achievement Edited by Timothy C. Urdan and Stuart A. Karabenick Volume 17: Transitions Across Schools and Cultures Edited by Stuart A. Karabenick and Timothy C. Urdan Volume 18: Motivational Interventions Edited by Stuart A. Karabenick and Timothy C. Urdan Volume 19: Recent Developments in Neuroscience: Research on Human Motivation Edited by Sung-Il Kim, Johnmarshall Reeve and Mimi Bong Volume 20: Motivation in Education at a Time of Global Change: Theory, Research, and Implications for Practice Edited by Eleftheria N. Gonida and Marina Serra Lemos Volume 21: Motivating the SEL Field Forward Through Equity Edited by Nicholas Yoder and Alexandra Skoog-Hoffman Title Page Advances in Motivation and Achievement Volume 22 Remembering the Life, Work, and Influence of Stuart A. Karabenick: A Legacy of Research on Self-Regulation, Help Seeking, Teacher Motivation, and More Edited By Tim Urdan Santa Clara University, USA And Eleftheria N. Gonida Aristotle University of Thessaloniki, Greece United Kingdom – North America – Japan – India – Malaysia – China Copyright Page Emerald Publishing Limited Howard House, Wagon Lane, Bingley BD16 1WA, UK First edition 2023 Editorial matter and selection © 2023 Tim Urdan and Eleftheria N. Gonida. Individual Chapters © The authors. Published under exclusive licence by Emerald Publishing Limited. Chapter 2, On the Rewards of Being Open to Opportunities and Their Challenges, is Open Access with copyright assigned to respective chapter authors. Published by Emerald Publishing Limited. These works are published under the Creative Commons Attribution (CC BY 4.0) licence. Anyone may reproduce, distribute, translate and create derivative works of these works (for both commercial and non-commercial purposes), subject to full attribution to the original publication and authors. The full terms of this licence may be seen at http://creativecommons.org/licences/by/4.0/legalcode Original article: Karabenick, S. A. (2020, June 3). On the rewards of being open to opportunities and their challenges. Acquired Wisdom Series, S. Nieto, F. Erickson, \& P. Winne (Eds.), Education Review, 27. http://dx.doi.org/10.14507/er.v27.2965 This work is published under the Creative Commons Attribution (CC BY 4.0) licence. Anyone may reproduce, distribute, translate and create derivative works of this book (for both commercial and non-commercial purposes), subject to full attribution to the original publication and authors. The full terms of this licence may be seen at http://creativecommons.org/licences/by/4.0/legalcode The ebook edition of this title is Open Access and is freely available to read online. British Library Cataloguing in Publication Data A catalogue record for this book is available from the British Library ISBN: 978-1-80455-711-2 (Print) ISBN: 978-1-80455-710-5 (Online) ISBN: 978-1-80455-712-9 (Epub) ISSN: 0749-7423 (Series) List of Contributors Jeffrey R. Albrecht University of Michigan, USA Jin-Seo Bae University of Michigan, USA Hefer Bembenutty Queens College, City University of New York, USA Jean-Louis Berger University of Fribourg, Switzerland Rhonda S. Bondie Harvard University, USA Shannon Elkins University of Michigan, USA Eleftheria N. Gonida Aristotle University of Thessaloniki, Greece Cameron A. Hecht University of Texas at Austin, USA Jessica E. Kilday University of Michigan, USA Revathy Kumar University of Toledo, USA Colleen M. Kuusinen University of Massachusetts Amherst, USA Fani Lauermann TU Dortmund University, Germany Kara A. Makara University of Glasgow, UK Loren M. Marulis Connecticut College, USA Vitaliy Popov University of Michigan, USA Paul W. Richardson Monash University, Australia Allison M. Ryan University of Michigan, USA Nancy Parker Seay Simmons College of Kentucky, USA Stefanie D. Teasley University of Michigan, USA Daniel Teramoto Santa Clara University, USA Tim Urdan Santa Clara University, USA Jeffery H. Warnke Walsh University, USA Helen M. G. Watt The University of Sydney, Australia Akane Zusho Fordham University, USA Book Chapters Prelims Introduction On the Rewards of Being Open to Opportunities and Their Challenges Section I Academic Help Seeking How Interpersonal Factors Matter for Help Seeking in the Classroom Academic Help Seeking and Motivational Beliefs in Academically Talented Students Academic Help Seeking as a Process of Seeking Formative Feedback on Learning Section II Teacher Motivation Who Is Responsible for Student Learning? Teachers' Beliefs about Professional Responsibility, Teacher Attributions, and Their Implications for Classroom Processes and Outcomes Supportive School Workplaces for Beginning Teachers' Motivations and Career Satisfaction How Teachers Provide Help that Furthers Learning in Digital and Nondigital Learning Contexts Section III Self-Regulated Learning Self-Regulated Learning, Core Properties of Human Agency, and Systematic Pedagogies Facilitating Strategic and Effective Approaches to Learning and Academic Success: The Development and Evaluation of Models of Associations between Early Years Metacognition and Self-Regulation Motivational Beliefs, Metacognition, and Self-Regulated Learning: Investigating the Learning Triumvirate with Stuart Karabenick Self-Regulated Learning Theory and Epistemic Network Analysis: Understanding University Students' Use of a Learning Analytics Dashboard Section IV Relevance, Culture, and Methodology Educational Relevance in the Motivation Sciences: An Interdisciplinary Synthesis Bridging Perspectives: The Epistemic Links Between Mastery-Focused and Culturally Inclusive and Responsive Learning Environments Making It Better: Stuart Karabenick's Contribution to the Field of Cognitive Pretesting},
    url = "https://doi.org/10.1108/s0749-742320230000022020"
}

@article{ref655_88fa71,
    title = "Index",
    year = "2023",
    doi = "10.1108/978-1-83753-618-420231018",
    abstract = {Citation (2023), "Index", Lytras, M.D. (Ed.) Active and Transformative Learning in STEAM Disciplines (Emerald Studies in Active and Transformative Learning in Higher Education), Emerald Publishing Limited, Leeds, pp. 297-306. https://doi.org/10.1108/978-1-83753-618-420231018 Publisher: Emerald Publishing Limited Copyright © 2024 Miltiadis D. Lytras. Published under exclusive licence by Emerald Publishing Limited INDEX Academic management, 135 Active and Transformative Learning (ATL), 1–2 core components in higher education in times of change, 15–19 determinants, 1–2 as enabler and multiplier of entrepreneurship, 14–15 enabler and multiplier of research, development and innovation in times of change, as, 16 enablers, 288–291 in higher education, 2 higher education in times of fast and disruptive changes, 5, 7–8 as integrated set of value-adding initiatives, 284–288 ontological proposition, 19–20 strategic alignment, 10–11 supplementary material, 21–22 system in higher education, 8–15 Active blended learning (ABL), 231 Active learning, 7, 9, 11, 26, 42, 62–63, 66–67, 104, 126–127, 157, 199, 248, 261–262 approaches, 46–47 assessing effectiveness of active learning at SNIH, 52–53 challenges and opportunities for promoting active learning at SNIH, 53–54 in education, 63, 231 key features of active learning methods, 51–52 and relation to transformative education and research skills capability, 46–47 strategies in economics education, 69 “Active Learning in Large Economics Lecture” case study, 73–74 Active learning integrative model (ALIM), 163 Active participation, 64 Active training model (ATM), 164, 167 Activity reports, 87–88, 91 apply data mining, 91 collect, 89 data cleanup, 90–91 data validity, 91 interpret, evaluate, and deploy results, 91 pre-process, 89 Administrative support, 18 Adult education, 126 Adult learners, 132 Adult learning, 129–130 2030 Agenda for Sustainable Development, 27 2014 Aicha-Nagoya Declaration on Education for Sustainable Development, 28, 33–34 Analysis, design, development, implementation, and evaluation model (ADDIE model), 213 Andragogy, 132 Appraisals, 158 Apprenticeships, 50 Artificial intelligence (AI), 9, 11, 178, 282, 285 education supported by, 180 replace writers, 114–115 tools, 179 Assessment and Teaching of the 21st Century Skills (ATC21S), 29–30 Asynchronous learning and communication tools, 216 Audience Response System (ARS), 219–220 Augmented reality, 178 Backchannel technology, 219–220 Being proactive, importance of, 108 Benchmark assessment, 264–265 Big Data analytics, 82–83 Black swan event, 62 Blackboard, 81–84 Blended learning, 178 background information, 230 COVID-19 educational challenges, 230–231 definitions, 232 effective in economics, 238 hypothesis derived from COVID-19 challenges, 231–232 models, 232–236 pre-COVID-19 learning in universities and colleges, 230 prerequisites and application of blended learning in economics, 236–238 sustainability, digital transformation, and active learning in education, 231 teaching orientation, 241–243 Blogs, 62–63, 113 Bloom’s taxonomy, 66 Bow Valley College, 206 Business papers, 108 Business plans, 108, 113 Business professionals, 105, 107 Business strategy, 252 Business texting, 113 Business writing, 113 Cabrini University, 206 California Common Core Standards (CACCS), 269–270 Case studies, 51 Case-based active learning approach, 66 Case-based learning (CBL), 50 Class flipping, 68 Clickers, 219–220 Cloud computing, 282 Cloud words, 183 Cognitive computation, 282 Cognitive computing, 282 Collaboration, 45, 156–157 Collaborative learning, 50, 161 Collaborative partnerships, 49 College of Business and Administration (CBA), 82–83 College of Engineering (CE), 82–83 College of Law (CL), 82–83 Communication skills, 105 in business, 108 evolution, 109–110 Communication through writing, 111 Communities of practice, 155 Community of inquiry (CoI), 214 Computer-based test (CBT), 268 Constructivism, 42–43, 45 Content reflection, 130–131 Content writing, 111–114 blogs, 113 business plans, 113 business texting, 113 communication through writing, 111 creative writing, 112 email, 112–113 journalism, 112 letter writing, 113–114 memorandum, 113 research, 111–112 social media writing, 114 Continuous Professional Development (CPD), 158, 162–163 Conventional educational approaches, 64–65 Council of International Schools (CIS), 269–270 COVID-19 educational challenges, 230–231 health crisis, 159 pandemic, 62, 65, 135–136, 154 Creative writing, 106, 112 Critical reflection theory, 130–131 Dashboards, 83 Data analysis, 92–99 Data mining, 83–87, 92 analysis, 93–94 discussion forums, 95–96, 99 file, 98 four-year historic activity report analysis, 93–94 log file data mining results, 96 quiz, 95 quiz attempt view, 96 status of submission, 96–97 turnitin assignment, 97–98 Data security, 49 Data sources, 86–87 DBMiner, 85–86 Delgado Community College, 203–204 Delphi study, 206–207 Development, 45 capacity, 1 DigComp, 161 Digital Economy and Society Index (DESI), 169 Digital identities, 110 Digital innovation, 80–81 Digital learning technologies, 80–81 Digital resources, 142 Digital technology, 33, 154, 180–181 Digital transformation (see also Educational transformation), 63, 104, 126–127, 199, 231 propositions for initiatives related to digital transformation of education, 144–148 Digitalization, 138 Discovery-based active learning approach, 66 Discussion forums, 95–96 Distance education, 178 Divergent situations, 141 E-learning, 50, 282 E-mentoring programs, 168 E-professionalism, 110 Economics, 238 education, 62 Economics students, 62 challenges and opportunities, 68 future recommendations, 72–73 implications for policymakers, 70–72 key findings, 68–70 literature review, 64–70 methodology, 63–64 teaching orientation, 73–74 theoretical framework, 64–65 Education, 27, 42–43, 129–130 administrators, 71, 105–106 data analytics framework, 178 supported by artificial intelligence, 180 for sustainability, 154 Education 4.0, 178 Education for Sustainable Development, 28–29 Education for Sustainable Education (ESD), 26 Educational challenges, 230–231 Educational Data Mining (EDM), 83–86 process, 84 Educational institutions, 32, 65, 156 Educational leadership, 18 Educational policies and practices, 28–29 Educational sustainability, 26 association of educational sustainability with educational transformation, 28 and education for sustainable development, 26–27 educational policies and practices, 28–29 educational transformation to prepare students for jobs, 29–30 external relations to develop competencies, 33–34 key drivers for transforming education, 30–33 need for, 27 rethinking approaches of educational practice, 29 and transformation in education, 26–30 Educational transformation educational sustainability and, 26–30 key drivers for, 30–33 to prepare students for jobs, 29–30 Educators, 66, 68, 134 Email, 112–113 EmSAT test, 260, 265 English language learners (ELLs), 232–233 Entrepreneurship, 1 Entrepreneurship, 11, 13 European School of Sustainability Sciences and Research (ESSSR), 31 Evidence-based EdTech solutions, 73 Experience, 132 Experiential learning, 132–133 Exploratory case study, 86 Extrinsic motivation, 251 Face-to-face driver model, 232–233 Facilitator, 137–138 Faculty, 71, 105–106 capacity, 17 members, 104–105 training and support, 53 Feedback, 52 Flex model, 234–235 Flexibly accessible learning environment (FALE), 200–201 Flipped classroom method, 50, 52 Four-year historic activity report analysis, 93–94 Fund-raising, 13 General Educational Development (GED), 268 Google Docs, 199 Google Wave, 199 Government officers, 71, 106 Graduate studies, 134–135 Grant applications, 108 Group discussions and debate, 51 Group work, 43 Health Information Administration (HIA), 104–105 Health profession education, 42 key concepts and principles of active learning in, 45–46 Health research institutions, 49 High school diploma, 265 High School Equivalency policy (HSE policy), 260–261 aims and objective, 261–262 benchmark assessment, 264–265 knowledge-based economy, 263 literature review, 262–269 methodology, 269–271 options and challenges, 268–269 qualifications and skills mismatch, 263–264 results, 271–272 teaching orientation, 274–276 theoretical and conceptual framework, 262–263 in UAE, 265 in US context, 267–268 for US-Curriculum Schools, 266–267 Higher education (HE), 1, 9, 31, 80, 105, 207 active and transformative learning system in, 8–15 faculty’s commitments, 31 institutions, 99 matching in times of change, 12 in times of fast and disruptive changes, 5, 7–8 Higher-order thinking skills, promotion of, 45 Historic 4-year log data, 96 Historic learning analytics, 85 Hybrid education model, 178, 180–181 clustering stage, 186–188 data exploration stage, 183–185 experimental results, 189–194 extraction and collected data stage, 181–183 future work, 194 interpretation stage, 188 proposed methodology, 181–189 related work, 179–181 validation stage, 189 Hybrid learning, 180 in-service teacher education, professional development, and teacher evaluation, 158–160 innovative hybrid learning models, 162–169 new educational paradigm, 155–158 structuring principles of active teacher training, 165 teaching and research orientation, 171 technology-enhanced active learning, 160–162 Hybrid-Flexible course design (HyFlex course design), 198 benefits, 201–202 empirical evidence, 203–206 guiding principles and policies for curriculum alignment and enhanced learning, 217 instructional design, 202–203 limitations and challenges, 212–216 literature review, 200–201 longitudinal and real-time empirical data, 219 methodology, 206–207 need for guiding principles and policies for clear and effective communication, 219 need for guiding principles and policies for institutional practices, 218 need for guiding principles and policies for training and support, 217–218 policy recommendations, 211–212 results, 207–209 scalable technological and pedagogical approach, 219–220 strategies and actions, 216–217 Idea, 248 Ill-structured problems, 141 Impact measurement \& learning analytics, 18 Implementation guidelines, 202–203 In-service teacher education (ITE), 154, 157–158, 160 Incheon Declaration, 27 Industry, 9 matching in times of change, 12 Information and communication technology (ICT), 156–157 Information technologies, 62 Innovation, 1, 13, 135 Innovative hybrid learning models, 162–169 Inquiry-based active learning approach, 66 Inquiry-based learning (IBL), 50 Instructional design, 198, 202–203 Instructional technologies (ITEC), 198 Instructors, 216 Interactive multimedia, 142 Interdisciplinary program, 26 International Standard Classification of Education (ISCED), 263 International studies, 154–155 Internships, 50 Interprofessional education (IPE), 42 Intrinsic motivation, 251 Isomorphism, 166 Jeddah College of Advertising (JCA), 82–83 Journalism, 106, 112 K-means algorithm, 188 Key drivers for transforming education, 30–33 Key performance indicators (KPIs), 285 Knowledge and content, 17 Knowledge assessment methodology tool (KAM tool), 263 Knowledge-based economy, 29–30, 260, 263 Leadership model, 33 Learner-centered instruction, 45 Learner-centeredness, 45 Learners, 132 Learning, 42, 126 spheres, 129 strategies, 17, 164 Learning analytics, 80–81, 83 in Saudi Arabia, 82–83 Learning management systems (LMS), 62–63, 80, 82–83, 213–214 Lecturers, 80 Legal writing, 116 Lehigh Valley Campus of Pennsylvania State University (PSU-LV), 200–201 Letter writing, 113–114 LinkedIn, 110 Live lecture streaming (LLS), 219–220 Locke’s Goal Setting Theory, 250 Log file, 87 data mining results, 96 historic 4-year log data, 96 Machine learning (ML), 285 Management and business theories, 251–252 Marketing agents, 110 Massive open online courses (MOOCs), 286 Medical education, 42 Medical educators, 42 Memorandum, 113 Mentoring, 50 Metaverse, 14–15, 282 Mezirow’s transformative learning theory, 64, 72 Modified Cornell split-page method, 141 Moodle, 81–84 Moodle Log File, 91–92 apply data mining, 92 collect, 91–92 interpret, evaluate, and deploy results, 92 pre-process, 92 Motivation, 248 literature review, 249–255 significance of research, 248–249 teaching orientation, 256–257 theory, 251 New England Association of Schools and Colleges (NEASC), 269–270 One-minute papers strategy, 141 Online driver model, 236 Online education, 135–136 Online lab model, 235 Online learning, 64–65 frameworks, 62 Online simulations, 142 Online-based active learning strategies, 67 Online-based solutions, 62 Opportunities and challenges of HyFlex course design, 200 Oracle based information systems for registration, grading, attendance, and advising systems (OPERA systems), 82–83 Organization for Economic Co-operation and Development (OECD), 29–30 Orientation to learning, 132 Participatory pedagogies, 32 Partnership for 21st Century Skills (P21), 29–30 Passive learning, 62–63 Pedagogical differentiation, 166 Pedagogical methods, 62–63 Pedagogy, 231, 248 of sustainability, 29 Peer evaluation, 52–53 Peer learning, 156–157 Peer teaching, 52 Personal digital mobile devices, 167 Policy makers, 71, 106 Post-Covid-19 adoption, 63 Post-digital era, 180–181 Post-digital learning spaces, 180–181 Practical learning activities, 43 Practical technologies, 216 Pre and post-assessments, 52 Pre-COVID-19 learning in universities and colleges, 230 Premise reflection, 130–131 Principles of Clinical Trial Program, 54–57 Privacy, 49 Problem-based active learning approach, 66 Problem-based learning (PBL), 43, 45, 50 Process reflection, 130–131 Professional development, 154, 158, 160 Professional learning, 156–157 Professional writing, 107–108 Project-based active learning approach, 66 Project-based learning, 43, 51, 161 Promoting transformative active learning at SNIH, 49–54 Qualifications and skills mismatch, 263–264 Quiz, 95 attempt view, 96 Readiness to learn, 132 Reflection, 52 Reflective learning, 130 Reinforcement Theory of Motivation, 250 Remote live participation (RLP), 200–201 Research, 1 education, 46 Research, Development (R\&D), 13 Resistance to change, 53 Resources, 32–33 Role-playing, 51 Rotation model, 234 Rubrics, 52 Sakai, 81–82 Saudi Arabia, 281–282 Active and Transformative Learning as integrated set of value-adding initiatives, 284–288 Active and Transformative Learning enablers, 288–291 supplementary material, 291–295 Vision 2030 implementation and resilience, 281–284 Saudi Arabian higher education, 80 contribution to theory and applied industry, 100 data analysis, 92–99 data mining, 87–92 EDM, 83–86 findings, 99 future research, 101 learning analytics, 81–83 limitation, 100 methodology, 86–87 transformative learning, 80–81 Saudi National Institute of Health (SNIH), 42–43, 58 assessing effectiveness of active learning at, 52–53 background and context, 42–43 case studies, 54–57 challenges and opportunities for promoting active learning at, 53–54 current challenges and opportunities, 49 current teaching methods and approaches, 49–51 educational and training initiatives, 48 future direction for transformative active learning, 58–59 history and mission, 47–48 key features of active learning methods, 51–52 objectives, strategies, and outcomes, 55–57 promoting transformative active learning at, 49–54 purpose, 43 recommendations, 57–58 theoretical framework, 43–47 Saudi Vision 2030 program, 47 School systems, 157 Science, Technology, Engineering, and Mathematics (STEM), 59, 261, 268–269 Science, technology, engineering, arts, and mathematics disciplines (STEAM disciplines), 70, 117, 127, 249 Science Technology and Innovation (STI), 261 Self-blend model, 236 Self-concept, 132 Self-directed learning, 44, 126 Self-Efficacy Theory of Motivation, 250 Self-regulated learning (SRL), 101 Simulation-based learning, 45–46 Simulations, 51 Skills and competencies uniqueness, 17 Smartphones, 167 Social impact, 134 Social media, 62–63 and business, 115 for student writers, 110–111 writing, 114 Social movement, 155–156 Social presence, 214 Social sensing, 282 Space and resources, 53 SPSS Clementine, 85–86 Stakeholders, 62 Standard operating procedures (SOPs), 108 Start-up incubators, 13 Status of submission, 96–97 Strategic writing, 111 Strategy, 252–255 teacher employ, 252–254 Strength, weaknesses, opportunities, threats analysis (SWOT analysis), 253 Student engagement, 64–66, 231 and motivation, 53 Student writers, importance of social media for, 110–111 Student-centered learning, 199 Student-mediated podcasts, 62–63 Students, 71 Students’ engagement, 31–32 Successful teaching and learning, 128 Surveys, 52 Sustainability, 26–27, 63, 104, 126–127, 199, 231 Sustainable development (SD), 26, 283–284 17 sustainable development goals (SDG), 26–27 Synchronous learning in distributed environments (SLIDE), 200–201 Tablets, 167 Teachers, 126, 154 appraisal, 160 education, 154 educators, 154–155, 157, 159 evaluation, 158–160 Teaching, 230 presence, 214 sustainability, 29 Teaching writing in promoting active and transformative learning, 116–117 research findings about, 104–106 Team-based learning, 68 Technical communication (TC), 104 Technical reports, 108 Technological skills, 155 Technology, 46, 80, 248 enablers, 164 infrastructure, 53 transfer offices, 13 Technology-enhanced active learning, 160–162 Technology-enhanced learning, 170 enhancement, 17 Topic modeling, 183 Training models, 162 Transformation, 268–269, 283–284 Transformational learning theory, 128, 131 Transformational visions, 30–31 Transformative active learning, 42–43, 45 future direction for, 58–59 Transformative learning, 9, 11, 31–32, 62–63, 128, 134, 155 implications of research, 143 propositions for initiatives related to digital transformation of education, 144–148 Saudi Arabian higher education, 80–81 strategies, 128, 138, 143 in transforming higher education, 134–138 Transforming higher education, 134–138 Turnitin assignment, 97–98 Twenty-first-century skills, 29–30 UAE Ministry of Education (MOE), 260 Undergraduate studies, 134–135 UNESCO, 29–30 Universities, 109, 156 digitalization, 135 University of Business and Technology (UBT), 82–83 University of St. Thomas, 205 Virtual and augmented reality technology (VR/AR), 256 Virtual education, 179 Virtual labs, 142 Virtual learning, 236 Virtual reality, 178, 282 Vision 2030 implementation and resilience, 281–284 Vocational training programs, 134–135 Web science, 282 Weka tool, 84–86 Word Clouds, 183–185 Workplace writing, 115–116 World Culture Theory, 264 Writing, 103–104, 141 artificial intelligence replace writers, 114–115 business professionals, 107 communication skills in business, 108 content writing, 111–114 evolution of communication skills, 109–110 importance of being proactive, 108 importance of social media for student writers, 110–111 nutshell, 107 research findings about teaching writing, 104–106 social media and business, 115 teaching orientation, 120–122 teaching writing in promoting active and transformative learning, 116–117 transformative role of writing in active learning approaches in STEAM disciplines, 106–111 universities, 109 workplace writing, 115–116 Book Chapters Prelims Introduction: Active and Transformative Learning (ATL) as a New Higher Educational Paradigm Chapter 1 Active and Transformative Learning (ATL) in Higher Education in Times of Artificial Intelligence and ChatGPT: Investigating a New Value-Based Framework Chapter 2 Educational Sustainability for Transforming Education: A New Approach of Active Learning in an Interdisciplinary Program in Higher Education Chapter 3 Transformative Active Learning in the Saudi National Institute of Health: Promoting Education and Research Skills Capability Chapter 4 Practical Applicability of Active Learning Strategies for Stimulating Engagement Among Economics Students: The Post-Pandemic Outlook Chapter 5 Historic Learning Analytics Transforming Learning in Saudi Arabian Higher Education Chapter 6 From Passive to Active Learning: The Transformative Power of Writing in Higher Education Chapter 7 Transformative Learning Strategies for Successful Teaching and Learning in the Transforming Higher Education Chapter 8 Innovative Hybrid Learning: A New Paradigm in Teacher Education for Transformative Learning Chapter 9 How a Hybrid Education Model Can Support the COVID-19 Sanitary Emergency Based on Information and Communication Technologies Chapter 10 Adopting HyFlex Course Design: Actions for Policymakers, Researchers, and Practitioners Chapter 11 Blended Learning as the Baseline for Post-COVID-19 Higher Education Chapter 12 Seeking Motivation for the Success of Active Learning Chapter 13 Implementation of a High School Equivalency Policy in an Active Learning Environment: A Case Study of US-Curriculum School in the United Arab Emirates Chapter 14 An Integrated Transformative Learning Strategy at National Level: Bold Initiatives Toward Vision 2030 in Saudi Arabia Index},
    url = "https://doi.org/10.1108/978-1-83753-618-420231018"
}

@article{ref656_88fa71,
    title = "Index",
    year = "2018",
    issn = "1534-0856",
    doi = "10.1108/s1534-085620180000019002",
    url = "https://doi.org/10.1108/s1534-085620180000019002"
}
