@article{HAMID2022100044,
title = {Hybrid modelling for remote process monitoring and optimisation},
journal = {Digital Chemical Engineering},
volume = {4},
pages = {100044},
year = {2022},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2022.100044},
url = {https://www.sciencedirect.com/science/article/pii/S2772508122000333},
author = {Anuar Hamid and Anton Heryanto Hasan and Siti Nurfaqihah Azhari and Zalina Harun and Zulfan A. Putra},
keywords = {First principle modeling, Machine learning, Digital process engineering, Process modeling, Process simulation, Process optimization, Multi objective optimization, Dehydration unit, Natural gas, Water dew point, Reboiler duty},
abstract = {Process simulation is used to develop a digital twin representation of chemical processes typically for process optimization or what-if scenarios. However, it is known to be computationally expensive and there is an increasing need for process remote monitoring and optimization. This is where machine learning models shine, where they can run up to several orders of magnitude faster than their equivalent first principle process simulation models. Most of the previous work in this area tend to demonstrate the ability of machine learning models to accurately capture complex, non-linear relationships between process parameters in various chemical processes. Two important aspects are rarely discussed, namely the construction of machine learning models using operating data and the deployment of these models. In this paper, we address these two aspects and review the different information silos in industrial settings that need to be considered when working with hybrid models (combining process simulation and machine learning). This is illustrated by a case study for an operating natural gas dehydration unit, covering data management, process simulation, machine learning and visualization. We demonstrate how the hybrid models can be constructed and packaged as an online monitoring and a prediction dashboard. Several unique challenges are also highlighted including the reliability of field data and operational deviations due to operability and controllability - all of which need to be understood in order to successfully translate operating systems to process simulation and machine learning models that are reliable and accurate. While the exact implementation may vary from project to project, the current work serves as an example and highlights the important considerations to make when working with such systems.}
}
@article{MALLIKARJUNAN20235308,
title = {Lessons Learned from Nanbar Health: Developing a Multi-System Software Suite for Predictive Algorithms and Remote Monitoring for People Living with Sickle Cell Disease},
journal = {Blood},
volume = {142},
pages = {5308},
year = {2023},
note = {65th ASH Annual Meeting Abstracts},
issn = {0006-4971},
doi = {https://doi.org/10.1182/blood-2023-188977},
url = {https://www.sciencedirect.com/science/article/pii/S0006497123119094},
author = {Arvind Mallikarjunan and Elizabeth Hensley and Abhinav Gundala and Jhana Parikh and Olivia Fernandez and Kumar Utkarsh and Caroline Vuong and Shannon Ford and Nirmish Shah},
abstract = {Background: Sickle cell disease (SCD) is a genetic disorder that alters the shape of red blood cells, leading to significant complications and requiring highly complex care. Efforts often focus on prevention and early diagnosis of pain to improve outcomes for those with SCD. Remote monitoring (RPM) and predictive algorithms has grown exponentially in the last decade as a support tool for managing chronic diseases like heart failure and diabetes. Implementing similar technology for SCD care could improve patient outcomes through early detection of complications and providing physicians with longitudinal data to support clinical decisions. Our team previously used mobile health (mHealth) as RPM in this population and developed a machine learning algorithm to predict pain scores with 86% accuracy. Here, we describe the development process for the Nanbar Health software alongside the lessons learned from its development. Objectives: Describe the methods necessary to build a comprehensive mHealth remote monitoring system (Nanbar Health) that facilitates SCD health management.Develop a set of lessons for other developers when building a remote monitoring software suite. Methods: Nanbar Health's development process has taken 2 years. First, our team reviewed feedback and “bug” reports from patients and providers of other mobile applications our team built at Duke University. Using the feedback collected an user-interface (UI) interface (mock-ups) was created for the Nanbar Health mobile app using Figma. These mock-ups were then shown to healthcare providers with expertise in caring for individuals with SCD for feedback. Upon approval of mock-ups, our team developed the mobile application using Swift and Apple HealthKit. The team also built a physician-facing dashboard using React, NodeJS, and Python. The backend software for this system utilized Docker containers mounted on AWS EC2 instances, a Parse server, MongoDB and PostgreSQL databases, and Nginx for deployment and load balancing. Next, the dashboard was reviewed by medical providers for feedback. The analytics portion of the dashboard processed data in Python backend scripts and interactive visualizations were built using the Plotly library in NodeJS. The analytics portion of the tool is currently being reviewed for feedback. Throughout the process of developing and piloting the Nanbar Health Software, our team identified several challenges. These challenges were collected into a comprehensive index named, “Lessons Learned”. Results: The lessons learned include: 1. Overall Gain user feedback and place yourself in the shoes of the user: At every step of your software's development ensure you are continuously getting feedback from your users. 2. Mobile Application Engagement is key to acquiring good data: Ensure your team uses a variety of methods methods to engage your users.No mobile app is one size fits all, but one app can customize to all: The development team should create re-usable, dynamic components wherever possible. 3. Provider Dashboard Testing to build: Developers should build a user interface that is easy to run and test as a “pseudo-user” while they are building it, making it easier for the developer to understand the user experience and build a user-friendly application.Building to test: Plan for beta testers from the beginning and ensure that they have access to continuously-updated and fake but realistic data. 4. Network Analysis Complex analytics are computationally demanding - Advanced analytical tools incur more complications when incorporated into a High-Usage SystemFor heavy data processing, go serverless - Isolating heavy data processing from the rest of the application saves tremendous time and money. 5. Research and Operations Don't cry over spilt code: Mistakes and bugs are part of the process when developing any software, and part of the research process.Lack of engagement is not a reason for disappointment, but rather an opportunity for growth and improvement: Our tool has continuously experienced changes and improvements to increase patient engagement. Conclusion: Our team was successfully able to create and deploy a SCD care-focused Nanbar Health. These tools show promise of improving care for people living with SCD through the usage of RPM and predictive models. Further research should be conducted to determine methods to increase user engagement, and maintain large predictive models in cloud-based systems.}
}
@article{WOOD20215429,
title = {ASH Research Collaborative: a real-world data infrastructure to support real-world evidence development and learning healthcare systems in hematology},
journal = {Blood Advances},
volume = {5},
number = {23},
pages = {5429-5438},
year = {2021},
issn = {2473-9529},
doi = {https://doi.org/10.1182/bloodadvances.2021005902},
url = {https://www.sciencedirect.com/science/article/pii/S2473952921007606},
author = {William A. Wood and Peter Marks and Robert M. Plovnick and Kathleen Hewitt and Donna S. Neuberg and Sam Walters and Brendan K. Dolan and Emily A. Tucker and Charles S. Abrams and Alexis A. Thompson and Kenneth C. Anderson and Paul Kluetz and Ann Farrell and Donna Rivera and Matthew Gertzog and Gregory Pappas},
abstract = {Abstract
The ASH Research Collaborative is a nonprofit organization established through the American Society of Hematology's commitment to patients with hematologic conditions and the science that informs clinical care and future therapies. The ASH Research Collaborative houses 2 major initiatives: (1) the Data Hub and (2) the Clinical Trials Network (CTN). The Data Hub is a program for hematologic diseases in which networks of clinical care delivery sites are developed in specific disease areas, with individual patient data contributed through electronic health record (EHR) integration, direct data entry through electronic data capture, and external data sources. Disease-specific data models are constructed so that data can be assembled into analytic datasets and used to enhance clinical care through dashboards and other mechanisms. Initial models have been built in multiple myeloma (MM) and sickle cell disease (SCD) using the Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) and Fast Healthcare Interoperability Resources (FHIR) standards. The Data Hub also provides a framework for development of disease-specific learning communities (LC) and testing of health care delivery strategies. The ASH Research Collaborative SCD CTN is a clinical trials accelerator that creates efficiencies in the execution of multicenter clinical trials and has been initially developed for SCD. Both components are operational, with the Data Hub actively aggregating source data and the SCD CTN reviewing study candidates. This manuscript describes processes involved in developing core features of the ASH Research Collaborative to inform the stakeholder community in preparation for expansion to additional disease areas.}
}
@article{ZHU202024,
title = {A survey on automatic infographics and visualization recommendations},
journal = {Visual Informatics},
volume = {4},
number = {3},
pages = {24-40},
year = {2020},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2020.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X20300292},
author = {Sujia Zhu and Guodao Sun and Qi Jiang and Meng Zha and Ronghua Liang},
keywords = {Automatic visualization, Data-driven, Knowledge-based, Machine learning, Visual embellishments},
abstract = {Automatic infographics generators employ machine learning algorithms/user-defined rules and visual embellishments into the creation of infographics. It is an emerging topic in the field of information visualization that has requirements in many sectors, such as dashboard design, data analysis, and visualization recommendation. The growing popularity of visual analytics in recent years brings increased attention to automatic infographics. This creates the need for a broad survey that reviews and assesses the significant advances in this field. Automatic tools aim to lower the barrier for visually analyzing data by automatically generating visualizations for analysts to search and make a choice, instead of manually specifying. This survey reviews and classifies automatic tools and papers of visualization recommendations into a set of application categories including network-graph visualizations, annotation visualizations, and storytelling visualization. More importantly, this report presents several challenges and promising directions for future work in the field of automatic infographics and visualization recommendations.}
}