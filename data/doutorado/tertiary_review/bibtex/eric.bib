
@article{paulsen_learning_2024,
	title = {Learning {Analytics} {Dashboards} {Are} {Increasingly} {Becoming} about {Learning} and {Not} {Just} {Analytics} -- {A} {Systematic} {Review}},
	volume = {29},
	issn = {1573-7608},
	abstract = {This systematic review explores the emerging themes in the design and implementation of student-facing learning analytics dashboards in higher education. Learning Analytics has long been criticised for focusing too much on the analytics, and not enough on the learning. The review is then guided by an interest in whether these dashboards are still primarily analytics-driven or if they have become pedagogically informed over time. By mapping the identified themes of technological maturity, informing frameworks, affordances, data sources, and analytical levels over publications per year, the review identifies an emerging trajectory towards student-focused dashboards. These dashboards are informed by theory-oriented frameworks, designed to incorporate affordances that supporting student learning, and realised through integration of more than just activity data from learning management systems -- allowing the dashboards to better support students' learnings processes. Based on this emerging trajectory, the review provides a series of design recommendations for student-focused dashboards that are connected to learning sciences as well as analytics.},
	language = {English},
	number = {11},
	journal = {Education and Information Technologies},
	author = {Paulsen, Lucas and Lindsay, Euan},
	year = {2024},
	note = {ERIC Number: EJ1437394},
	keywords = {Affordances, College Students, Educational Technology, Learning Analytics, Learning Management Systems, Learning Processes, Student Centered Learning},
	pages = {14279--14308},
}

@article{ley_towards_2023,
	title = {Towards a {Partnership} of {Teachers} and {Intelligent} {Learning} {Technology}: {A} {Systematic} {Literature} {Review} of {Model}-{Based} {Learning} {Analytics}},
	volume = {39},
	issn = {1365-2729},
	abstract = {Background: With increased use of artificial intelligence in the classroom, there is now a need to better understand the complementarity of intelligent learning technology and teachers to produce effective instruction. Objective: The paper reviews the current research on intelligent learning technology designed to make models of student learning and instruction transparent to teachers, an area we call model-based learning analytics. We intended to gain an insight into the coupling between the knowledge models that underpin the intelligent system and the knowledge used by teachers in their classroom decision making. Methods: Using a systematic literature review methodology, we first identified 42 papers, mainly from the domain of intelligent tutoring systems and learning analytics dashboards that conformed to our selection criteria. We then qualitatively analysed the context in which the systems were applied, models they used and benefits reported for teachers and learners. Results and Conclusions: A majority of papers used either domain or learner models, suggesting that instructional decisions are mostly left to teachers. Compared to previous reviews, our set of papers appeared to have a stronger focus on providing teachers with theory-driven insights and instructional decisions. This suggests that model-based learning analytics can address some of the shortcomings of the field, like meaningfulness and actionability of learning analytics tools. However, impact in the classroom still needs further research, as in half of the cases the reported benefits were not backed with evidence. Future research should focus on the dynamic interaction between teachers and technology and how learning analytics has an impact on learning and decision making by teachers and students. We offer a taxonomy of knowledge models that can serve as a starting point for designing such interaction.},
	language = {English},
	number = {5},
	journal = {Journal of Computer Assisted Learning},
	author = {Ley, Tobias and Tammets, Kairit and Pishtari, Gerti and Chejara, Pankaj and Kasepalu, Reet and Khalil, Mohammad and Saar, Merike and Tuvi, Iiris and VÃ¤ljataga, Terje and Wasson, Barbara},
	month = oct,
	year = {2023},
	note = {ERIC Number: EJ1391972},
	keywords = {Artificial Intelligence, Decision Making, Educational Benefits, Instructional Effectiveness, Learning Analytics, Models, Taxonomy, Technology Uses in Education},
	pages = {1397--1417},
}

@article{kaliisa_checklist_2023,
	title = {A {Checklist} to {Guide} the {Planning}, {Designing}, {Implementation}, and {Evaluation} of {Learning} {Analytics} {Dashboards}},
	volume = {20},
	issn = {2365-9440},
	abstract = {Higher education institutions are moving to design and implement teacher-facing learning analytics (LA) dashboards with the hope that instructors can extract deep insights about student learning and make informed decisions to improve their teaching. While much attention has been paid to developing teacher-facing dashboards, less is known about how they are designed, implemented and evaluated. This paper presents a systematic literature review of existing studies reporting on teacher-facing LA dashboards. Out of the 1968 articles retrieved from several databases, 50 articles were included in the final analysis. Guided by several frameworks, articles were coded based on the following dimensions: purpose, theoretical grounding, stakeholder involvement, ethics and privacy, design, implementation, and evaluation criteria. The findings show that most dashboards are designed to increase teachers' awareness but with limited actionable insights to allow intervention. Moreover, while teachers are involved in the design process, this is mainly at the exploratory/problem definition stage, with little input beyond this stage. Most dashboards were prescriptive, less customisable, and implicit about the theoretical constructs behind their designs. In addition, dashboards are deployed at prototype and pilot stages, and the evaluation is dominated by self-reports and users' reactions with limited focus on changes to teaching and learning. Besides, only one study considered privacy as a design requirement. Based on the findings of the study and synthesis of existing literature, we propose a four-dimensional checklist for planning, designing, implementing and evaluating LA dashboards.},
	language = {English},
	journal = {International Journal of Educational Technology in Higher Education},
	author = {Kaliisa, Rogers and Jivet, Ioana and Prinsloo, Paul},
	year = {2023},
	note = {ERIC Number: EJ1375927},
	keywords = {College Faculty, Design, Evaluation, Higher Education, Learning Analytics, Learning Management Systems, Planning},
}

@article{ramaswami_use_2023,
	title = {Use of {Predictive} {Analytics} within {Learning} {Analytics} {Dashboards}: {A} {Review} of {Case} {Studies}},
	volume = {28},
	issn = {2211-1670},
	abstract = {Learning analytics dashboards (LADs) provide educators and students with a comprehensive snapshot of the learning domain. Visualizations showcasing student learning behavioral patterns can help students gain greater self-awareness of their learning progression, and at the same time assist educators in identifying those students who may be facing learning difficulties. While LADs have gained popularity, existing LADs are still far behind when it comes to employing predictive analytics into their designs. Our systematic literature review has revealed limitations in the utilization of predictive analytics tools among existing LADs. We find that studies leveraging predictive analytics only go as far as identifying the at-risk students and do not employ model interpretation or explainability capabilities. This limits the ability of LADs to offer data-driven prescriptive advice to students that can offer them guidance on appropriate learning adjustments. Further, published studies have mostly described LADs that are still at prototype stages; hence, robust evaluations of how LADs affect student outcomes have not yet been conducted. The evaluations until now are limited to LAD functionalities and usability rather than their effectiveness as a pedagogical treatment. We conclude by making recommendations for the design of advanced dashboards that more fully take advantage of machine learning technologies, while using suitable visualizations to project only relevant information. Finally, we stress the importance of developing dashboards that are ultimately evaluated for their effectiveness.},
	language = {English},
	number = {3},
	journal = {Technology, Knowledge and Learning},
	author = {Ramaswami, Gomathy and Susnjak, Teo and Mathrani, Anuradha and Umer, Rahila},
	month = sep,
	year = {2023},
	note = {ERIC Number: EJ1385682},
	keywords = {Artificial Intelligence, At Risk Students, Feedback (Response), Identification, Learning Analytics, Learning Management Systems, Models, Prediction},
	pages = {959--980},
}

@article{tretow-fish_methods_2023,
	title = {Methods for {Evaluating} {Learning} {Analytics} and {Learning} {Analytics} {Dashboards} in {Adaptive} {Learning} {Platforms}: {A} {Systematic} {Review}},
	volume = {21},
	issn = {1479-4403},
	abstract = {This research paper highlights and addresses the lack of a systematic review of the methods used to evaluate Learning Analytics (LA) and Learning Analytics Dashboards (LAD) of Adaptive Learning Platforms (ALPs) in the current literature. Addressing this gap, the authors built upon the work of Tretow-Fish and Khalid (2022) and analyzed 32 papers, which were grouped into six categories (C1-6) based on their themes. The categories include C1) the evaluation of LA and LAD design and framework, C2) the evaluation of user performance with LA and LAD, C3) the evaluation of adaptivity, C4) the evaluation of ALPs through perceived value, C5) the evaluation of Multimodal methods, and C6) the evaluation of the pedagogical implementation of ALP's LA and LAD. The results include a tabular summary of the papers including the categories, evaluation unit(s), methods, variables and purpose. While there are numerous studies in categories C1-4 that focus on the design, development, and impact assessment of ALP's LA and LAD, there are only a few studies in categories C5 and C6. For the category of C5), very few studies applied any evaluation methods assessing the multimodal features of LA and LADs on ALPs. Especially for C6), evaluating the pedagogical implementation of ALP's LA and LAD, the three dimensions of signature pedagogy are used to assess the level of pedagogy evaluation. Findings showed that no studies focus on evaluating the deep or implicit structure of ALP's LA. All studies examine the structural surface dimension of learning activities and interactions between students, teachers, and ALP's LA and LAD, as examined in categories C2-C5. No studies were exclusively categorized as a C6 category, indicating that all studies evaluate ALP's LA and LAD on the surface structure dimension of signature pedagogy. This review highlights the lack of pedagogical methodology and theory in ALP's LA and LAD, which are recommended to be emphasized in future research and ALP development and implementation.},
	language = {English},
	number = {5},
	journal = {Electronic Journal of e-Learning},
	author = {Tretow-Fish, Tobias Alexander Bang and Khalid, Md Saifuddin},
	year = {2023},
	note = {ERIC Number: EJ1414672},
	keywords = {Design, Educational Improvement, Educational Quality, Evaluation Methods, Learning Analytics, Learning Experience, Models, Outcomes of Education, Program Implementation, Usability, Value Judgment},
	pages = {430--449},
}

@article{matcha_systematic_2020,
	title = {A {Systematic} {Review} of {Empirical} {Studies} on {Learning} {Analytics} {Dashboards}: {A} {Self}-{Regulated} {Learning} {Perspective}},
	volume = {13},
	issn = {1939-1382},
	abstract = {This paper presents a systematic literature review of learning analytics dashboards (LADs) research that reports empirical findings to assess the impact on learning and teaching. Several previous literature reviews identified self-regulated learning as a primary focus of LADs. However, there has been much less understanding how learning analytics are grounded in the literature on self-regulated learning and how self-regulated learning is supported. To address this limitation, this review analyzed the existing empirical studies on LADs based on the well-known model of self-regulated learning proposed by Winne and Hadwin. The results show that existing LADs are rarely grounded in learning theory, cannot be suggested to support metacognition, do not offer any information about effective learning tactics and strategies, and have significant limitations in how their evaluation is conducted and reported. Based on the findings of the study and through the synthesis of the literature, the paper proposes that future research and development should not make any a priori design decisions about representation of data and analytic results in learning analytics systems such as LADs. To formalize this proposal, the paper defines the model for user-centered learning analytics systems (MULAS). The MULAS consists of the four dimensions that are cyclically and recursively interconnected including: theory, design, feedback, and evaluation.},
	language = {English},
	number = {2},
	journal = {IEEE Transactions on Learning Technologies},
	author = {Matcha, Wannisa and Uzir, Nora'ayu Ahmad and Gasevic, Dragan and Pardo, Abelardo},
	month = jun,
	year = {2020},
	note = {ERIC Number: EJ1279917},
	keywords = {Computer Interfaces, Design, Educational Research, Feedback (Response), Learning Analytics, Learning Strategies, Learning Theories, Outcome Measures, Research Design, Visual Aids},
	pages = {226--245},
}

@article{tepgec_learning_2022,
	title = {Learning {Analytics} {Based} {Interventions}: {A} {Systematic} {Review} of {Experimental} {Studies}},
	abstract = {Learning analytics includes interventions that will support learning and improve learning environments. Despite the fact that learning analytics is a promising field of study, the lack of empirical evidence on the effects of learning analytics-based interventions has been widely addressed in recent years. In this context, insights validated by experimental studies may play a crucial role. Therefore, there is a need for a report describing the methodological aspects and effects of current experimental interventions based on learning analytics. This systematic review provides an in-depth examination of learning analytics research that reports experimental findings to evaluate learning analytics-based interventions. The PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) 2020 protocol provided the basis for the work of this systematic review. This review contained 52 papers that met the inclusion and exclusion criteria. The results show that student-facing dashboards are the most common learning analytics-based intervention. Evidence from how user data is handled for interventions demonstrates that the most common method is the distillation of data for human judgment. This study confirms that a significant proportion of experimental studies employing learning analytics interventions have demonstrated significant effects on learning outcomes. The effectiveness of learning analytics-based interventions is also addressed in this review in terms of motivation, engagement, and system usage behaviors. The findings of this study will contribute to the literature in terms of describing the experimentally validated findings of learning analytics-based interventions in depth.},
	journal = {International Association for Development of the Information Society},
	author = {TepgeÃ§, Mustafa and Ifenthaler, Dirk},
	year = {2022},
	note = {ERIC Number: ED626950},
	keywords = {Elementary Secondary Education, Instructional Effectiveness, Intervention, Kindergarten, Learner Engagement, Learning Analytics, Learning Management Systems, Learning Motivation, Meta Analysis, Outcomes of Education, Research Reports},
}

@article{schwendimann_perceiving_2017,
	title = {Perceiving {Learning} at a {Glance}: {A} {Systematic} {Literature} {Review} of {Learning} {Dashboard} {Research}},
	volume = {10},
	issn = {1939-1382},
	abstract = {This paper presents a systematic literature review of the state-of-the-art of research on learning dashboards in the fields of Learning Analytics and Educational Data Mining. Research on learning dashboards aims to identify what data is meaningful to different stakeholders and how data can be presented to support sense-making processes. Learning dashboards are becoming popular due to the increased use of educational technologies, such as Learning Management Systems (LMS) and Massive Open Online Courses (MOOCs). The initial search of five main academic databases and GScholar resulted in 346 papers out of which 55 papers were included in the final analysis. Our review distinguishes different kinds of research studies as well as various aspects of learning dashboards and their maturity regarding evaluation. As the research field is still relatively young, most studies are exploratory and proof-of-concept. The review concludes by offering a definition for learning dashboards and by outlining open issues and future lines of work in the area of learning dashboards. There is a need for longitudinal research in authentic settings and studies that systematically compare different dashboard designs.},
	language = {English},
	number = {1},
	journal = {IEEE Transactions on Learning Technologies},
	author = {Schwendimann, Beat A. and Rodriguez-Triana, Maria Jesus and Vozniuk, Andrii and Prieto, Luis P. and Boroujeni, Mina Shirvani and Holzer, Adrian and Gillet, Denis and Dillenbourg, Pierre},
	month = mar,
	year = {2017},
	note = {ERIC Number: EJ1141028},
	keywords = {Computer Software Evaluation, Data Analysis, Data Processing, Database Management Systems, Educational Environment, Educational Research, Educational Technology, Learning Activities, Literature Reviews, Online Courses, Performance Factors, Usability, Users (Information), Visualization},
	pages = {30--41},
}
